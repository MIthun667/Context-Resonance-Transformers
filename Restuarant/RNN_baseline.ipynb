{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Category and Polarity:\n",
      "                  Category  Category_encoded  Polarity  Polarity_encoded\n",
      "0  anecdotes/miscellaneous                 1   neutral                 2\n",
      "1                 ambience                 0  conflict                 0\n",
      "2                 ambience                 0  conflict                 0\n",
      "3                  service                 4  conflict                 0\n",
      "4                     food                 2   neutral                 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 128, 128)     15302016    ['input_ids[0][0]']              \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 128)          131584      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Category (Dense)               (None, 5)            645         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Polarity (Dense)               (None, 4)            516         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,434,761\n",
      "Trainable params: 15,434,761\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "184/184 [==============================] - 14s 29ms/step - loss: 2.9933 - Category_loss: 1.6072 - Polarity_loss: 1.3861 - Category_accuracy: 0.2336 - Polarity_accuracy: 0.2523 - val_loss: 2.9896 - val_Category_loss: 1.6043 - val_Polarity_loss: 1.3853 - val_Category_accuracy: 0.2813 - val_Polarity_accuracy: 0.2722\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 4s 22ms/step - loss: 2.9865 - Category_loss: 1.6023 - Polarity_loss: 1.3842 - Category_accuracy: 0.2828 - Polarity_accuracy: 0.3083 - val_loss: 2.9818 - val_Category_loss: 1.5983 - val_Polarity_loss: 1.3834 - val_Category_accuracy: 0.2966 - val_Polarity_accuracy: 0.3547\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 4s 21ms/step - loss: 2.9775 - Category_loss: 1.5956 - Polarity_loss: 1.3819 - Category_accuracy: 0.3146 - Polarity_accuracy: 0.3364 - val_loss: 2.9686 - val_Category_loss: 1.5882 - val_Polarity_loss: 1.3804 - val_Category_accuracy: 0.3563 - val_Polarity_accuracy: 0.3073\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 4s 22ms/step - loss: 2.9583 - Category_loss: 1.5839 - Polarity_loss: 1.3744 - Category_accuracy: 0.3167 - Polarity_accuracy: 0.2962 - val_loss: 2.9356 - val_Category_loss: 1.5649 - val_Polarity_loss: 1.3708 - val_Category_accuracy: 0.3318 - val_Polarity_accuracy: 0.3119\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 4s 22ms/step - loss: 2.9256 - Category_loss: 1.5647 - Polarity_loss: 1.3609 - Category_accuracy: 0.3133 - Polarity_accuracy: 0.3245 - val_loss: 2.8996 - val_Category_loss: 1.5440 - val_Polarity_loss: 1.3556 - val_Category_accuracy: 0.3456 - val_Polarity_accuracy: 0.3440\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 4s 22ms/step - loss: 2.8797 - Category_loss: 1.5406 - Polarity_loss: 1.3391 - Category_accuracy: 0.3371 - Polarity_accuracy: 0.3818 - val_loss: 2.8463 - val_Category_loss: 1.5176 - val_Polarity_loss: 1.3287 - val_Category_accuracy: 0.3716 - val_Polarity_accuracy: 0.3991\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 4s 21ms/step - loss: 2.8105 - Category_loss: 1.5083 - Polarity_loss: 1.3023 - Category_accuracy: 0.3635 - Polarity_accuracy: 0.4172 - val_loss: 2.7801 - val_Category_loss: 1.4883 - val_Polarity_loss: 1.2919 - val_Category_accuracy: 0.4083 - val_Polarity_accuracy: 0.3991\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 4s 21ms/step - loss: 2.7378 - Category_loss: 1.4769 - Polarity_loss: 1.2609 - Category_accuracy: 0.3977 - Polarity_accuracy: 0.4370 - val_loss: 2.7175 - val_Category_loss: 1.4617 - val_Polarity_loss: 1.2559 - val_Category_accuracy: 0.4343 - val_Polarity_accuracy: 0.4251\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 4s 21ms/step - loss: 2.6749 - Category_loss: 1.4477 - Polarity_loss: 1.2272 - Category_accuracy: 0.4060 - Polarity_accuracy: 0.4465 - val_loss: 2.6613 - val_Category_loss: 1.4335 - val_Polarity_loss: 1.2278 - val_Category_accuracy: 0.4343 - val_Polarity_accuracy: 0.4450\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 4s 21ms/step - loss: 2.6233 - Category_loss: 1.4197 - Polarity_loss: 1.2036 - Category_accuracy: 0.4143 - Polarity_accuracy: 0.4456 - val_loss: 2.6160 - val_Category_loss: 1.4063 - val_Polarity_loss: 1.2097 - val_Category_accuracy: 0.4480 - val_Polarity_accuracy: 0.4373\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 2.5987 - Category_loss: 1.4127 - Polarity_loss: 1.1860 - Category_accuracy: 0.4225 - Polarity_accuracy: 0.4378\n",
      "Test Loss and Accuracy: [2.598672389984131, 1.412663459777832, 1.1860090494155884, 0.4225352108478546, 0.437844455242157]\n",
      "52/52 [==============================] - 1s 6ms/step\n",
      "Classification Report for Category:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               ambience     0.5069    0.5909    0.5457       374\n",
      "anecdotes/miscellaneous     0.3429    0.8735    0.4925       411\n",
      "                   food     0.0000    0.0000    0.0000       270\n",
      "                  price     0.7297    0.2914    0.4165       278\n",
      "                service     0.7436    0.0967    0.1711       300\n",
      "\n",
      "               accuracy                         0.4225      1633\n",
      "              macro avg     0.4646    0.3705    0.3251      1633\n",
      "           weighted avg     0.4632    0.4225    0.3512      1633\n",
      "\n",
      "Classification Report for Polarity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict     0.6381    0.6740    0.6555       408\n",
      "    negative     0.3684    0.1373    0.2000       408\n",
      "     neutral     0.3669    0.9289    0.5260       408\n",
      "    positive     0.2941    0.0122    0.0235       409\n",
      "\n",
      "    accuracy                         0.4378      1633\n",
      "   macro avg     0.4169    0.4381    0.3513      1633\n",
      "weighted avg     0.4168    0.4378    0.3511      1633\n",
      "\n",
      "\n",
      "Macro Metrics for Category:\n",
      "Precision: 0.4646\n",
      "Recall:    0.3705\n",
      "F1-Score:  0.3251\n",
      "\n",
      "Macro Metrics for Polarity:\n",
      "Precision: 0.4169\n",
      "Recall:    0.4381\n",
      "F1-Score:  0.3513\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "# Note: NLTK does not include Bengali stopwords by default\n",
    "# Use a custom list or skip stopword removal if unavailable\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "except OSError:\n",
    "    print(\"Bengali stopwords not found. Skipping stopword removal.\")\n",
    "    stop_words = set()\n",
    "\n",
    "# Initialize lemmatizer (WordNetLemmatizer is for English)\n",
    "# Consider removing lemmatization for Bengali or use a Bengali-specific lemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load and Preprocess the Dataset\n",
    "# -------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"F:\\Context-Resonance Transformer\\Restuarant\\Restaurant - Sheet1.csv\")\n",
    "df = df[['Text', 'Category', 'Polarity']]\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep only Bengali characters\n",
    "    text = re.sub(r'\\d+', '', text)                 # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # Remove extra spaces\n",
    "    words = text.split()\n",
    "    if stop_words:\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "    # Optionally, remove lemmatization if not suitable for Bengali\n",
    "    # words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Upsampling for Class Balance\n",
    "# -------------------------------\n",
    "\n",
    "def upsample(df, target_column):\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        df_label = df[df[target_column] == label]\n",
    "        if len(df_label) < max_count:\n",
    "            df_upsampled = resample(\n",
    "                df_label,\n",
    "                replace=True,\n",
    "                n_samples=max_count,\n",
    "                random_state=42\n",
    "            )\n",
    "            upsampled_dfs.append(df_upsampled)\n",
    "        else:\n",
    "            upsampled_dfs.append(df_label)\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Upsample 'Category' and 'Polarity' separately\n",
    "# Note: Upsampling both can lead to a large dataset\n",
    "df = upsample(df, 'Category')\n",
    "df = upsample(df, 'Polarity')\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Encode Labels\n",
    "# -------------------------------\n",
    "\n",
    "label_encoder_cat = LabelEncoder()\n",
    "label_encoder_pol = LabelEncoder()\n",
    "\n",
    "df['Category_encoded'] = label_encoder_cat.fit_transform(df['Category'])\n",
    "df['Polarity_encoded'] = label_encoder_pol.fit_transform(df['Polarity'])\n",
    "\n",
    "# Display encoded labels\n",
    "print(\"Encoded Category and Polarity:\")\n",
    "print(df[['Category', 'Category_encoded', 'Polarity', 'Polarity_encoded']].head())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenize the Text\n",
    "# -------------------------------\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def tokenize_data(df, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in df['Text']:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,      # Add '[CLS]' and '[SEP]'\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    # Convert lists to tensors\n",
    "    input_ids = tf.concat(input_ids, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks, axis=0)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "input_ids, attention_masks = tokenize_data(df)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Split the Data\n",
    "# -------------------------------\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, att_mask_train, att_mask_test, y_cat_train, y_cat_test, y_pol_train, y_pol_test = train_test_split(\n",
    "    input_ids.numpy(), attention_masks.numpy(),\n",
    "    df['Category_encoded'].values, df['Polarity_encoded'].values,\n",
    "    test_size=0.2, random_state=42, stratify=df[['Category_encoded', 'Polarity_encoded']]\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. One-Hot Encode the Labels\n",
    "# -------------------------------\n",
    "\n",
    "num_cat_classes = len(label_encoder_cat.classes_)\n",
    "num_pol_classes = len(label_encoder_pol.classes_)\n",
    "\n",
    "y_cat_train = to_categorical(y_cat_train, num_classes=num_cat_classes)\n",
    "y_cat_test = to_categorical(y_cat_test, num_classes=num_cat_classes)\n",
    "y_pol_train = to_categorical(y_pol_train, num_classes=num_pol_classes)\n",
    "y_pol_test = to_categorical(y_pol_test, num_classes=num_pol_classes)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Define the Multi-Task LSTM Model\n",
    "# -------------------------------\n",
    "\n",
    "# Define input layers\n",
    "input_ids_layer = Input(shape=(128,), dtype='int32', name='input_ids')\n",
    "attention_mask_layer = Input(shape=(128,), dtype='int32', name='attention_mask')\n",
    "\n",
    "# Embedding layer with mask_zero=True to handle padding\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=tokenizer.vocab_size, \n",
    "    output_dim=128, \n",
    "    input_length=128, \n",
    "    mask_zero=True\n",
    ")(input_ids_layer)\n",
    "\n",
    "# Shared LSTM layer (unidirectional)\n",
    "lstm_layer = LSTM(128, return_sequences=False)(embedding_layer)\n",
    "dropout_layer = Dropout(0.3)(lstm_layer)\n",
    "\n",
    "# Task-specific Dense layers\n",
    "category_output = Dense(num_cat_classes, activation='softmax', name='Category')(dropout_layer)\n",
    "polarity_output = Dense(num_pol_classes, activation='softmax', name='Polarity')(dropout_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=[category_output, polarity_output])\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Compile the Model\n",
    "# -------------------------------\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss={\n",
    "        'Category': 'categorical_crossentropy',\n",
    "        'Polarity': 'categorical_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'Category': 'accuracy',\n",
    "        'Polarity': 'accuracy'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Train the Model with EarlyStopping\n",
    "# -------------------------------\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, att_mask_train],\n",
    "    {'Category': y_cat_train, 'Polarity': y_pol_train},\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Evaluate the Model\n",
    "# -------------------------------\n",
    "\n",
    "results = model.evaluate(\n",
    "    [X_test, att_mask_test],\n",
    "    {'Category': y_cat_test, 'Polarity': y_pol_test}\n",
    ")\n",
    "\n",
    "print(f\"Test Loss and Accuracy: {results}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Classification Report with Macro Metrics\n",
    "# -------------------------------\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict([X_test, att_mask_test])\n",
    "\n",
    "# Convert predictions and true labels from one-hot to integer labels\n",
    "y_cat_pred = np.argmax(predictions[0], axis=1)\n",
    "y_pol_pred = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "y_cat_true = np.argmax(y_cat_test, axis=1)\n",
    "y_pol_true = np.argmax(y_pol_test, axis=1)\n",
    "\n",
    "# Generate classification reports\n",
    "report_cat = classification_report(\n",
    "    y_cat_true, y_cat_pred, \n",
    "    target_names=label_encoder_cat.classes_, \n",
    "    digits=4,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "report_pol = classification_report(\n",
    "    y_pol_true, y_pol_pred, \n",
    "    target_names=label_encoder_pol.classes_, \n",
    "    digits=4,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(\"Classification Report for Category:\")\n",
    "print(report_cat)\n",
    "\n",
    "print(\"Classification Report for Polarity:\")\n",
    "print(report_pol)\n",
    "\n",
    "# Extract and print macro precision, recall, F1-score\n",
    "def extract_macro_metrics(report):\n",
    "    report_dict = classification_report(\n",
    "        y_true, y_pred, \n",
    "        target_names=label_encoder.classes_, \n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "        output_dict=True\n",
    "    )\n",
    "    macro_p = report_dict['macro avg']['precision']\n",
    "    macro_r = report_dict['macro avg']['recall']\n",
    "    macro_f1 = report_dict['macro avg']['f1-score']\n",
    "    return macro_p, macro_r, macro_f1\n",
    "\n",
    "# For Category\n",
    "report_cat_dict = classification_report(\n",
    "    y_cat_true, y_cat_pred, \n",
    "    target_names=label_encoder_cat.classes_, \n",
    "    digits=4,\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "macro_p_cat = report_cat_dict['macro avg']['precision']\n",
    "macro_r_cat = report_cat_dict['macro avg']['recall']\n",
    "macro_f1_cat = report_cat_dict['macro avg']['f1-score']\n",
    "\n",
    "print(\"\\nMacro Metrics for Category:\")\n",
    "print(f\"Precision: {macro_p_cat:.4f}\")\n",
    "print(f\"Recall:    {macro_r_cat:.4f}\")\n",
    "print(f\"F1-Score:  {macro_f1_cat:.4f}\")\n",
    "\n",
    "# For Polarity\n",
    "report_pol_dict = classification_report(\n",
    "    y_pol_true, y_pol_pred, \n",
    "    target_names=label_encoder_pol.classes_, \n",
    "    digits=4,\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "macro_p_pol = report_pol_dict['macro avg']['precision']\n",
    "macro_r_pol = report_pol_dict['macro avg']['recall']\n",
    "macro_f1_pol = report_pol_dict['macro avg']['f1-score']\n",
    "\n",
    "print(\"\\nMacro Metrics for Polarity:\")\n",
    "print(f\"Precision: {macro_p_pol:.4f}\")\n",
    "print(f\"Recall:    {macro_r_pol:.4f}\")\n",
    "print(f\"F1-Score:  {macro_f1_pol:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 20s 58ms/step - loss: 2.9924 - Category_loss: 1.6063 - Polarity_loss: 1.3860 - Category_accuracy: 0.2513 - Polarity_accuracy: 0.2571 - val_loss: 2.9884 - val_Category_loss: 1.6024 - val_Polarity_loss: 1.3860 - val_Category_accuracy: 0.2446 - val_Polarity_accuracy: 0.2278\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 2.9848 - Category_loss: 1.5995 - Polarity_loss: 1.3853 - Category_accuracy: 0.2563 - Polarity_accuracy: 0.2619 - val_loss: 2.9795 - val_Category_loss: 1.5946 - val_Polarity_loss: 1.3849 - val_Category_accuracy: 0.2446 - val_Polarity_accuracy: 0.2492\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 2.9767 - Category_loss: 1.5938 - Polarity_loss: 1.3830 - Category_accuracy: 0.2585 - Polarity_accuracy: 0.2911 - val_loss: 2.9697 - val_Category_loss: 1.5872 - val_Polarity_loss: 1.3825 - val_Category_accuracy: 0.2538 - val_Polarity_accuracy: 0.2936\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 2.9650 - Category_loss: 1.5874 - Polarity_loss: 1.3776 - Category_accuracy: 0.2959 - Polarity_accuracy: 0.3121 - val_loss: 2.9483 - val_Category_loss: 1.5774 - val_Polarity_loss: 1.3709 - val_Category_accuracy: 0.3364 - val_Polarity_accuracy: 0.3654\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 2.8806 - Category_loss: 1.5569 - Polarity_loss: 1.3237 - Category_accuracy: 0.3332 - Polarity_accuracy: 0.3752 - val_loss: 2.7556 - val_Category_loss: 1.5153 - val_Polarity_loss: 1.2403 - val_Category_accuracy: 0.3394 - val_Polarity_accuracy: 0.4098\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 7s 39ms/step - loss: 2.6992 - Category_loss: 1.4898 - Polarity_loss: 1.2094 - Category_accuracy: 0.3572 - Polarity_accuracy: 0.4281 - val_loss: 2.6269 - val_Category_loss: 1.4713 - val_Polarity_loss: 1.1556 - val_Category_accuracy: 0.3838 - val_Polarity_accuracy: 0.4709\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 7s 39ms/step - loss: 2.5926 - Category_loss: 1.4472 - Polarity_loss: 1.1454 - Category_accuracy: 0.3708 - Polarity_accuracy: 0.4523 - val_loss: 2.5295 - val_Category_loss: 1.4202 - val_Polarity_loss: 1.1093 - val_Category_accuracy: 0.3899 - val_Polarity_accuracy: 0.4740\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 2.5147 - Category_loss: 1.4067 - Polarity_loss: 1.1080 - Category_accuracy: 0.3966 - Polarity_accuracy: 0.4749 - val_loss: 2.4589 - val_Category_loss: 1.3693 - val_Polarity_loss: 1.0896 - val_Category_accuracy: 0.4144 - val_Polarity_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 7s 39ms/step - loss: 2.4205 - Category_loss: 1.3422 - Polarity_loss: 1.0783 - Category_accuracy: 0.4404 - Polarity_accuracy: 0.4875 - val_loss: 2.3572 - val_Category_loss: 1.3007 - val_Polarity_loss: 1.0565 - val_Category_accuracy: 0.4602 - val_Polarity_accuracy: 0.5168\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 7s 39ms/step - loss: 2.3346 - Category_loss: 1.2821 - Polarity_loss: 1.0525 - Category_accuracy: 0.4620 - Polarity_accuracy: 0.4994 - val_loss: 2.2655 - val_Category_loss: 1.2386 - val_Polarity_loss: 1.0269 - val_Category_accuracy: 0.5031 - val_Polarity_accuracy: 0.5428\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 2.3728 - Category_loss: 1.2971 - Polarity_loss: 1.0758 - Category_accuracy: 0.4685 - Polarity_accuracy: 0.4899\n",
      "Test Loss and Accuracy: [2.372804880142212, 1.2970528602600098, 1.0757511854171753, 0.4684629440307617, 0.48989591002464294]\n",
      "52/52 [==============================] - 3s 15ms/step\n",
      "Classification Report for Category:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'average'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 226\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# Category - Macro metrics\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification Report for Category:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_cat_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_cat_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# Polarity - Macro metrics\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mClassification Report for Polarity:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m params \u001b[38;5;241m=\u001b[39m func_sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    192\u001b[0m params\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\inspect.py:3045\u001b[0m, in \u001b[0;36mSignature.bind\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3040\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3041\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[0;32m   3042\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[0;32m   3043\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\inspect.py:3034\u001b[0m, in \u001b[0;36mSignature._bind\u001b[1;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[0;32m   3032\u001b[0m         arguments[kwargs_param\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m   3033\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3035\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   3036\u001b[0m                 arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[0;32m   3038\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[1;31mTypeError\u001b[0m: got an unexpected keyword argument 'average'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout, Multiply, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv(r\"F:\\Context-Resonance Transformer\\Restuarant\\Restaurant - Sheet1.csv\")\n",
    "df = df[['Text', 'Category', 'Polarity']]\n",
    "\n",
    "# Tokenize Bengali text (pre-cleaning example provided)\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep only Bengali characters\n",
    "    text = re.sub(r'\\d+', '', text)                 # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # Remove extra spaces\n",
    "    words = text.split()\n",
    "    if stop_words:\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Upsampling for Class Balance\n",
    "# -------------------------------\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def upsample(df, target_column):\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        df_label = df[df[target_column] == label]\n",
    "        if len(df_label) < max_count:\n",
    "            df_upsampled = resample(\n",
    "                df_label,\n",
    "                replace=True,\n",
    "                n_samples=max_count,\n",
    "                random_state=42\n",
    "            )\n",
    "            upsampled_dfs.append(df_upsampled)\n",
    "        else:\n",
    "            upsampled_dfs.append(df_label)\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Upsample 'Category' and 'Polarity' separately\n",
    "df = upsample(df, 'Category')\n",
    "df = upsample(df, 'Polarity')\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Encode Labels\n",
    "# -------------------------------\n",
    "\n",
    "label_encoder_cat = LabelEncoder()\n",
    "label_encoder_pol = LabelEncoder()\n",
    "\n",
    "# Ensure labels are strings before encoding\n",
    "df['Category'] = df['Category'].astype(str)\n",
    "df['Polarity'] = df['Polarity'].astype(str)\n",
    "\n",
    "df['Category_encoded'] = label_encoder_cat.fit_transform(df['Category'])\n",
    "df['Polarity_encoded'] = label_encoder_pol.fit_transform(df['Polarity'])\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenize Text Using BERT Tokenizer\n",
    "# -------------------------------\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def tokenize_data(df, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in df['Text']:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True, \n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    input_ids = tf.squeeze(input_ids, axis=1)\n",
    "    attention_masks = tf.squeeze(attention_masks, axis=1)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "input_ids, attention_masks = tokenize_data(df)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Split the Data\n",
    "# -------------------------------\n",
    "\n",
    "X_train, X_test, att_mask_train, att_mask_test, y_cat_train, y_cat_test, y_polarity_train, y_polarity_test = train_test_split(\n",
    "    input_ids.numpy(), attention_masks.numpy(),\n",
    "    df['Category_encoded'].values, df['Polarity_encoded'].values,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_cat_train = to_categorical(y_cat_train, num_classes=len(np.unique(df['Category_encoded'])))\n",
    "y_cat_test = to_categorical(y_cat_test, num_classes=len(np.unique(df['Category_encoded'])))\n",
    "y_polarity_train = to_categorical(y_polarity_train, num_classes=len(np.unique(df['Polarity_encoded'])))\n",
    "y_polarity_test = to_categorical(y_polarity_test, num_classes=len(np.unique(df['Polarity_encoded'])))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Define the Multi-task GRU Model\n",
    "# -------------------------------\n",
    "\n",
    "input_ids_layer = Input(shape=(128,), dtype='int32', name='input_ids')\n",
    "attention_mask_layer = Input(shape=(128,), dtype='int32', name='attention_mask')\n",
    "\n",
    "# Embedding layer\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=tokenizer.vocab_size, \n",
    "    output_dim=128, \n",
    "    input_length=128, \n",
    "    mask_zero=True\n",
    ")(input_ids_layer)\n",
    "\n",
    "# Apply the attention mask to the embeddings\n",
    "mask_float = Lambda(lambda x: K.cast(x, dtype='float32'))(attention_mask_layer)\n",
    "masked_embedding = Multiply()([embedding_layer, mask_float])\n",
    "\n",
    "# Shared GRU layers\n",
    "gru_layer = GRU(256, return_sequences=True)(masked_embedding)\n",
    "gru_layer = GRU(128, return_sequences=True)(gru_layer)\n",
    "gru_layer = GRU(64, return_sequences=False)(gru_layer)\n",
    "dropout_layer = Dropout(0.3)(gru_layer)\n",
    "\n",
    "# Task-specific output layers\n",
    "category_output = Dense(len(y_cat_train[0]), activation='softmax', name='Category')(dropout_layer)\n",
    "polarity_output = Dense(len(y_polarity_train[0]), activation='softmax', name='Polarity')(dropout_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=[category_output, polarity_output])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss={'Category': 'categorical_crossentropy', 'Polarity': 'categorical_crossentropy'},\n",
    "    metrics={'Category': 'accuracy', 'Polarity': 'accuracy'}\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Train the Model with EarlyStopping and Class Weights\n",
    "# -------------------------------\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, att_mask_train],\n",
    "    {'Category': y_cat_train, 'Polarity': y_polarity_train},\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Evaluate the Model\n",
    "# -------------------------------\n",
    "\n",
    "results = model.evaluate(\n",
    "    [X_test, att_mask_test],\n",
    "    {'Category': y_cat_test, 'Polarity': y_polarity_test}\n",
    ")\n",
    "\n",
    "print(f\"Test Loss and Accuracy: {results}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Classification Report with Macro Precision, Recall, F1-Score\n",
    "# -------------------------------\n",
    "\n",
    "predictions = model.predict([X_test, att_mask_test])\n",
    "y_cat_pred = np.argmax(predictions[0], axis=1)\n",
    "y_polarity_pred = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "# Category - Macro metrics\n",
    "print(\"Classification Report for Category:\")\n",
    "print(classification_report(np.argmax(y_cat_test, axis=1), y_cat_pred, average='macro'))\n",
    "\n",
    "# Polarity - Macro metrics\n",
    "print(\"\\nClassification Report for Polarity:\")\n",
    "print(classification_report(np.argmax(y_polarity_test, axis=1), y_polarity_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 1s 16ms/step\n",
      "Classification Report for Category:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.61      0.55       360\n",
      "           1       0.51      0.84      0.63       405\n",
      "           2       0.00      0.00      0.00       295\n",
      "           3       0.49      0.25      0.33       269\n",
      "           4       0.36      0.46      0.40       304\n",
      "\n",
      "    accuracy                           0.47      1633\n",
      "   macro avg       0.37      0.43      0.38      1633\n",
      "weighted avg       0.38      0.47      0.41      1633\n",
      "\n",
      "\n",
      "Classification Report for Polarity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.83      0.61       414\n",
      "           1       0.42      0.23      0.30       421\n",
      "           2       0.55      0.84      0.66       399\n",
      "           3       0.30      0.07      0.11       399\n",
      "\n",
      "    accuracy                           0.49      1633\n",
      "   macro avg       0.44      0.49      0.42      1633\n",
      "weighted avg       0.44      0.49      0.42      1633\n",
      "\n",
      "\n",
      "Macro Precision, Recall, F1 for Category:\n",
      "Precision: 0.37056076564979323, Recall: 0.4307437963627228, F1: 0.3823272063888245\n",
      "\n",
      "Macro Precision, Recall, F1 for Polarity:\n",
      "Precision: 0.4390977408453484, Recall: 0.4902804232516642, F1: 0.4203127386819851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict([X_test, att_mask_test])\n",
    "y_cat_pred = np.argmax(predictions[0], axis=1)\n",
    "y_polarity_pred = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "# Category - Macro metrics using classification_report\n",
    "print(\"Classification Report for Category:\")\n",
    "print(classification_report(np.argmax(y_cat_test, axis=1), y_cat_pred))\n",
    "\n",
    "# Polarity - Macro metrics using classification_report\n",
    "print(\"\\nClassification Report for Polarity:\")\n",
    "print(classification_report(np.argmax(y_polarity_test, axis=1), y_polarity_pred))\n",
    "\n",
    "# Alternatively, calculate precision, recall, F1 score manually:\n",
    "# Category - Macro metrics\n",
    "cat_precision, cat_recall, cat_f1, _ = precision_recall_fscore_support(\n",
    "    np.argmax(y_cat_test, axis=1), y_cat_pred, average='macro'\n",
    ")\n",
    "\n",
    "# Polarity - Macro metrics\n",
    "polarity_precision, polarity_recall, polarity_f1, _ = precision_recall_fscore_support(\n",
    "    np.argmax(y_polarity_test, axis=1), y_polarity_pred, average='macro'\n",
    ")\n",
    "\n",
    "# Print Macro metrics\n",
    "print(\"\\nMacro Precision, Recall, F1 for Category:\")\n",
    "print(f\"Precision: {cat_precision}, Recall: {cat_recall}, F1: {cat_f1}\")\n",
    "\n",
    "print(\"\\nMacro Precision, Recall, F1 for Polarity:\")\n",
    "print(f\"Precision: {polarity_precision}, Recall: {polarity_recall}, F1: {polarity_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category distribution after upsampling:\n",
      "Category\n",
      "anecdotes/miscellaneous    2091\n",
      "ambience                   1929\n",
      "service                    1506\n",
      "price                      1394\n",
      "food                       1308\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Polarity distribution after upsampling:\n",
      "Polarity\n",
      "positive    2057\n",
      "neutral     2057\n",
      "conflict    2057\n",
      "negative    2057\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: (8228, 128)\n",
      "Attention masks shape: (8228, 128)\n",
      "Label 1 (Category) shape: (8228,)\n",
      "Label 2 (Polarity) shape: (8228,)\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, 128, 128)     15302016    ['input_ids[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 128, 256)     198144      ['embedding_8[0][0]']            \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 128)         123648      ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           8256        ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Category (Dense)               (None, 5)            325         ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " Polarity (Dense)               (None, 4)            260         ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,632,649\n",
      "Trainable params: 15,632,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "186/186 [==============================] - 17s 60ms/step - loss: 2.4103 - Category_loss: 1.2591 - Polarity_loss: 1.1511 - Category_accuracy: 0.4744 - Polarity_accuracy: 0.4565 - val_loss: 1.7654 - val_Category_loss: 0.9029 - val_Polarity_loss: 0.8625 - val_Category_accuracy: 0.6692 - val_Polarity_accuracy: 0.6328\n",
      "Epoch 2/10\n",
      "186/186 [==============================] - 10s 54ms/step - loss: 1.5730 - Category_loss: 0.7897 - Polarity_loss: 0.7833 - Category_accuracy: 0.7165 - Polarity_accuracy: 0.6765 - val_loss: 1.4049 - val_Category_loss: 0.6681 - val_Polarity_loss: 0.7368 - val_Category_accuracy: 0.7663 - val_Polarity_accuracy: 0.6631\n",
      "Epoch 3/10\n",
      "186/186 [==============================] - 10s 54ms/step - loss: 1.2828 - Category_loss: 0.6537 - Polarity_loss: 0.6290 - Category_accuracy: 0.7722 - Polarity_accuracy: 0.7447 - val_loss: 1.1328 - val_Category_loss: 0.5671 - val_Polarity_loss: 0.5656 - val_Category_accuracy: 0.8042 - val_Polarity_accuracy: 0.7709\n",
      "Epoch 4/10\n",
      "186/186 [==============================] - 10s 54ms/step - loss: 1.1020 - Category_loss: 0.5558 - Polarity_loss: 0.5463 - Category_accuracy: 0.8096 - Polarity_accuracy: 0.7923 - val_loss: 1.1091 - val_Category_loss: 0.5563 - val_Polarity_loss: 0.5528 - val_Category_accuracy: 0.8149 - val_Polarity_accuracy: 0.7860\n",
      "Epoch 5/10\n",
      "186/186 [==============================] - 9s 51ms/step - loss: 0.9629 - Category_loss: 0.5081 - Polarity_loss: 0.4548 - Category_accuracy: 0.8329 - Polarity_accuracy: 0.8335 - val_loss: 0.9989 - val_Category_loss: 0.5015 - val_Polarity_loss: 0.4974 - val_Category_accuracy: 0.8361 - val_Polarity_accuracy: 0.8331\n",
      "Epoch 6/10\n",
      "186/186 [==============================] - 9s 50ms/step - loss: 0.8297 - Category_loss: 0.4495 - Polarity_loss: 0.3802 - Category_accuracy: 0.8504 - Polarity_accuracy: 0.8707 - val_loss: 0.9031 - val_Category_loss: 0.4947 - val_Polarity_loss: 0.4083 - val_Category_accuracy: 0.8392 - val_Polarity_accuracy: 0.8483\n",
      "Epoch 7/10\n",
      "186/186 [==============================] - 9s 51ms/step - loss: 0.7619 - Category_loss: 0.4253 - Polarity_loss: 0.3366 - Category_accuracy: 0.8631 - Polarity_accuracy: 0.8842 - val_loss: 0.8947 - val_Category_loss: 0.4757 - val_Polarity_loss: 0.4190 - val_Category_accuracy: 0.8483 - val_Polarity_accuracy: 0.8543\n",
      "Epoch 8/10\n",
      "186/186 [==============================] - 9s 50ms/step - loss: 0.7035 - Category_loss: 0.4029 - Polarity_loss: 0.3006 - Category_accuracy: 0.8683 - Polarity_accuracy: 0.8992 - val_loss: 0.8487 - val_Category_loss: 0.5110 - val_Polarity_loss: 0.3377 - val_Category_accuracy: 0.8422 - val_Polarity_accuracy: 0.8877\n",
      "Epoch 9/10\n",
      "186/186 [==============================] - 9s 49ms/step - loss: 0.6350 - Category_loss: 0.3688 - Polarity_loss: 0.2663 - Category_accuracy: 0.8779 - Polarity_accuracy: 0.9131 - val_loss: 0.8063 - val_Category_loss: 0.4590 - val_Polarity_loss: 0.3473 - val_Category_accuracy: 0.8589 - val_Polarity_accuracy: 0.8877\n",
      "Epoch 10/10\n",
      "186/186 [==============================] - 9s 50ms/step - loss: 0.5691 - Category_loss: 0.3394 - Polarity_loss: 0.2297 - Category_accuracy: 0.8904 - Polarity_accuracy: 0.9240 - val_loss: 0.7461 - val_Category_loss: 0.4219 - val_Polarity_loss: 0.3242 - val_Category_accuracy: 0.8725 - val_Polarity_accuracy: 0.8983\n",
      "52/52 [==============================] - 1s 21ms/step - loss: 0.6839 - Category_loss: 0.4118 - Polarity_loss: 0.2721 - Category_accuracy: 0.8676 - Polarity_accuracy: 0.9156\n",
      "Test Loss and Accuracy: [0.6838600039482117, 0.4117687940597534, 0.2720913290977478, 0.8675577044487, 0.9155528545379639]\n",
      "52/52 [==============================] - 2s 21ms/step\n",
      "Classification Report for Category:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91       396\n",
      "           1       0.91      0.93      0.92       410\n",
      "           2       0.78      0.76      0.77       264\n",
      "           3       0.80      0.85      0.82       266\n",
      "           4       0.90      0.84      0.86       310\n",
      "\n",
      "    accuracy                           0.87      1646\n",
      "   macro avg       0.86      0.86      0.86      1646\n",
      "weighted avg       0.87      0.87      0.87      1646\n",
      "\n",
      "\n",
      "Classification Report for Polarity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       416\n",
      "           1       0.89      0.89      0.89       410\n",
      "           2       0.94      0.95      0.94       424\n",
      "           3       0.88      0.83      0.85       396\n",
      "\n",
      "    accuracy                           0.92      1646\n",
      "   macro avg       0.91      0.91      0.91      1646\n",
      "weighted avg       0.91      0.92      0.91      1646\n",
      "\n",
      "\n",
      "Macro-Averaged Scores for Category:\n",
      "Precision: 0.8586, Recall: 0.8572, F1 Score: 0.8575\n",
      "\n",
      "Macro-Averaged Scores for Polarity:\n",
      "Precision: 0.9139, Recall: 0.9142, F1 Score: 0.9138\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Layer, GRU, Bidirectional, Dense, Input, Reshape, GlobalAveragePooling1D\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv(r\"F:\\Context-Resonance Transformer\\Restuarant\\Restaurant - Sheet1.csv\")\n",
    "df.head()\n",
    "df = df[['Text', 'Category', 'Polarity']]\n",
    "df.head()\n",
    "df['Category'].value_counts()\n",
    "df['Polarity'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep only Bengali characters\n",
    "    text = re.sub(r'\\d+', '', text)                 # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "df.head()\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Category' and 'Polarity'\n",
    "df_upsampled_category = upsample(df, 'Category')\n",
    "df_upsampled_polarity = upsample(df_upsampled_category, 'Polarity')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled_polarity.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Category distribution after upsampling:\")\n",
    "print(df_upsampled['Category'].value_counts())\n",
    "print(\"\\nPolarity distribution after upsampling:\")\n",
    "print(df_upsampled['Polarity'].value_counts())\n",
    "\n",
    "df_upsampled.head()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "category_encoder = LabelEncoder()\n",
    "polarity_encoder = LabelEncoder()\n",
    "\n",
    "df_upsampled['Category_encoded'] = category_encoder.fit_transform(df_upsampled['Category'])\n",
    "df_upsampled['Polarity_encoded'] = polarity_encoder.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Tokenize the text using DistilBERT with padding and truncation\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')\n",
    "\n",
    "df_upsampled['tokens'] = df_upsampled['Text'].apply(lambda x: tokenize_function(x))\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df_upsampled, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Convert to TensorFlow Dataset\n",
    "def create_tensor_dataset(df):\n",
    "    # Tokenize input text and convert to TensorFlow tensors\n",
    "    inputs = tokenizer(list(df['Text']), padding='max_length', truncation=True, max_length=128, return_tensors='tf')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Convert labels to tensors\n",
    "    labels_category = tf.convert_to_tensor(df['Category_encoded'].values)\n",
    "    labels_polarity = tf.convert_to_tensor(df['Polarity_encoded'].values)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices(((input_ids, attention_mask), (labels_category, labels_polarity)))\n",
    "\n",
    "def tokenize_data(df_upsampled, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sentence in df_upsampled['Text']:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True, \n",
    "            max_length=max_length,    \n",
    "            padding='max_length',    \n",
    "            truncation=True,           \n",
    "            return_attention_mask=True, \n",
    "            return_tensors='tf'        \n",
    "        )\n",
    "        \n",
    "        # Append to lists\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    \n",
    "    # Squeeze the extra dimension\n",
    "    input_ids = tf.squeeze(input_ids, axis=1)\n",
    "    attention_masks = tf.squeeze(attention_masks, axis=1)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "input_ids, attention_masks = tokenize_data(df_upsampled)\n",
    "\n",
    "label_1 = tf.convert_to_tensor(df_upsampled['Category'])\n",
    "label_2 = tf.convert_to_tensor(df_upsampled['Polarity'])\n",
    "\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention masks shape: {attention_masks.shape}\")\n",
    "print(f\"Label 1 (Category) shape: {label_1.shape}\")\n",
    "print(f\"Label 2 (Polarity) shape: {label_2.shape}\")\n",
    "# Ensure input_ids and attention_masks are converted to integer type tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks, dtype=tf.int32)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoders for the string labels\n",
    "label_encoder_1 = LabelEncoder()\n",
    "label_encoder_2 = LabelEncoder()\n",
    "\n",
    "# Encode string labels into integers\n",
    "df_upsampled['Category'] = label_encoder_1.fit_transform(df_upsampled['Category'])\n",
    "df_upsampled['Polarity'] = label_encoder_2.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Convert labels to TensorFlow tensors\n",
    "label_1 = tf.convert_to_tensor(df_upsampled['Category'], dtype=tf.int32)\n",
    "label_2 = tf.convert_to_tensor(df_upsampled['Polarity'], dtype=tf.int32)\n",
    "\n",
    "# Ensure input_ids and attention_masks are correctly formatted as tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks, dtype=tf.int32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, att_mask_train, att_mask_test, y_cat_train, y_cat_test, y_gender_train, y_gender_test = train_test_split(\n",
    "    input_ids.numpy(), attention_masks.numpy(),\n",
    "    label_1.numpy(), label_2.numpy(),  \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_bilstm_model(input_shape):\n",
    "    input_ids = tf.keras.layers.Input(shape=(input_shape,), dtype='int32', name='input_ids')\n",
    "    attention_masks = tf.keras.layers.Input(shape=(input_shape,), dtype='int32', name='attention_masks')\n",
    "\n",
    "    # Embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=128)(input_ids)\n",
    "\n",
    "    # First BiLSTM layer with dropout\n",
    "    lstm_output = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(128, return_sequences=True, dropout=0.3))(embedding_layer)\n",
    "\n",
    "    # Second BiLSTM layer\n",
    "    lstm_output_2 = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(64, return_sequences=False, dropout=0.3))(lstm_output)\n",
    "\n",
    "    # Dense layer before output layers\n",
    "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(lstm_output_2)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    dropout_layer = tf.keras.layers.Dropout(0.3)(dense_layer)\n",
    "\n",
    "    # Output layers for multi-task learning\n",
    "    output_category = tf.keras.layers.Dense(5, activation='softmax', name='Category')(dropout_layer)\n",
    "    output_polarity = tf.keras.layers.Dense(4, activation='softmax', name='Polarity')(dropout_layer)\n",
    "\n",
    "    # Define the model with inputs and outputs\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_masks],\n",
    "                           outputs=[output_category, output_polarity])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the BiLSTM model\n",
    "bilstm_model = create_bilstm_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "bilstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'Category': 'sparse_categorical_crossentropy', 'Polarity': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'Category': 'accuracy', 'Polarity': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "bilstm_model.summary()\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "         \n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=3,          # Stop training after 3 epochs of no improvement\n",
    "    restore_best_weights=True  # Restore the best weights from the epoch with the lowest validation loss\n",
    ")\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "history = bilstm_model.fit(\n",
    "    [X_train, att_mask_train],  # Inputs\n",
    "    {'Category': y_cat_train, 'Polarity': y_gender_train},  # Outputs\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]  # Add the EarlyStopping callback\n",
    ")\n",
    "# Evaluate the model on the test set\n",
    "results = bilstm_model.evaluate(\n",
    "    [X_test, att_mask_test],\n",
    "    {'Category': y_cat_test, 'Polarity': y_gender_test}\n",
    ")\n",
    "\n",
    "print(f\"Test Loss and Accuracy: {results}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Get Predictions\n",
    "predictions = bilstm_model.predict([X_test, att_mask_test])\n",
    "\n",
    "# Step 2: Convert predictions to class labels\n",
    "y_cat_pred = np.argmax(predictions[0], axis=1)   # 'Category' prediction\n",
    "y_gender_pred = np.argmax(predictions[1], axis=1)  # 'Polarity' prediction\n",
    "\n",
    "# Step 3: Generate Classification Report with zero_division specified\n",
    "# For Category\n",
    "print(\"Classification Report for Category:\")\n",
    "print(classification_report(y_cat_test, y_cat_pred, zero_division=0))\n",
    "\n",
    "# For Polarity\n",
    "print(\"\\nClassification Report for Polarity:\")\n",
    "print(classification_report(y_gender_test, y_gender_pred, zero_division=0))\n",
    "\n",
    "# If you want the macro-averaged precision, recall, and F1 scores separately:\n",
    "cat_precision = precision_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "cat_recall = recall_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "cat_f1 = f1_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "\n",
    "gender_precision = precision_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "gender_recall = recall_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "gender_f1 = f1_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\nMacro-Averaged Scores for Category:\")\n",
    "print(f\"Precision: {cat_precision:.4f}, Recall: {cat_recall:.4f}, F1 Score: {cat_f1:.4f}\")\n",
    "\n",
    "print(\"\\nMacro-Averaged Scores for Polarity:\")\n",
    "print(f\"Precision: {gender_precision:.4f}, Recall: {gender_recall:.4f}, F1 Score: {gender_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category distribution after upsampling:\n",
      "Category\n",
      "anecdotes/miscellaneous    2091\n",
      "ambience                   1929\n",
      "service                    1506\n",
      "price                      1394\n",
      "food                       1308\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Polarity distribution after upsampling:\n",
      "Polarity\n",
      "positive    2057\n",
      "neutral     2057\n",
      "conflict    2057\n",
      "negative    2057\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: (8228, 128)\n",
      "Attention masks shape: (8228, 128)\n",
      "Label 1 (Category) shape: (8228,)\n",
      "Label 2 (Polarity) shape: (8228,)\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 128, 128)     15302016    ['input_ids[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 128, 256)    263168      ['embedding_9[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 128)         164352      ['bidirectional_2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Category (Dense)               (None, 5)            325         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " Polarity (Dense)               (None, 4)            260         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,738,377\n",
      "Trainable params: 15,738,377\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "186/186 [==============================] - 18s 73ms/step - loss: 2.5756 - Category_loss: 1.3521 - Polarity_loss: 1.2235 - Category_accuracy: 0.4341 - Polarity_accuracy: 0.4234 - val_loss: 2.1404 - val_Category_loss: 1.0847 - val_Polarity_loss: 1.0557 - val_Category_accuracy: 0.5888 - val_Polarity_accuracy: 0.5205\n",
      "Epoch 2/10\n",
      "186/186 [==============================] - 11s 58ms/step - loss: 1.8444 - Category_loss: 0.9221 - Polarity_loss: 0.9223 - Category_accuracy: 0.6574 - Polarity_accuracy: 0.6058 - val_loss: 1.5428 - val_Category_loss: 0.7691 - val_Polarity_loss: 0.7737 - val_Category_accuracy: 0.7238 - val_Polarity_accuracy: 0.6616\n",
      "Epoch 3/10\n",
      "186/186 [==============================] - 11s 58ms/step - loss: 1.3611 - Category_loss: 0.7158 - Polarity_loss: 0.6453 - Category_accuracy: 0.7456 - Polarity_accuracy: 0.7422 - val_loss: 1.1717 - val_Category_loss: 0.6013 - val_Polarity_loss: 0.5704 - val_Category_accuracy: 0.7906 - val_Polarity_accuracy: 0.7997\n",
      "Epoch 4/10\n",
      "186/186 [==============================] - 11s 59ms/step - loss: 1.0169 - Category_loss: 0.5572 - Polarity_loss: 0.4596 - Category_accuracy: 0.8212 - Polarity_accuracy: 0.8374 - val_loss: 0.9431 - val_Category_loss: 0.4983 - val_Polarity_loss: 0.4448 - val_Category_accuracy: 0.8346 - val_Polarity_accuracy: 0.8346\n",
      "Epoch 5/10\n",
      "186/186 [==============================] - 11s 59ms/step - loss: 0.8284 - Category_loss: 0.4566 - Polarity_loss: 0.3718 - Category_accuracy: 0.8575 - Polarity_accuracy: 0.8741 - val_loss: 0.9009 - val_Category_loss: 0.4939 - val_Polarity_loss: 0.4070 - val_Category_accuracy: 0.8422 - val_Polarity_accuracy: 0.8558\n",
      "Epoch 6/10\n",
      "186/186 [==============================] - 11s 58ms/step - loss: 0.7253 - Category_loss: 0.4285 - Polarity_loss: 0.2969 - Category_accuracy: 0.8627 - Polarity_accuracy: 0.9070 - val_loss: 0.7690 - val_Category_loss: 0.4824 - val_Polarity_loss: 0.2866 - val_Category_accuracy: 0.8437 - val_Polarity_accuracy: 0.8938\n",
      "Epoch 7/10\n",
      "186/186 [==============================] - 11s 58ms/step - loss: 0.6263 - Category_loss: 0.3808 - Polarity_loss: 0.2456 - Category_accuracy: 0.8749 - Polarity_accuracy: 0.9217 - val_loss: 0.7336 - val_Category_loss: 0.4002 - val_Polarity_loss: 0.3334 - val_Category_accuracy: 0.8771 - val_Polarity_accuracy: 0.8998\n",
      "Epoch 8/10\n",
      "186/186 [==============================] - 11s 58ms/step - loss: 0.5882 - Category_loss: 0.3553 - Polarity_loss: 0.2329 - Category_accuracy: 0.8854 - Polarity_accuracy: 0.9269 - val_loss: 0.6959 - val_Category_loss: 0.3856 - val_Polarity_loss: 0.3102 - val_Category_accuracy: 0.8832 - val_Polarity_accuracy: 0.9105\n",
      "Epoch 9/10\n",
      "186/186 [==============================] - 11s 59ms/step - loss: 0.5188 - Category_loss: 0.3137 - Polarity_loss: 0.2051 - Category_accuracy: 0.8970 - Polarity_accuracy: 0.9372 - val_loss: 0.5948 - val_Category_loss: 0.3679 - val_Polarity_loss: 0.2269 - val_Category_accuracy: 0.8877 - val_Polarity_accuracy: 0.9211\n",
      "Epoch 10/10\n",
      "186/186 [==============================] - 11s 57ms/step - loss: 0.4928 - Category_loss: 0.3121 - Polarity_loss: 0.1807 - Category_accuracy: 0.8979 - Polarity_accuracy: 0.9458 - val_loss: 0.6384 - val_Category_loss: 0.3561 - val_Polarity_loss: 0.2823 - val_Category_accuracy: 0.8741 - val_Polarity_accuracy: 0.9211\n",
      "52/52 [==============================] - 1s 24ms/step - loss: 0.6203 - Category_loss: 0.3782 - Polarity_loss: 0.2421 - Category_accuracy: 0.8773 - Polarity_accuracy: 0.9326\n",
      "Test Loss and Accuracy: [0.6202530264854431, 0.3781528174877167, 0.24210035800933838, 0.8772782683372498, 0.9325637817382812]\n",
      "52/52 [==============================] - 2s 22ms/step\n",
      "Classification Report for Category:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       396\n",
      "           1       0.95      0.91      0.93       410\n",
      "           2       0.81      0.78      0.80       264\n",
      "           3       0.80      0.93      0.86       266\n",
      "           4       0.87      0.85      0.86       310\n",
      "\n",
      "    accuracy                           0.88      1646\n",
      "   macro avg       0.87      0.87      0.87      1646\n",
      "weighted avg       0.88      0.88      0.88      1646\n",
      "\n",
      "\n",
      "Classification Report for Polarity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       416\n",
      "           1       0.93      0.91      0.92       410\n",
      "           2       0.95      0.97      0.96       424\n",
      "           3       0.93      0.85      0.89       396\n",
      "\n",
      "    accuracy                           0.93      1646\n",
      "   macro avg       0.93      0.93      0.93      1646\n",
      "weighted avg       0.93      0.93      0.93      1646\n",
      "\n",
      "\n",
      "Macro-Averaged Scores for Category:\n",
      "Precision: 0.8693, Recall: 0.8727, F1 Score: 0.8699\n",
      "\n",
      "Macro-Averaged Scores for Polarity:\n",
      "Precision: 0.9326, Recall: 0.9313, F1 Score: 0.9310\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Layer, GRU, Bidirectional, Dense, Input, Reshape, GlobalAveragePooling1D\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv(r\"F:\\Context-Resonance Transformer\\Restuarant\\Restaurant - Sheet1.csv\")\n",
    "df.head()\n",
    "df = df[['Text', 'Category', 'Polarity']]\n",
    "df.head()\n",
    "df['Category'].value_counts()\n",
    "df['Polarity'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep only Bengali characters\n",
    "    text = re.sub(r'\\d+', '', text)                 # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "df.head()\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Category' and 'Polarity'\n",
    "df_upsampled_category = upsample(df, 'Category')\n",
    "df_upsampled_polarity = upsample(df_upsampled_category, 'Polarity')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled_polarity.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Category distribution after upsampling:\")\n",
    "print(df_upsampled['Category'].value_counts())\n",
    "print(\"\\nPolarity distribution after upsampling:\")\n",
    "print(df_upsampled['Polarity'].value_counts())\n",
    "\n",
    "df_upsampled.head()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "category_encoder = LabelEncoder()\n",
    "polarity_encoder = LabelEncoder()\n",
    "\n",
    "df_upsampled['Category_encoded'] = category_encoder.fit_transform(df_upsampled['Category'])\n",
    "df_upsampled['Polarity_encoded'] = polarity_encoder.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Tokenize the text using DistilBERT with padding and truncation\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')\n",
    "\n",
    "df_upsampled['tokens'] = df_upsampled['Text'].apply(lambda x: tokenize_function(x))\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df_upsampled, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Convert to TensorFlow Dataset\n",
    "def create_tensor_dataset(df):\n",
    "    # Tokenize input text and convert to TensorFlow tensors\n",
    "    inputs = tokenizer(list(df['Text']), padding='max_length', truncation=True, max_length=128, return_tensors='tf')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Convert labels to tensors\n",
    "    labels_category = tf.convert_to_tensor(df['Category_encoded'].values)\n",
    "    labels_polarity = tf.convert_to_tensor(df['Polarity_encoded'].values)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices(((input_ids, attention_mask), (labels_category, labels_polarity)))\n",
    "\n",
    "def tokenize_data(df_upsampled, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sentence in df_upsampled['Text']:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True, \n",
    "            max_length=max_length,    \n",
    "            padding='max_length',    \n",
    "            truncation=True,           \n",
    "            return_attention_mask=True, \n",
    "            return_tensors='tf'        \n",
    "        )\n",
    "        \n",
    "        # Append to lists\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    \n",
    "    # Squeeze the extra dimension\n",
    "    input_ids = tf.squeeze(input_ids, axis=1)\n",
    "    attention_masks = tf.squeeze(attention_masks, axis=1)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "input_ids, attention_masks = tokenize_data(df_upsampled)\n",
    "\n",
    "label_1 = tf.convert_to_tensor(df_upsampled['Category'])\n",
    "label_2 = tf.convert_to_tensor(df_upsampled['Polarity'])\n",
    "\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention masks shape: {attention_masks.shape}\")\n",
    "print(f\"Label 1 (Category) shape: {label_1.shape}\")\n",
    "print(f\"Label 2 (Polarity) shape: {label_2.shape}\")\n",
    "# Ensure input_ids and attention_masks are converted to integer type tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks, dtype=tf.int32)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoders for the string labels\n",
    "label_encoder_1 = LabelEncoder()\n",
    "label_encoder_2 = LabelEncoder()\n",
    "\n",
    "# Encode string labels into integers\n",
    "df_upsampled['Category'] = label_encoder_1.fit_transform(df_upsampled['Category'])\n",
    "df_upsampled['Polarity'] = label_encoder_2.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Convert labels to TensorFlow tensors\n",
    "label_1 = tf.convert_to_tensor(df_upsampled['Category'], dtype=tf.int32)\n",
    "label_2 = tf.convert_to_tensor(df_upsampled['Polarity'], dtype=tf.int32)\n",
    "\n",
    "# Ensure input_ids and attention_masks are correctly formatted as tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks, dtype=tf.int32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, att_mask_train, att_mask_test, y_cat_train, y_cat_test, y_gender_train, y_gender_test = train_test_split(\n",
    "    input_ids.numpy(), attention_masks.numpy(),\n",
    "    label_1.numpy(), label_2.numpy(),  \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_bilstm_model(input_shape):\n",
    "    input_ids = tf.keras.layers.Input(shape=(input_shape,), dtype='int32', name='input_ids')\n",
    "    attention_masks = tf.keras.layers.Input(shape=(input_shape,), dtype='int32', name='attention_masks')\n",
    "\n",
    "    # Embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=128)(input_ids)\n",
    "\n",
    "    # First BiLSTM layer with dropout\n",
    "    lstm_output = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3))(embedding_layer)\n",
    "\n",
    "    # Second BiLSTM layer\n",
    "    lstm_output_2 = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False, dropout=0.3))(lstm_output)\n",
    "\n",
    "    # Dense layer before output layers\n",
    "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(lstm_output_2)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    dropout_layer = tf.keras.layers.Dropout(0.3)(dense_layer)\n",
    "\n",
    "    # Output layers for multi-task learning\n",
    "    output_category = tf.keras.layers.Dense(5, activation='softmax', name='Category')(dropout_layer)\n",
    "    output_polarity = tf.keras.layers.Dense(4, activation='softmax', name='Polarity')(dropout_layer)\n",
    "\n",
    "    # Define the model with inputs and outputs\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_masks],\n",
    "                           outputs=[output_category, output_polarity])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the BiLSTM model\n",
    "bilstm_model = create_bilstm_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "bilstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'Category': 'sparse_categorical_crossentropy', 'Polarity': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'Category': 'accuracy', 'Polarity': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "bilstm_model.summary()\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "         \n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=3,          # Stop training after 3 epochs of no improvement\n",
    "    restore_best_weights=True  # Restore the best weights from the epoch with the lowest validation loss\n",
    ")\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "history = bilstm_model.fit(\n",
    "    [X_train, att_mask_train],  # Inputs\n",
    "    {'Category': y_cat_train, 'Polarity': y_gender_train},  # Outputs\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]  # Add the EarlyStopping callback\n",
    ")\n",
    "# Evaluate the model on the test set\n",
    "results = bilstm_model.evaluate(\n",
    "    [X_test, att_mask_test],\n",
    "    {'Category': y_cat_test, 'Polarity': y_gender_test}\n",
    ")\n",
    "\n",
    "print(f\"Test Loss and Accuracy: {results}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Get Predictions\n",
    "predictions = bilstm_model.predict([X_test, att_mask_test])\n",
    "\n",
    "# Step 2: Convert predictions to class labels\n",
    "y_cat_pred = np.argmax(predictions[0], axis=1)   # 'Category' prediction\n",
    "y_gender_pred = np.argmax(predictions[1], axis=1)  # 'Polarity' prediction\n",
    "\n",
    "# Step 3: Generate Classification Report with zero_division specified\n",
    "# For Category\n",
    "print(\"Classification Report for Category:\")\n",
    "print(classification_report(y_cat_test, y_cat_pred, zero_division=0))\n",
    "\n",
    "# For Polarity\n",
    "print(\"\\nClassification Report for Polarity:\")\n",
    "print(classification_report(y_gender_test, y_gender_pred, zero_division=0))\n",
    "\n",
    "# If you want the macro-averaged precision, recall, and F1 scores separately:\n",
    "cat_precision = precision_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "cat_recall = recall_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "cat_f1 = f1_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "\n",
    "gender_precision = precision_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "gender_recall = recall_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "gender_f1 = f1_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\nMacro-Averaged Scores for Category:\")\n",
    "print(f\"Precision: {cat_precision:.4f}, Recall: {cat_recall:.4f}, F1 Score: {cat_f1:.4f}\")\n",
    "\n",
    "print(\"\\nMacro-Averaged Scores for Polarity:\")\n",
    "print(f\"Precision: {gender_precision:.4f}, Recall: {gender_recall:.4f}, F1 Score: {gender_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
