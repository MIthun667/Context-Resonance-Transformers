{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth for 1 GPU(s).\n",
      "Initial DataFrame:\n",
      "                                                Text                 Category\n",
      "0              স্টাফ কিন্তু, আমাদের জন্য ভয়ঙ্কর ছিল।                  service\n",
      "1  শুধুমাত্র,রিডামিং ফ্যাক্টর খাদ্য ছিল,পুরোপুরি ...                     food\n",
      "2  শুধুমাত্র,রিডামিং ফ্যাক্টর খাদ্য ছিল,পুরোপুরি ...  anecdotes/miscellaneous\n",
      "3  খাবার একদমই ব্যতিক্রমী, একটি খুব সক্ষম রান্নাঘ...                     food\n",
      "4  যেখানে গাব্রিয়েলা লোকালি আপনাকে শুভেচ্ছা জানা...                  service\n",
      "Initial Data Shape: (2059, 2)\n",
      "DataFrame after text cleaning:\n",
      "                                                Text                 Category\n",
      "0                                       স্টাফ ভয়ঙ্কর                  service\n",
      "1  শুধুমাত্ররিডামিং ফ্যাক্টর খাদ্য ছিলপুরোপুরি ন্...                     food\n",
      "2  শুধুমাত্ররিডামিং ফ্যাক্টর খাদ্য ছিলপুরোপুরি ন্...  anecdotes/miscellaneous\n",
      "3  খাবার একদমই ব্যতিক্রমী সক্ষম রান্নাঘর গর্বের খ...                     food\n",
      "4  গাব্রিয়েলা লোকালি আপনাকে শুভেচ্ছা আপনাকে খেতে...                  service\n",
      "Category distribution after upsampling:\n",
      "Category\n",
      "service                    710\n",
      "anecdotes/miscellaneous    710\n",
      "food                       710\n",
      "price                      710\n",
      "ambience                   710\n",
      "Name: count, dtype: int64\n",
      "Encoded Category:\n",
      "                  Category  Category_encoded\n",
      "0                  service                 4\n",
      "1  anecdotes/miscellaneous                 1\n",
      "2                     food                 2\n",
      "3                    price                 3\n",
      "4                 ambience                 0\n",
      "\n",
      "Tokenizing data for model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Tokenizing: 100%|██████████| 111/111 [00:00<00:00, 141.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing data for model: sagorsarker/bangla-bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 111/111 [00:00<00:00, 999.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model for: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: bert-base-multilingual-cased\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 35s 233ms/step - loss: 1.5845 - accuracy: 0.2687 - val_loss: 1.3861 - val_accuracy: 0.4479\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 18s 205ms/step - loss: 1.2538 - accuracy: 0.5109 - val_loss: 1.0172 - val_accuracy: 0.5972\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 18s 205ms/step - loss: 0.8970 - accuracy: 0.6926 - val_loss: 0.8610 - val_accuracy: 0.6958\n",
      "\n",
      "Evaluating model: bert-base-multilingual-cased\n",
      "23/23 [==============================] - 4s 59ms/step\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 371\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m history, pred_categories \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_ids_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_masks_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_category_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test_ids_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test_masks_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test_category_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[0;32m    382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# Save the model and tokenizer\u001b[39;00m\n\u001b[0;32m    385\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./fine_tuned_models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_category\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 345\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, X_train_ids, X_train_masks, y_train_cat, X_test_ids, X_test_masks, y_test_cat, model_name, epochs, batch_size)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    344\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: X_test_ids, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: X_test_masks})\n\u001b[1;32m--> 345\u001b[0m pred_categories \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# Category Evaluation\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCategory Classification Report for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# sentiment_analysis_single_task_finetune.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFBertModel,\n",
    ")\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK resources if not already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "# Note: NLTK may not have comprehensive Bengali stopwords. Consider using a custom list if needed.\n",
    "try:\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "except LookupError:\n",
    "    print(\"Bengali stopwords not found. Skipping stopword removal.\")\n",
    "    stop_words = set()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. GPU Memory Management\n",
    "# -------------------------------\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Enabled memory growth for {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "# Ensure the CSV has at least two columns: 'Text' and 'Category'\n",
    "data_path = r\"F:\\Context-Resonance Transformer\\Restuarant\\Restaurant - Sheet1.csv\"  # Update this path as needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['Text', 'Category']]\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"Initial Data Shape: {df.shape}\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Keep only Bengali characters: Unicode range for Bengali: \\u0980-\\u09FF\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    # Lemmatize and remove stopwords if available\n",
    "    if stop_words:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['Text'] = df['Text'].astype(str).apply(clean_text)\n",
    "print(\"DataFrame after text cleaning:\")\n",
    "print(df.head())\n",
    "\n",
    "# Upsampling 'Category' to balance classes\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Category'\n",
    "df_upsampled = upsample(df, 'Category')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Category distribution after upsampling:\")\n",
    "print(df_upsampled['Category'].value_counts())\n",
    "\n",
    "# Encode 'Category' labels\n",
    "category_encoder = LabelEncoder()\n",
    "df_upsampled['Category_encoded'] = category_encoder.fit_transform(df_upsampled['Category'])\n",
    "\n",
    "# Verify encoding\n",
    "print(\"Encoded Category:\")\n",
    "print(df_upsampled[['Category', 'Category_encoded']].head())\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Define the list of pre-trained models to fine-tune\n",
    "pretrained_models = {\n",
    "    'bert-base-multilingual-cased': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'bert-base-multilingual-cased'\n",
    "    },\n",
    "    'sagorsarker/bangla-bert-base': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'sagorsarker/bangla-bert-base'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define selected models\n",
    "selected_models = list(pretrained_models.keys())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenization\n",
    "# -------------------------------\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(sentences, tokenizer, max_len=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tokenizes sentences in batches for efficiency.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Tokenizing\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        try:\n",
    "            encoded = tokenizer(\n",
    "                list(batch),\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization for batch starting at index {i}: {e}\")\n",
    "\n",
    "    # Concatenate all batches\n",
    "    input_ids = tf.concat(input_ids, axis=0).numpy()\n",
    "    attention_masks = tf.concat(attention_masks, axis=0).numpy()\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Tokenize the data for each model and store in a dictionary\n",
    "tokenized_data = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    print(f\"\\nTokenizing data for model: {model_name}\")\n",
    "    tokenizer_class = pretrained_models[model_name]['tokenizer']\n",
    "    pretrained_name = pretrained_models[model_name]['pretrained_name']\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {model_name}: {e}\")\n",
    "        continue\n",
    "    input_ids, attention_masks = tokenize_sentences(df_upsampled['Text'].values, tokenizer, max_len=20, batch_size=32)\n",
    "    tokenized_data[model_name] = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Preparing Labels and Splits\n",
    "# -------------------------------\n",
    "\n",
    "# Define labels for single-task learning\n",
    "labels_category = df_upsampled['Category_encoded'].values\n",
    "\n",
    "# Split the data into training and testing sets for each model\n",
    "X_train_ids_dict = {}\n",
    "X_test_ids_dict = {}\n",
    "X_train_masks_dict = {}\n",
    "X_test_masks_dict = {}\n",
    "y_train_category_dict = {}\n",
    "y_test_category_dict = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "    X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train_cat, y_test_cat = train_test_split(\n",
    "        tokenized_data[model_name]['input_ids'],\n",
    "        tokenized_data[model_name]['attention_masks'],\n",
    "        labels_category,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_category\n",
    "    )\n",
    "    X_train_ids_dict[model_name] = X_train_ids\n",
    "    X_test_ids_dict[model_name] = X_test_ids\n",
    "    X_train_masks_dict[model_name] = X_train_masks\n",
    "    X_test_masks_dict[model_name] = X_test_masks\n",
    "    y_train_category_dict[model_name] = y_train_cat\n",
    "    y_test_category_dict[model_name] = y_test_cat\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Model Building, Training, and Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "# Number of classes\n",
    "num_categories = df_upsampled['Category_encoded'].nunique()\n",
    "\n",
    "# Function to build and compile the model\n",
    "def build_model(pretrained_model_info, num_categories, max_len=20):\n",
    "    \"\"\"\n",
    "    Builds a single-task model with shared pre-trained layers and a single output layer.\n",
    "    \"\"\"\n",
    "    tokenizer_class = pretrained_model_info['tokenizer']\n",
    "    model_class = pretrained_model_info['model']\n",
    "    pretrained_name = pretrained_model_info['pretrained_name']\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {pretrained_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the model with TensorFlow weights first\n",
    "        base_model = model_class.from_pretrained(pretrained_name)\n",
    "    except OSError:\n",
    "        # If TensorFlow weights are unavailable, try loading PyTorch weights\n",
    "        print(f\"TensorFlow weights not found for {pretrained_name}. Attempting to load PyTorch weights.\")\n",
    "        try:\n",
    "            base_model = model_class.from_pretrained(pretrained_name, from_pt=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model for {pretrained_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # Get base model outputs\n",
    "    base_outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = base_outputs[1]\n",
    "\n",
    "    # Shared Dense layer\n",
    "    shared_dense = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    shared_dense = tf.keras.layers.Dropout(0.3)(shared_dense)\n",
    "\n",
    "    # Category output\n",
    "    category_output = tf.keras.layers.Dense(num_categories, activation='softmax', name='category')(shared_dense)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[category_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss={\n",
    "            'category': 'sparse_categorical_crossentropy',\n",
    "        },\n",
    "        metrics={\n",
    "            'category': 'accuracy',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model, X_train_ids, X_train_masks, y_train_cat,\n",
    "                       X_test_ids, X_test_masks, y_test_cat, model_name, epochs=3, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    history = model.fit(\n",
    "        {'input_ids': X_train_ids, 'attention_mask': X_train_masks},\n",
    "        {'category': y_train_cat},\n",
    "        validation_data=(\n",
    "            {'input_ids': X_test_ids, 'attention_mask': X_test_masks},\n",
    "            {'category': y_test_cat}\n",
    "        ),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    predictions = model.predict({'input_ids': X_test_ids, 'attention_mask': X_test_masks})\n",
    "    pred_categories = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "    # Category Evaluation\n",
    "    print(f\"\\nCategory Classification Report for {model_name}:\")\n",
    "    print(classification_report(y_test_cat, pred_categories, target_names=category_encoder.classes_))\n",
    "\n",
    "    # Return history and predictions if needed\n",
    "    return history, pred_categories\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nBuilding model for: {model_name}\")\n",
    "    pretrained_model_info = pretrained_models[model_name]\n",
    "    model, tokenizer = build_model(pretrained_model_info, num_categories, max_len=20)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"Skipping training for {model_name} due to build errors.\")\n",
    "        continue\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    history, pred_categories = train_and_evaluate(\n",
    "        model,\n",
    "        X_train_ids_dict[model_name],\n",
    "        X_train_masks_dict[model_name],\n",
    "        y_train_category_dict[model_name],\n",
    "        X_test_ids_dict[model_name],\n",
    "        X_test_masks_dict[model_name],\n",
    "        y_test_category_dict[model_name],\n",
    "        model_name,\n",
    "        epochs=3,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    save_dir = f'./fine_tuned_models/{model_name.replace(\"/\", \"_\")}_category'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    try:\n",
    "        model.save(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Model and tokenizer saved to {save_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model for {model_name}: {e}\")\n",
    "\n",
    "    # Store results\n",
    "    model_results[model_name] = {\n",
    "        'history': history,\n",
    "        'pred_categories': pred_categories\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models have been trained and evaluated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Optional: Compare Model Performances\n",
    "# -------------------------------\n",
    "\n",
    "# Example: Plotting category accuracy for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "model_labels = []\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in model_results:\n",
    "        continue\n",
    "    history = model_results[model_name]['history']\n",
    "    train_acc.append(history.history['category_accuracy'][-1])\n",
    "    val_acc.append(history.history['val_category_accuracy'][-1])\n",
    "    model_labels.append(model_name)\n",
    "\n",
    "x = np.arange(len(model_labels))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, train_acc, width, label='Train Accuracy')\n",
    "rects2 = ax.bar(x + width/2, val_acc, width, label='Validation Accuracy')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Category Classification Accuracy by Model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Attach a text label above each bar\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth for 1 GPU(s).\n",
      "Initial DataFrame:\n",
      "                                                Text                 Category\n",
      "0              স্টাফ কিন্তু, আমাদের জন্য ভয়ঙ্কর ছিল।                  service\n",
      "1  শুধুমাত্র,রিডামিং ফ্যাক্টর খাদ্য ছিল,পুরোপুরি ...                     food\n",
      "2  শুধুমাত্র,রিডামিং ফ্যাক্টর খাদ্য ছিল,পুরোপুরি ...  anecdotes/miscellaneous\n",
      "3  খাবার একদমই ব্যতিক্রমী, একটি খুব সক্ষম রান্নাঘ...                     food\n",
      "4  যেখানে গাব্রিয়েলা লোকালি আপনাকে শুভেচ্ছা জানা...                  service\n",
      "Initial Data Shape: (2059, 2)\n",
      "DataFrame after text cleaning:\n",
      "                                                Text                 Category\n",
      "0                                       স্টাফ ভয়ঙ্কর                  service\n",
      "1  শুধুমাত্ররিডামিং ফ্যাক্টর খাদ্য ছিলপুরোপুরি ন্...                     food\n",
      "2  শুধুমাত্ররিডামিং ফ্যাক্টর খাদ্য ছিলপুরোপুরি ন্...  anecdotes/miscellaneous\n",
      "3  খাবার একদমই ব্যতিক্রমী সক্ষম রান্নাঘর গর্বের খ...                     food\n",
      "4  গাব্রিয়েলা লোকালি আপনাকে শুভেচ্ছা আপনাকে খেতে...                  service\n",
      "Category distribution after upsampling:\n",
      "Category\n",
      "service                    710\n",
      "anecdotes/miscellaneous    710\n",
      "food                       710\n",
      "price                      710\n",
      "ambience                   710\n",
      "Name: count, dtype: int64\n",
      "Encoded Category:\n",
      "                  Category  Category_encoded\n",
      "0                  service                 4\n",
      "1  anecdotes/miscellaneous                 1\n",
      "2                     food                 2\n",
      "3                    price                 3\n",
      "4                 ambience                 0\n",
      "Number of unique categories: 5\n",
      "\n",
      "Tokenizing data for model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Tokenizing: 100%|██████████| 111/111 [00:00<00:00, 924.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing data for model: sagorsarker/bangla-bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 111/111 [00:00<00:00, 1037.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model for: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: bert-base-multilingual-cased\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 35s 237ms/step - loss: 1.5812 - accuracy: 0.2687 - val_loss: 1.3945 - val_accuracy: 0.4113\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 18s 205ms/step - loss: 1.2522 - accuracy: 0.5014 - val_loss: 1.0273 - val_accuracy: 0.6268\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 18s 206ms/step - loss: 0.8784 - accuracy: 0.6961 - val_loss: 0.8498 - val_accuracy: 0.7028\n",
      "\n",
      "Evaluating model: bert-base-multilingual-cased\n",
      "23/23 [==============================] - 4s 57ms/step\n",
      "\n",
      "Category Classification Report for bert-base-multilingual-cased:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               ambience       0.70      0.62      0.66       142\n",
      "anecdotes/miscellaneous       0.74      0.73      0.74       142\n",
      "                   food       0.60      0.61      0.60       142\n",
      "                  price       0.80      0.84      0.82       142\n",
      "                service       0.68      0.72      0.70       142\n",
      "\n",
      "               accuracy                           0.70       710\n",
      "              macro avg       0.70      0.70      0.70       710\n",
      "           weighted avg       0.70      0.70      0.70       710\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_models/bert-base-multilingual-cased_category\n",
      "\n",
      "Building model for: sagorsarker/bangla-bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: sagorsarker/bangla-bert-base\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 34s 226ms/step - loss: 1.3714 - accuracy: 0.4320 - val_loss: 1.0119 - val_accuracy: 0.6423\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 16s 177ms/step - loss: 0.8606 - accuracy: 0.6838 - val_loss: 0.8039 - val_accuracy: 0.7380\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 15s 165ms/step - loss: 0.5827 - accuracy: 0.7989 - val_loss: 0.7200 - val_accuracy: 0.7704\n",
      "\n",
      "Evaluating model: sagorsarker/bangla-bert-base\n",
      "23/23 [==============================] - 4s 51ms/step\n",
      "\n",
      "Category Classification Report for sagorsarker/bangla-bert-base:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               ambience       0.68      0.82      0.75       142\n",
      "anecdotes/miscellaneous       0.77      0.82      0.80       142\n",
      "                   food       0.77      0.60      0.67       142\n",
      "                  price       0.78      0.85      0.81       142\n",
      "                service       0.87      0.76      0.81       142\n",
      "\n",
      "               accuracy                           0.77       710\n",
      "              macro avg       0.78      0.77      0.77       710\n",
      "           weighted avg       0.78      0.77      0.77       710\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_models/sagorsarker_bangla-bert-base_category\n",
      "\n",
      "All models have been trained and evaluated.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'category_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 421\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    420\u001b[0m history \u001b[38;5;241m=\u001b[39m model_results[model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 421\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mappend(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    422\u001b[0m val_acc\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_category_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    423\u001b[0m model_labels\u001b[38;5;241m.\u001b[39mappend(model_name)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'category_accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentiment_analysis_single_task_finetune.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFBertModel,\n",
    ")\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK resources if not already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "# Note: NLTK may not have comprehensive Bengali stopwords. Consider using a custom list if needed.\n",
    "try:\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "except LookupError:\n",
    "    print(\"Bengali stopwords not found. Skipping stopword removal.\")\n",
    "    stop_words = set()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. GPU Memory Management\n",
    "# -------------------------------\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Enabled memory growth for {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "# Ensure the CSV has at least two columns: 'Text' and 'Category'\n",
    "data_path = r\"F:\\Context-Resonance Transformer\\Restuarant\\Restaurant - Sheet1.csv\"  # Update this path as needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['Text', 'Category']]\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"Initial Data Shape: {df.shape}\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Keep only Bengali characters: Unicode range for Bengali: \\u0980-\\u09FF\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    # Lemmatize and remove stopwords if available\n",
    "    if stop_words:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['Text'] = df['Text'].astype(str).apply(clean_text)\n",
    "print(\"DataFrame after text cleaning:\")\n",
    "print(df.head())\n",
    "\n",
    "# Upsampling 'Category' to balance classes\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Category'\n",
    "df_upsampled = upsample(df, 'Category')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Category distribution after upsampling:\")\n",
    "print(df_upsampled['Category'].value_counts())\n",
    "\n",
    "# Encode 'Category' labels\n",
    "category_encoder = LabelEncoder()\n",
    "df_upsampled['Category_encoded'] = category_encoder.fit_transform(df_upsampled['Category'])\n",
    "\n",
    "# Verify encoding\n",
    "print(\"Encoded Category:\")\n",
    "print(df_upsampled[['Category', 'Category_encoded']].head())\n",
    "\n",
    "# Verify number of unique classes\n",
    "num_categories = df_upsampled['Category_encoded'].nunique()\n",
    "print(f\"Number of unique categories: {num_categories}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Define the list of pre-trained models to fine-tune\n",
    "pretrained_models = {\n",
    "    'bert-base-multilingual-cased': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'bert-base-multilingual-cased'\n",
    "    },\n",
    "    'sagorsarker/bangla-bert-base': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'sagorsarker/bangla-bert-base'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define selected models\n",
    "selected_models = list(pretrained_models.keys())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenization\n",
    "# -------------------------------\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(sentences, tokenizer, max_len=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tokenizes sentences in batches for efficiency.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Tokenizing\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        try:\n",
    "            encoded = tokenizer(\n",
    "                list(batch),\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization for batch starting at index {i}: {e}\")\n",
    "\n",
    "    # Concatenate all batches\n",
    "    input_ids = tf.concat(input_ids, axis=0).numpy()\n",
    "    attention_masks = tf.concat(attention_masks, axis=0).numpy()\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Tokenize the data for each model and store in a dictionary\n",
    "tokenized_data = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    print(f\"\\nTokenizing data for model: {model_name}\")\n",
    "    tokenizer_class = pretrained_models[model_name]['tokenizer']\n",
    "    pretrained_name = pretrained_models[model_name]['pretrained_name']\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {model_name}: {e}\")\n",
    "        continue\n",
    "    input_ids, attention_masks = tokenize_sentences(df_upsampled['Text'].values, tokenizer, max_len=20, batch_size=32)\n",
    "    tokenized_data[model_name] = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Preparing Labels and Splits\n",
    "# -------------------------------\n",
    "\n",
    "# Define labels for single-task learning\n",
    "labels_category = df_upsampled['Category_encoded'].values\n",
    "\n",
    "# Split the data into training and testing sets for each model\n",
    "X_train_ids_dict = {}\n",
    "X_test_ids_dict = {}\n",
    "X_train_masks_dict = {}\n",
    "X_test_masks_dict = {}\n",
    "y_train_category_dict = {}\n",
    "y_test_category_dict = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "    X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train_cat, y_test_cat = train_test_split(\n",
    "        tokenized_data[model_name]['input_ids'],\n",
    "        tokenized_data[model_name]['attention_masks'],\n",
    "        labels_category,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_category\n",
    "    )\n",
    "    X_train_ids_dict[model_name] = X_train_ids\n",
    "    X_test_ids_dict[model_name] = X_test_ids\n",
    "    X_train_masks_dict[model_name] = X_train_masks\n",
    "    X_test_masks_dict[model_name] = X_test_masks\n",
    "    y_train_category_dict[model_name] = y_train_cat\n",
    "    y_test_category_dict[model_name] = y_test_cat\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Model Building, Training, and Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "# Function to build and compile the model\n",
    "def build_model(pretrained_model_info, num_categories, max_len=20):\n",
    "    \"\"\"\n",
    "    Builds a single-task model with shared pre-trained layers and a single output layer.\n",
    "    \"\"\"\n",
    "    tokenizer_class = pretrained_model_info['tokenizer']\n",
    "    model_class = pretrained_model_info['model']\n",
    "    pretrained_name = pretrained_model_info['pretrained_name']\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {pretrained_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the model with TensorFlow weights first\n",
    "        base_model = model_class.from_pretrained(pretrained_name)\n",
    "    except OSError:\n",
    "        # If TensorFlow weights are unavailable, try loading PyTorch weights\n",
    "        print(f\"TensorFlow weights not found for {pretrained_name}. Attempting to load PyTorch weights.\")\n",
    "        try:\n",
    "            base_model = model_class.from_pretrained(pretrained_name, from_pt=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model for {pretrained_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # Get base model outputs\n",
    "    base_outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = base_outputs[1]  # Typically the [CLS] token representation\n",
    "\n",
    "    # Shared Dense layer\n",
    "    shared_dense = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    shared_dense = tf.keras.layers.Dropout(0.3)(shared_dense)\n",
    "\n",
    "    # Category output\n",
    "    category_output = tf.keras.layers.Dense(num_categories, activation='softmax', name='category')(shared_dense)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[category_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss={\n",
    "            'category': 'sparse_categorical_crossentropy',\n",
    "        },\n",
    "        metrics={\n",
    "            'category': 'accuracy',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model, X_train_ids, X_train_masks, y_train_cat,\n",
    "                       X_test_ids, X_test_masks, y_test_cat, model_name, epochs=3, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    history = model.fit(\n",
    "        {'input_ids': X_train_ids, 'attention_mask': X_train_masks},\n",
    "        {'category': y_train_cat},\n",
    "        validation_data=(\n",
    "            {'input_ids': X_test_ids, 'attention_mask': X_test_masks},\n",
    "            {'category': y_test_cat}\n",
    "        ),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    predictions = model.predict({'input_ids': X_test_ids, 'attention_mask': X_test_masks})\n",
    "    \n",
    "    # For single-output models, predictions is a single NumPy array\n",
    "    # Apply argmax directly on the predictions array\n",
    "    pred_categories = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Category Evaluation\n",
    "    print(f\"\\nCategory Classification Report for {model_name}:\")\n",
    "    print(classification_report(y_test_cat, pred_categories, target_names=category_encoder.classes_))\n",
    "\n",
    "    # Return history and predictions if needed\n",
    "    return history, pred_categories\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nBuilding model for: {model_name}\")\n",
    "    pretrained_model_info = pretrained_models[model_name]\n",
    "    model, tokenizer = build_model(pretrained_model_info, num_categories, max_len=20)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"Skipping training for {model_name} due to build errors.\")\n",
    "        continue\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    history, pred_categories = train_and_evaluate(\n",
    "        model,\n",
    "        X_train_ids_dict[model_name],\n",
    "        X_train_masks_dict[model_name],\n",
    "        y_train_category_dict[model_name],\n",
    "        X_test_ids_dict[model_name],\n",
    "        X_test_masks_dict[model_name],\n",
    "        y_test_category_dict[model_name],\n",
    "        model_name,\n",
    "        epochs=3,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    save_dir = f'./fine_tuned_models/{model_name.replace(\"/\", \"_\")}_category'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    try:\n",
    "        model.save(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Model and tokenizer saved to {save_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model for {model_name}: {e}\")\n",
    "\n",
    "    # Store results\n",
    "    model_results[model_name] = {\n",
    "        'history': history,\n",
    "        'pred_categories': pred_categories\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models have been trained and evaluated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Optional: Compare Model Performances\n",
    "# -------------------------------\n",
    "\n",
    "# Example: Plotting category accuracy for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "model_labels = []\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in model_results:\n",
    "        continue\n",
    "    history = model_results[model_name]['history']\n",
    "    train_acc.append(history.history['category_accuracy'][-1])\n",
    "    val_acc.append(history.history['val_category_accuracy'][-1])\n",
    "    model_labels.append(model_name)\n",
    "\n",
    "x = np.arange(len(model_labels))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, train_acc, width, label='Train Accuracy')\n",
    "rects2 = ax.bar(x + width/2, val_acc, width, label='Validation Accuracy')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Category Classification Accuracy by Model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Attach a text label above each bar\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth for 1 GPU(s).\n",
      "Initial DataFrame:\n",
      "                                                Text  Polarity\n",
      "0              স্টাফ কিন্তু, আমাদের জন্য ভয়ঙ্কর ছিল।  negative\n",
      "1  শুধুমাত্র,রিডামিং ফ্যাক্টর খাদ্য ছিল,পুরোপুরি ...  positive\n",
      "2  শুধুমাত্র,রিডামিং ফ্যাক্টর খাদ্য ছিল,পুরোপুরি ...  negative\n",
      "3  খাবার একদমই ব্যতিক্রমী, একটি খুব সক্ষম রান্নাঘ...  positive\n",
      "4  যেখানে গাব্রিয়েলা লোকালি আপনাকে শুভেচ্ছা জানা...  positive\n",
      "Initial Data Shape: (2059, 2)\n",
      "DataFrame after text cleaning:\n",
      "                                                Text  Polarity\n",
      "0                                       স্টাফ ভয়ঙ্কর  negative\n",
      "1  শুধুমাত্ররিডামিং ফ্যাক্টর খাদ্য ছিলপুরোপুরি ন্...  positive\n",
      "2  শুধুমাত্ররিডামিং ফ্যাক্টর খাদ্য ছিলপুরোপুরি ন্...  negative\n",
      "3  খাবার একদমই ব্যতিক্রমী সক্ষম রান্নাঘর গর্বের খ...  positive\n",
      "4  গাব্রিয়েলা লোকালি আপনাকে শুভেচ্ছা আপনাকে খেতে...  positive\n",
      "Polarity distribution after upsampling:\n",
      "Polarity\n",
      "negative    1221\n",
      "positive    1221\n",
      "conflict    1221\n",
      "neutral     1221\n",
      "Name: count, dtype: int64\n",
      "Encoded Polarity:\n",
      "   Polarity  Polarity_encoded\n",
      "0  negative                 1\n",
      "1  negative                 1\n",
      "2  positive                 3\n",
      "3  conflict                 0\n",
      "4  positive                 3\n",
      "Number of unique polarities: 4\n",
      "\n",
      "Tokenizing data for model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Tokenizing: 100%|██████████| 153/153 [00:00<00:00, 915.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing data for model: sagorsarker/bangla-bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 153/153 [00:00<00:00, 718.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model for: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: bert-base-multilingual-cased\n",
      "Epoch 1/3\n",
      "123/123 [==============================] - 45s 227ms/step - loss: 1.2879 - accuracy: 0.3706 - val_loss: 1.0170 - val_accuracy: 0.5793\n",
      "Epoch 2/3\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.7422 - accuracy: 0.7072 - val_loss: 0.5633 - val_accuracy: 0.8035\n",
      "Epoch 3/3\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.4231 - accuracy: 0.8485 - val_loss: 0.4386 - val_accuracy: 0.8557\n",
      "\n",
      "Evaluating model: bert-base-multilingual-cased\n",
      "31/31 [==============================] - 4s 58ms/step\n",
      "\n",
      "Polarity Classification Report for bert-base-multilingual-cased:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.92      1.00      0.96       244\n",
      "    negative       0.87      0.77      0.82       244\n",
      "     neutral       0.78      0.94      0.86       245\n",
      "    positive       0.86      0.71      0.78       244\n",
      "\n",
      "    accuracy                           0.86       977\n",
      "   macro avg       0.86      0.86      0.85       977\n",
      "weighted avg       0.86      0.86      0.85       977\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_models/bert-base-multilingual-cased_polarity\n",
      "\n",
      "Building model for: sagorsarker/bangla-bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: sagorsarker/bangla-bert-base\n",
      "Epoch 1/3\n",
      "123/123 [==============================] - 43s 223ms/step - loss: 1.1166 - accuracy: 0.5058 - val_loss: 0.6779 - val_accuracy: 0.7564\n",
      "Epoch 2/3\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.5025 - accuracy: 0.8219 - val_loss: 0.3374 - val_accuracy: 0.8792\n",
      "Epoch 3/3\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.2659 - accuracy: 0.9155 - val_loss: 0.3035 - val_accuracy: 0.9038\n",
      "\n",
      "Evaluating model: sagorsarker/bangla-bert-base\n",
      "31/31 [==============================] - 4s 59ms/step\n",
      "\n",
      "Polarity Classification Report for sagorsarker/bangla-bert-base:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.95      1.00      0.98       244\n",
      "    negative       0.89      0.86      0.87       244\n",
      "     neutral       0.84      0.96      0.90       245\n",
      "    positive       0.96      0.80      0.87       244\n",
      "\n",
      "    accuracy                           0.90       977\n",
      "   macro avg       0.91      0.90      0.90       977\n",
      "weighted avg       0.91      0.90      0.90       977\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_models/sagorsarker_bangla-bert-base_polarity\n",
      "\n",
      "All models have been trained and evaluated.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'polarity_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 421\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    420\u001b[0m history \u001b[38;5;241m=\u001b[39m model_results[model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 421\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mappend(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolarity_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    422\u001b[0m val_acc\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_polarity_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    423\u001b[0m model_labels\u001b[38;5;241m.\u001b[39mappend(model_name)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'polarity_accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentiment_analysis_polarity_single_task_finetune.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFBertModel,\n",
    ")\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK resources if not already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "# Note: NLTK may not have comprehensive Bengali stopwords. Consider using a custom list if needed.\n",
    "try:\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "except LookupError:\n",
    "    print(\"Bengali stopwords not found. Skipping stopword removal.\")\n",
    "    stop_words = set()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. GPU Memory Management\n",
    "# -------------------------------\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Enabled memory growth for {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "# Ensure the CSV has at least two columns: 'Text' and 'Polarity'\n",
    "data_path = r\"F:\\Context-Resonance Transformer\\Restuarant\\Restaurant - Sheet1.csv\"  # Update this path as needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['Text', 'Polarity']]\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"Initial Data Shape: {df.shape}\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Keep only Bengali characters: Unicode range for Bengali: \\u0980-\\u09FF\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    # Lemmatize and remove stopwords if available\n",
    "    if stop_words:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['Text'] = df['Text'].astype(str).apply(clean_text)\n",
    "print(\"DataFrame after text cleaning:\")\n",
    "print(df.head())\n",
    "\n",
    "# Upsampling 'Polarity' to balance classes\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Polarity'\n",
    "df_upsampled = upsample(df, 'Polarity')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Polarity distribution after upsampling:\")\n",
    "print(df_upsampled['Polarity'].value_counts())\n",
    "\n",
    "# Encode 'Polarity' labels\n",
    "polarity_encoder = LabelEncoder()\n",
    "df_upsampled['Polarity_encoded'] = polarity_encoder.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Verify encoding\n",
    "print(\"Encoded Polarity:\")\n",
    "print(df_upsampled[['Polarity', 'Polarity_encoded']].head())\n",
    "\n",
    "# Verify number of unique classes\n",
    "num_polarities = df_upsampled['Polarity_encoded'].nunique()\n",
    "print(f\"Number of unique polarities: {num_polarities}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Define the list of pre-trained models to fine-tune\n",
    "pretrained_models = {\n",
    "    'bert-base-multilingual-cased': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'bert-base-multilingual-cased'\n",
    "    },\n",
    "    'sagorsarker/bangla-bert-base': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'sagorsarker/bangla-bert-base'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define selected models\n",
    "selected_models = list(pretrained_models.keys())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenization\n",
    "# -------------------------------\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(sentences, tokenizer, max_len=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tokenizes sentences in batches for efficiency.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Tokenizing\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        try:\n",
    "            encoded = tokenizer(\n",
    "                list(batch),\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization for batch starting at index {i}: {e}\")\n",
    "\n",
    "    # Concatenate all batches\n",
    "    input_ids = tf.concat(input_ids, axis=0).numpy()\n",
    "    attention_masks = tf.concat(attention_masks, axis=0).numpy()\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Tokenize the data for each model and store in a dictionary\n",
    "tokenized_data = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    print(f\"\\nTokenizing data for model: {model_name}\")\n",
    "    tokenizer_class = pretrained_models[model_name]['tokenizer']\n",
    "    pretrained_name = pretrained_models[model_name]['pretrained_name']\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {model_name}: {e}\")\n",
    "        continue\n",
    "    input_ids, attention_masks = tokenize_sentences(df_upsampled['Text'].values, tokenizer, max_len=20, batch_size=32)\n",
    "    tokenized_data[model_name] = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Preparing Labels and Splits\n",
    "# -------------------------------\n",
    "\n",
    "# Define labels for single-task learning\n",
    "labels_polarity = df_upsampled['Polarity_encoded'].values\n",
    "\n",
    "# Split the data into training and testing sets for each model\n",
    "X_train_ids_dict = {}\n",
    "X_test_ids_dict = {}\n",
    "X_train_masks_dict = {}\n",
    "X_test_masks_dict = {}\n",
    "y_train_polarity_dict = {}\n",
    "y_test_polarity_dict = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "    X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train_pol, y_test_pol = train_test_split(\n",
    "        tokenized_data[model_name]['input_ids'],\n",
    "        tokenized_data[model_name]['attention_masks'],\n",
    "        labels_polarity,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_polarity\n",
    "    )\n",
    "    X_train_ids_dict[model_name] = X_train_ids\n",
    "    X_test_ids_dict[model_name] = X_test_ids\n",
    "    X_train_masks_dict[model_name] = X_train_masks\n",
    "    X_test_masks_dict[model_name] = X_test_masks\n",
    "    y_train_polarity_dict[model_name] = y_train_pol\n",
    "    y_test_polarity_dict[model_name] = y_test_pol\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Model Building, Training, and Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "# Function to build and compile the model\n",
    "def build_model(pretrained_model_info, num_polarities, max_len=20):\n",
    "    \"\"\"\n",
    "    Builds a single-task model with shared pre-trained layers and a single output layer.\n",
    "    \"\"\"\n",
    "    tokenizer_class = pretrained_model_info['tokenizer']\n",
    "    model_class = pretrained_model_info['model']\n",
    "    pretrained_name = pretrained_model_info['pretrained_name']\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {pretrained_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the model with TensorFlow weights first\n",
    "        base_model = model_class.from_pretrained(pretrained_name)\n",
    "    except OSError:\n",
    "        # If TensorFlow weights are unavailable, try loading PyTorch weights\n",
    "        print(f\"TensorFlow weights not found for {pretrained_name}. Attempting to load PyTorch weights.\")\n",
    "        try:\n",
    "            base_model = model_class.from_pretrained(pretrained_name, from_pt=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model for {pretrained_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # Get base model outputs\n",
    "    base_outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = base_outputs[1]  # Typically the [CLS] token representation\n",
    "\n",
    "    # Shared Dense layer\n",
    "    shared_dense = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    shared_dense = tf.keras.layers.Dropout(0.3)(shared_dense)\n",
    "\n",
    "    # Polarity output\n",
    "    polarity_output = tf.keras.layers.Dense(num_polarities, activation='softmax', name='polarity')(shared_dense)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[polarity_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss={\n",
    "            'polarity': 'sparse_categorical_crossentropy',\n",
    "        },\n",
    "        metrics={\n",
    "            'polarity': 'accuracy',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model, X_train_ids, X_train_masks, y_train_pol,\n",
    "                       X_test_ids, X_test_masks, y_test_pol, model_name, epochs=3, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    history = model.fit(\n",
    "        {'input_ids': X_train_ids, 'attention_mask': X_train_masks},\n",
    "        {'polarity': y_train_pol},\n",
    "        validation_data=(\n",
    "            {'input_ids': X_test_ids, 'attention_mask': X_test_masks},\n",
    "            {'polarity': y_test_pol}\n",
    "        ),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    predictions = model.predict({'input_ids': X_test_ids, 'attention_mask': X_test_masks})\n",
    "    \n",
    "    # For single-output models, predictions is a single NumPy array\n",
    "    # Apply argmax directly on the predictions array\n",
    "    pred_polarities = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Polarity Evaluation\n",
    "    print(f\"\\nPolarity Classification Report for {model_name}:\")\n",
    "    print(classification_report(y_test_pol, pred_polarities, target_names=polarity_encoder.classes_))\n",
    "\n",
    "    # Return history and predictions if needed\n",
    "    return history, pred_polarities\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nBuilding model for: {model_name}\")\n",
    "    pretrained_model_info = pretrained_models[model_name]\n",
    "    model, tokenizer = build_model(pretrained_model_info, num_polarities, max_len=20)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"Skipping training for {model_name} due to build errors.\")\n",
    "        continue\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    history, pred_polarities = train_and_evaluate(\n",
    "        model,\n",
    "        X_train_ids_dict[model_name],\n",
    "        X_train_masks_dict[model_name],\n",
    "        y_train_polarity_dict[model_name],\n",
    "        X_test_ids_dict[model_name],\n",
    "        X_test_masks_dict[model_name],\n",
    "        y_test_polarity_dict[model_name],\n",
    "        model_name,\n",
    "        epochs=3,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    save_dir = f'./fine_tuned_models/{model_name.replace(\"/\", \"_\")}_polarity'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    try:\n",
    "        model.save(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Model and tokenizer saved to {save_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model for {model_name}: {e}\")\n",
    "\n",
    "    # Store results\n",
    "    model_results[model_name] = {\n",
    "        'history': history,\n",
    "        'pred_polarities': pred_polarities\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models have been trained and evaluated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Optional: Compare Model Performances\n",
    "# -------------------------------\n",
    "\n",
    "# Example: Plotting polarity accuracy for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "model_labels = []\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in model_results:\n",
    "        continue\n",
    "    history = model_results[model_name]['history']\n",
    "    train_acc.append(history.history['polarity_accuracy'][-1])\n",
    "    val_acc.append(history.history['val_polarity_accuracy'][-1])\n",
    "    model_labels.append(model_name)\n",
    "\n",
    "x = np.arange(len(model_labels))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, train_acc, width, label='Train Accuracy')\n",
    "rects2 = ax.bar(x + width/2, val_acc, width, label='Validation Accuracy')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Polarity Classification Accuracy by Model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Attach a text label above each bar\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
