{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_analysis_single_task_finetune.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFBertModel,\n",
    ")\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK resources if not already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "# Note: NLTK may not have comprehensive Bengali stopwords. Consider using a custom list if needed.\n",
    "try:\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "except LookupError:\n",
    "    print(\"Bengali stopwords not found. Skipping stopword removal.\")\n",
    "    stop_words = set()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. GPU Memory Management\n",
    "# -------------------------------\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Enabled memory growth for {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "# Ensure the CSV has at least two columns: 'Text' and 'Category'\n",
    "data_path = r\"F:\\Context-Resonance Transformer\\Cricket\\Cricket - Sheet1.csv\"  # Update this path as needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['Text', 'Category']]\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"Initial Data Shape: {df.shape}\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Keep only Bengali characters: Unicode range for Bengali: \\u0980-\\u09FF\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    # Lemmatize and remove stopwords if available\n",
    "    if stop_words:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['Text'] = df['Text'].astype(str).apply(clean_text)\n",
    "print(\"DataFrame after text cleaning:\")\n",
    "print(df.head())\n",
    "\n",
    "# Upsampling 'Category' to balance classes\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Category'\n",
    "df_upsampled = upsample(df, 'Category')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Category distribution after upsampling:\")\n",
    "print(df_upsampled['Category'].value_counts())\n",
    "\n",
    "# Encode 'Category' labels\n",
    "category_encoder = LabelEncoder()\n",
    "df_upsampled['Category_encoded'] = category_encoder.fit_transform(df_upsampled['Category'])\n",
    "\n",
    "# Verify encoding\n",
    "print(\"Encoded Category:\")\n",
    "print(df_upsampled[['Category', 'Category_encoded']].head())\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Define the list of pre-trained models to fine-tune\n",
    "pretrained_models = {\n",
    "    'bert-base-multilingual-cased': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'bert-base-multilingual-cased'\n",
    "    },\n",
    "    'sagorsarker/bangla-bert-base': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'sagorsarker/bangla-bert-base'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define selected models\n",
    "selected_models = list(pretrained_models.keys())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenization\n",
    "# -------------------------------\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(sentences, tokenizer, max_len=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tokenizes sentences in batches for efficiency.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Tokenizing\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        try:\n",
    "            encoded = tokenizer(\n",
    "                list(batch),\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization for batch starting at index {i}: {e}\")\n",
    "\n",
    "    # Concatenate all batches\n",
    "    input_ids = tf.concat(input_ids, axis=0).numpy()\n",
    "    attention_masks = tf.concat(attention_masks, axis=0).numpy()\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Tokenize the data for each model and store in a dictionary\n",
    "tokenized_data = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    print(f\"\\nTokenizing data for model: {model_name}\")\n",
    "    tokenizer_class = pretrained_models[model_name]['tokenizer']\n",
    "    pretrained_name = pretrained_models[model_name]['pretrained_name']\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {model_name}: {e}\")\n",
    "        continue\n",
    "    input_ids, attention_masks = tokenize_sentences(df_upsampled['Text'].values, tokenizer, max_len=20, batch_size=32)\n",
    "    tokenized_data[model_name] = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Preparing Labels and Splits\n",
    "# -------------------------------\n",
    "\n",
    "# Define labels for single-task learning\n",
    "labels_category = df_upsampled['Category_encoded'].values\n",
    "\n",
    "# Split the data into training and testing sets for each model\n",
    "X_train_ids_dict = {}\n",
    "X_test_ids_dict = {}\n",
    "X_train_masks_dict = {}\n",
    "X_test_masks_dict = {}\n",
    "y_train_category_dict = {}\n",
    "y_test_category_dict = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "    X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train_cat, y_test_cat = train_test_split(\n",
    "        tokenized_data[model_name]['input_ids'],\n",
    "        tokenized_data[model_name]['attention_masks'],\n",
    "        labels_category,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_category\n",
    "    )\n",
    "    X_train_ids_dict[model_name] = X_train_ids\n",
    "    X_test_ids_dict[model_name] = X_test_ids\n",
    "    X_train_masks_dict[model_name] = X_train_masks\n",
    "    X_test_masks_dict[model_name] = X_test_masks\n",
    "    y_train_category_dict[model_name] = y_train_cat\n",
    "    y_test_category_dict[model_name] = y_test_cat\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Model Building, Training, and Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "# Number of classes\n",
    "num_categories = df_upsampled['Category_encoded'].nunique()\n",
    "\n",
    "# Function to build and compile the model\n",
    "def build_model(pretrained_model_info, num_categories, max_len=20):\n",
    "    \"\"\"\n",
    "    Builds a single-task model with shared pre-trained layers and a single output layer.\n",
    "    \"\"\"\n",
    "    tokenizer_class = pretrained_model_info['tokenizer']\n",
    "    model_class = pretrained_model_info['model']\n",
    "    pretrained_name = pretrained_model_info['pretrained_name']\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {pretrained_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the model with TensorFlow weights first\n",
    "        base_model = model_class.from_pretrained(pretrained_name)\n",
    "    except OSError:\n",
    "        # If TensorFlow weights are unavailable, try loading PyTorch weights\n",
    "        print(f\"TensorFlow weights not found for {pretrained_name}. Attempting to load PyTorch weights.\")\n",
    "        try:\n",
    "            base_model = model_class.from_pretrained(pretrained_name, from_pt=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model for {pretrained_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # Get base model outputs\n",
    "    base_outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = base_outputs[1]\n",
    "\n",
    "    # Shared Dense layer\n",
    "    shared_dense = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    shared_dense = tf.keras.layers.Dropout(0.3)(shared_dense)\n",
    "\n",
    "    # Category output\n",
    "    category_output = tf.keras.layers.Dense(num_categories, activation='softmax', name='category')(shared_dense)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[category_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss={\n",
    "            'category': 'sparse_categorical_crossentropy',\n",
    "        },\n",
    "        metrics={\n",
    "            'category': 'accuracy',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model, X_train_ids, X_train_masks, y_train_cat,\n",
    "                       X_test_ids, X_test_masks, y_test_cat, model_name, epochs=3, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    history = model.fit(\n",
    "        {'input_ids': X_train_ids, 'attention_mask': X_train_masks},\n",
    "        {'category': y_train_cat},\n",
    "        validation_data=(\n",
    "            {'input_ids': X_test_ids, 'attention_mask': X_test_masks},\n",
    "            {'category': y_test_cat}\n",
    "        ),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    predictions = model.predict({'input_ids': X_test_ids, 'attention_mask': X_test_masks})\n",
    "    pred_categories = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "    # Category Evaluation\n",
    "    print(f\"\\nCategory Classification Report for {model_name}:\")\n",
    "    print(classification_report(y_test_cat, pred_categories, target_names=category_encoder.classes_))\n",
    "\n",
    "    # Return history and predictions if needed\n",
    "    return history, pred_categories\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nBuilding model for: {model_name}\")\n",
    "    pretrained_model_info = pretrained_models[model_name]\n",
    "    model, tokenizer = build_model(pretrained_model_info, num_categories, max_len=20)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"Skipping training for {model_name} due to build errors.\")\n",
    "        continue\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    history, pred_categories = train_and_evaluate(\n",
    "        model,\n",
    "        X_train_ids_dict[model_name],\n",
    "        X_train_masks_dict[model_name],\n",
    "        y_train_category_dict[model_name],\n",
    "        X_test_ids_dict[model_name],\n",
    "        X_test_masks_dict[model_name],\n",
    "        y_test_category_dict[model_name],\n",
    "        model_name,\n",
    "        epochs=3,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    save_dir = f'./fine_tuned_models/{model_name.replace(\"/\", \"_\")}_category'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    try:\n",
    "        model.save(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Model and tokenizer saved to {save_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model for {model_name}: {e}\")\n",
    "\n",
    "    # Store results\n",
    "    model_results[model_name] = {\n",
    "        'history': history,\n",
    "        'pred_categories': pred_categories\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models have been trained and evaluated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Optional: Compare Model Performances\n",
    "# -------------------------------\n",
    "\n",
    "# Example: Plotting category accuracy for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "model_labels = []\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in model_results:\n",
    "        continue\n",
    "    history = model_results[model_name]['history']\n",
    "    train_acc.append(history.history['category_accuracy'][-1])\n",
    "    val_acc.append(history.history['val_category_accuracy'][-1])\n",
    "    model_labels.append(model_name)\n",
    "\n",
    "x = np.arange(len(model_labels))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, train_acc, width, label='Train Accuracy')\n",
    "rects2 = ax.bar(x + width/2, val_acc, width, label='Validation Accuracy')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Category Classification Accuracy by Model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Attach a text label above each bar\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth for 1 GPU(s).\n",
      "Initial DataFrame:\n",
      "                                                Text                 Category\n",
      "0              স্টাফ কিন্তু, আমাদের জন্য ভয়ঙ্কর ছিল।                  service\n",
      "1  শুধুমাত্র,রিডামিং ফ্যাক্টর খাদ্য ছিল,পুরোপুরি ...                     food\n",
      "2  শুধুমাত্র,রিডামিং ফ্যাক্টর খাদ্য ছিল,পুরোপুরি ...  anecdotes/miscellaneous\n",
      "3  খাবার একদমই ব্যতিক্রমী, একটি খুব সক্ষম রান্নাঘ...                     food\n",
      "4  যেখানে গাব্রিয়েলা লোকালি আপনাকে শুভেচ্ছা জানা...                  service\n",
      "Initial Data Shape: (2059, 2)\n",
      "DataFrame after text cleaning:\n",
      "                                                Text                 Category\n",
      "0                                       স্টাফ ভয়ঙ্কর                  service\n",
      "1  শুধুমাত্ররিডামিং ফ্যাক্টর খাদ্য ছিলপুরোপুরি ন্...                     food\n",
      "2  শুধুমাত্ররিডামিং ফ্যাক্টর খাদ্য ছিলপুরোপুরি ন্...  anecdotes/miscellaneous\n",
      "3  খাবার একদমই ব্যতিক্রমী সক্ষম রান্নাঘর গর্বের খ...                     food\n",
      "4  গাব্রিয়েলা লোকালি আপনাকে শুভেচ্ছা আপনাকে খেতে...                  service\n",
      "Category distribution after upsampling:\n",
      "Category\n",
      "service                    710\n",
      "anecdotes/miscellaneous    710\n",
      "food                       710\n",
      "price                      710\n",
      "ambience                   710\n",
      "Name: count, dtype: int64\n",
      "Encoded Category:\n",
      "                  Category  Category_encoded\n",
      "0                  service                 4\n",
      "1  anecdotes/miscellaneous                 1\n",
      "2                     food                 2\n",
      "3                    price                 3\n",
      "4                 ambience                 0\n",
      "Number of unique categories: 5\n",
      "\n",
      "Tokenizing data for model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Tokenizing: 100%|██████████| 111/111 [00:00<00:00, 860.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing data for model: sagorsarker/bangla-bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 111/111 [00:00<00:00, 1037.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model for: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: bert-base-multilingual-cased\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 32s 200ms/step - loss: 1.5692 - accuracy: 0.2757 - val_loss: 1.3738 - val_accuracy: 0.4239\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 16s 178ms/step - loss: 1.2032 - accuracy: 0.5359 - val_loss: 1.0137 - val_accuracy: 0.6451\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 16s 178ms/step - loss: 0.8620 - accuracy: 0.7025 - val_loss: 0.8451 - val_accuracy: 0.7042\n",
      "\n",
      "Evaluating model: bert-base-multilingual-cased\n",
      "23/23 [==============================] - 4s 51ms/step\n",
      "\n",
      "Category Classification Report for bert-base-multilingual-cased:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               ambience       0.68      0.70      0.69       142\n",
      "anecdotes/miscellaneous       0.77      0.69      0.73       142\n",
      "                   food       0.59      0.61      0.60       142\n",
      "                  price       0.82      0.82      0.82       142\n",
      "                service       0.68      0.71      0.69       142\n",
      "\n",
      "               accuracy                           0.70       710\n",
      "              macro avg       0.71      0.70      0.70       710\n",
      "           weighted avg       0.71      0.70      0.70       710\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_models/bert-base-multilingual-cased_category\n",
      "\n",
      "Building model for: sagorsarker/bangla-bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: sagorsarker/bangla-bert-base\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 35s 202ms/step - loss: 1.3714 - accuracy: 0.4320 - val_loss: 1.0119 - val_accuracy: 0.6423\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 15s 165ms/step - loss: 0.8606 - accuracy: 0.6838 - val_loss: 0.8039 - val_accuracy: 0.7380\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 15s 167ms/step - loss: 0.5827 - accuracy: 0.7989 - val_loss: 0.7200 - val_accuracy: 0.7704\n",
      "\n",
      "Evaluating model: sagorsarker/bangla-bert-base\n",
      "23/23 [==============================] - 4s 50ms/step\n",
      "\n",
      "Category Classification Report for sagorsarker/bangla-bert-base:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               ambience       0.68      0.82      0.75       142\n",
      "anecdotes/miscellaneous       0.77      0.82      0.80       142\n",
      "                   food       0.77      0.60      0.67       142\n",
      "                  price       0.78      0.85      0.81       142\n",
      "                service       0.87      0.76      0.81       142\n",
      "\n",
      "               accuracy                           0.77       710\n",
      "              macro avg       0.78      0.77      0.77       710\n",
      "           weighted avg       0.78      0.77      0.77       710\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_models/sagorsarker_bangla-bert-base_category\n",
      "\n",
      "All models have been trained and evaluated.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'category_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 421\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    420\u001b[0m history \u001b[38;5;241m=\u001b[39m model_results[model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 421\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mappend(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    422\u001b[0m val_acc\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_category_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    423\u001b[0m model_labels\u001b[38;5;241m.\u001b[39mappend(model_name)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'category_accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentiment_analysis_single_task_finetune.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFBertModel,\n",
    ")\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK resources if not already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "# Note: NLTK may not have comprehensive Bengali stopwords. Consider using a custom list if needed.\n",
    "try:\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "except LookupError:\n",
    "    print(\"Bengali stopwords not found. Skipping stopword removal.\")\n",
    "    stop_words = set()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. GPU Memory Management\n",
    "# -------------------------------\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Enabled memory growth for {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "# Ensure the CSV has at least two columns: 'Text' and 'Category'\n",
    "data_path = r\"F:\\Context-Resonance Transformer\\Restuarant\\Restaurant - Sheet1.csv\"  # Update this path as needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['Text', 'Category']]\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"Initial Data Shape: {df.shape}\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Keep only Bengali characters: Unicode range for Bengali: \\u0980-\\u09FF\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    # Lemmatize and remove stopwords if available\n",
    "    if stop_words:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['Text'] = df['Text'].astype(str).apply(clean_text)\n",
    "print(\"DataFrame after text cleaning:\")\n",
    "print(df.head())\n",
    "\n",
    "# Upsampling 'Category' to balance classes\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Category'\n",
    "df_upsampled = upsample(df, 'Category')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Category distribution after upsampling:\")\n",
    "print(df_upsampled['Category'].value_counts())\n",
    "\n",
    "# Encode 'Category' labels\n",
    "category_encoder = LabelEncoder()\n",
    "df_upsampled['Category_encoded'] = category_encoder.fit_transform(df_upsampled['Category'])\n",
    "\n",
    "# Verify encoding\n",
    "print(\"Encoded Category:\")\n",
    "print(df_upsampled[['Category', 'Category_encoded']].head())\n",
    "\n",
    "# Verify number of unique classes\n",
    "num_categories = df_upsampled['Category_encoded'].nunique()\n",
    "print(f\"Number of unique categories: {num_categories}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Define the list of pre-trained models to fine-tune\n",
    "pretrained_models = {\n",
    "    'bert-base-multilingual-cased': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'bert-base-multilingual-cased'\n",
    "    },\n",
    "    'sagorsarker/bangla-bert-base': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'sagorsarker/bangla-bert-base'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define selected models\n",
    "selected_models = list(pretrained_models.keys())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenization\n",
    "# -------------------------------\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(sentences, tokenizer, max_len=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tokenizes sentences in batches for efficiency.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Tokenizing\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        try:\n",
    "            encoded = tokenizer(\n",
    "                list(batch),\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization for batch starting at index {i}: {e}\")\n",
    "\n",
    "    # Concatenate all batches\n",
    "    input_ids = tf.concat(input_ids, axis=0).numpy()\n",
    "    attention_masks = tf.concat(attention_masks, axis=0).numpy()\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Tokenize the data for each model and store in a dictionary\n",
    "tokenized_data = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    print(f\"\\nTokenizing data for model: {model_name}\")\n",
    "    tokenizer_class = pretrained_models[model_name]['tokenizer']\n",
    "    pretrained_name = pretrained_models[model_name]['pretrained_name']\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {model_name}: {e}\")\n",
    "        continue\n",
    "    input_ids, attention_masks = tokenize_sentences(df_upsampled['Text'].values, tokenizer, max_len=20, batch_size=32)\n",
    "    tokenized_data[model_name] = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Preparing Labels and Splits\n",
    "# -------------------------------\n",
    "\n",
    "# Define labels for single-task learning\n",
    "labels_category = df_upsampled['Category_encoded'].values\n",
    "\n",
    "# Split the data into training and testing sets for each model\n",
    "X_train_ids_dict = {}\n",
    "X_test_ids_dict = {}\n",
    "X_train_masks_dict = {}\n",
    "X_test_masks_dict = {}\n",
    "y_train_category_dict = {}\n",
    "y_test_category_dict = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "    X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train_cat, y_test_cat = train_test_split(\n",
    "        tokenized_data[model_name]['input_ids'],\n",
    "        tokenized_data[model_name]['attention_masks'],\n",
    "        labels_category,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_category\n",
    "    )\n",
    "    X_train_ids_dict[model_name] = X_train_ids\n",
    "    X_test_ids_dict[model_name] = X_test_ids\n",
    "    X_train_masks_dict[model_name] = X_train_masks\n",
    "    X_test_masks_dict[model_name] = X_test_masks\n",
    "    y_train_category_dict[model_name] = y_train_cat\n",
    "    y_test_category_dict[model_name] = y_test_cat\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Model Building, Training, and Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "# Function to build and compile the model\n",
    "def build_model(pretrained_model_info, num_categories, max_len=20):\n",
    "    \"\"\"\n",
    "    Builds a single-task model with shared pre-trained layers and a single output layer.\n",
    "    \"\"\"\n",
    "    tokenizer_class = pretrained_model_info['tokenizer']\n",
    "    model_class = pretrained_model_info['model']\n",
    "    pretrained_name = pretrained_model_info['pretrained_name']\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {pretrained_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the model with TensorFlow weights first\n",
    "        base_model = model_class.from_pretrained(pretrained_name)\n",
    "    except OSError:\n",
    "        # If TensorFlow weights are unavailable, try loading PyTorch weights\n",
    "        print(f\"TensorFlow weights not found for {pretrained_name}. Attempting to load PyTorch weights.\")\n",
    "        try:\n",
    "            base_model = model_class.from_pretrained(pretrained_name, from_pt=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model for {pretrained_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # Get base model outputs\n",
    "    base_outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = base_outputs[1]  # Typically the [CLS] token representation\n",
    "\n",
    "    # Shared Dense layer\n",
    "    shared_dense = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    shared_dense = tf.keras.layers.Dropout(0.3)(shared_dense)\n",
    "\n",
    "    # Category output\n",
    "    category_output = tf.keras.layers.Dense(num_categories, activation='softmax', name='category')(shared_dense)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[category_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss={\n",
    "            'category': 'sparse_categorical_crossentropy',\n",
    "        },\n",
    "        metrics={\n",
    "            'category': 'accuracy',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model, X_train_ids, X_train_masks, y_train_cat,\n",
    "                       X_test_ids, X_test_masks, y_test_cat, model_name, epochs=3, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    history = model.fit(\n",
    "        {'input_ids': X_train_ids, 'attention_mask': X_train_masks},\n",
    "        {'category': y_train_cat},\n",
    "        validation_data=(\n",
    "            {'input_ids': X_test_ids, 'attention_mask': X_test_masks},\n",
    "            {'category': y_test_cat}\n",
    "        ),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    predictions = model.predict({'input_ids': X_test_ids, 'attention_mask': X_test_masks})\n",
    "    \n",
    "    # For single-output models, predictions is a single NumPy array\n",
    "    # Apply argmax directly on the predictions array\n",
    "    pred_categories = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Category Evaluation\n",
    "    print(f\"\\nCategory Classification Report for {model_name}:\")\n",
    "    print(classification_report(y_test_cat, pred_categories, target_names=category_encoder.classes_))\n",
    "\n",
    "    # Return history and predictions if needed\n",
    "    return history, pred_categories\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nBuilding model for: {model_name}\")\n",
    "    pretrained_model_info = pretrained_models[model_name]\n",
    "    model, tokenizer = build_model(pretrained_model_info, num_categories, max_len=20)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"Skipping training for {model_name} due to build errors.\")\n",
    "        continue\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    history, pred_categories = train_and_evaluate(\n",
    "        model,\n",
    "        X_train_ids_dict[model_name],\n",
    "        X_train_masks_dict[model_name],\n",
    "        y_train_category_dict[model_name],\n",
    "        X_test_ids_dict[model_name],\n",
    "        X_test_masks_dict[model_name],\n",
    "        y_test_category_dict[model_name],\n",
    "        model_name,\n",
    "        epochs=3,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    save_dir = f'./fine_tuned_models/{model_name.replace(\"/\", \"_\")}_category'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    try:\n",
    "        model.save(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Model and tokenizer saved to {save_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model for {model_name}: {e}\")\n",
    "\n",
    "    # Store results\n",
    "    model_results[model_name] = {\n",
    "        'history': history,\n",
    "        'pred_categories': pred_categories\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models have been trained and evaluated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Optional: Compare Model Performances\n",
    "# -------------------------------\n",
    "\n",
    "# Example: Plotting category accuracy for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "model_labels = []\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in model_results:\n",
    "        continue\n",
    "    history = model_results[model_name]['history']\n",
    "    train_acc.append(history.history['category_accuracy'][-1])\n",
    "    val_acc.append(history.history['val_category_accuracy'][-1])\n",
    "    model_labels.append(model_name)\n",
    "\n",
    "x = np.arange(len(model_labels))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, train_acc, width, label='Train Accuracy')\n",
    "rects2 = ax.bar(x + width/2, val_acc, width, label='Validation Accuracy')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Category Classification Accuracy by Model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Attach a text label above each bar\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth for 1 GPU(s).\n",
      "Initial DataFrame:\n",
      "                                                Text  Polarity\n",
      "0              স্টাফ কিন্তু, আমাদের জন্য ভয়ঙ্কর ছিল।  negative\n",
      "1  শুধুমাত্র,রিডামিং ফ্যাক্টর খাদ্য ছিল,পুরোপুরি ...  positive\n",
      "2  শুধুমাত্র,রিডামিং ফ্যাক্টর খাদ্য ছিল,পুরোপুরি ...  negative\n",
      "3  খাবার একদমই ব্যতিক্রমী, একটি খুব সক্ষম রান্নাঘ...  positive\n",
      "4  যেখানে গাব্রিয়েলা লোকালি আপনাকে শুভেচ্ছা জানা...  positive\n",
      "Initial Data Shape: (2059, 2)\n",
      "DataFrame after text cleaning:\n",
      "                                                Text  Polarity\n",
      "0                                       স্টাফ ভয়ঙ্কর  negative\n",
      "1  শুধুমাত্ররিডামিং ফ্যাক্টর খাদ্য ছিলপুরোপুরি ন্...  positive\n",
      "2  শুধুমাত্ররিডামিং ফ্যাক্টর খাদ্য ছিলপুরোপুরি ন্...  negative\n",
      "3  খাবার একদমই ব্যতিক্রমী সক্ষম রান্নাঘর গর্বের খ...  positive\n",
      "4  গাব্রিয়েলা লোকালি আপনাকে শুভেচ্ছা আপনাকে খেতে...  positive\n",
      "Polarity distribution after upsampling:\n",
      "Polarity\n",
      "negative    1221\n",
      "positive    1221\n",
      "conflict    1221\n",
      "neutral     1221\n",
      "Name: count, dtype: int64\n",
      "Encoded Polarity:\n",
      "   Polarity  Polarity_encoded\n",
      "0  negative                 1\n",
      "1  negative                 1\n",
      "2  positive                 3\n",
      "3  conflict                 0\n",
      "4  positive                 3\n",
      "Number of unique polarities: 4\n",
      "\n",
      "Tokenizing data for model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Tokenizing: 100%|██████████| 153/153 [00:00<00:00, 894.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing data for model: sagorsarker/bangla-bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 153/153 [00:00<00:00, 1013.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model for: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: bert-base-multilingual-cased\n",
      "Epoch 1/3\n",
      "123/123 [==============================] - 44s 226ms/step - loss: 1.2664 - accuracy: 0.3811 - val_loss: 0.9813 - val_accuracy: 0.5640\n",
      "Epoch 2/3\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.7079 - accuracy: 0.7236 - val_loss: 0.5728 - val_accuracy: 0.8025\n",
      "Epoch 3/3\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.4112 - accuracy: 0.8531 - val_loss: 0.4345 - val_accuracy: 0.8628\n",
      "\n",
      "Evaluating model: bert-base-multilingual-cased\n",
      "31/31 [==============================] - 4s 59ms/step\n",
      "\n",
      "Polarity Classification Report for bert-base-multilingual-cased:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.92      1.00      0.96       244\n",
      "    negative       0.89      0.77      0.82       244\n",
      "     neutral       0.78      0.96      0.86       245\n",
      "    positive       0.89      0.72      0.80       244\n",
      "\n",
      "    accuracy                           0.86       977\n",
      "   macro avg       0.87      0.86      0.86       977\n",
      "weighted avg       0.87      0.86      0.86       977\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_models/bert-base-multilingual-cased_polarity\n",
      "\n",
      "Building model for: sagorsarker/bangla-bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: sagorsarker/bangla-bert-base\n",
      "Epoch 1/3\n",
      "123/123 [==============================] - 36s 184ms/step - loss: 1.1166 - accuracy: 0.5058 - val_loss: 0.6779 - val_accuracy: 0.7564\n",
      "Epoch 2/3\n",
      "123/123 [==============================] - 20s 163ms/step - loss: 0.5025 - accuracy: 0.8219 - val_loss: 0.3374 - val_accuracy: 0.8792\n",
      "Epoch 3/3\n",
      "123/123 [==============================] - 20s 163ms/step - loss: 0.2659 - accuracy: 0.9155 - val_loss: 0.3035 - val_accuracy: 0.9038\n",
      "\n",
      "Evaluating model: sagorsarker/bangla-bert-base\n",
      "31/31 [==============================] - 4s 49ms/step\n",
      "\n",
      "Polarity Classification Report for sagorsarker/bangla-bert-base:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.95      1.00      0.98       244\n",
      "    negative       0.89      0.86      0.87       244\n",
      "     neutral       0.84      0.96      0.90       245\n",
      "    positive       0.96      0.80      0.87       244\n",
      "\n",
      "    accuracy                           0.90       977\n",
      "   macro avg       0.91      0.90      0.90       977\n",
      "weighted avg       0.91      0.90      0.90       977\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "# sentiment_analysis_polarity_single_task_finetune.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFBertModel,\n",
    ")\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK resources if not already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "# Note: NLTK may not have comprehensive Bengali stopwords. Consider using a custom list if needed.\n",
    "try:\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "except LookupError:\n",
    "    print(\"Bengali stopwords not found. Skipping stopword removal.\")\n",
    "    stop_words = set()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. GPU Memory Management\n",
    "# -------------------------------\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Enabled memory growth for {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "# Ensure the CSV has at least two columns: 'Text' and 'Polarity'\n",
    "data_path = r\"F:\\Context-Resonance Transformer\\Restuarant\\Restaurant - Sheet1.csv\"  # Update this path as needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['Text', 'Polarity']]\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"Initial Data Shape: {df.shape}\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Keep only Bengali characters: Unicode range for Bengali: \\u0980-\\u09FF\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    # Lemmatize and remove stopwords if available\n",
    "    if stop_words:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['Text'] = df['Text'].astype(str).apply(clean_text)\n",
    "print(\"DataFrame after text cleaning:\")\n",
    "print(df.head())\n",
    "\n",
    "# Upsampling 'Polarity' to balance classes\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Polarity'\n",
    "df_upsampled = upsample(df, 'Polarity')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Polarity distribution after upsampling:\")\n",
    "print(df_upsampled['Polarity'].value_counts())\n",
    "\n",
    "# Encode 'Polarity' labels\n",
    "polarity_encoder = LabelEncoder()\n",
    "df_upsampled['Polarity_encoded'] = polarity_encoder.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Verify encoding\n",
    "print(\"Encoded Polarity:\")\n",
    "print(df_upsampled[['Polarity', 'Polarity_encoded']].head())\n",
    "\n",
    "# Verify number of unique classes\n",
    "num_polarities = df_upsampled['Polarity_encoded'].nunique()\n",
    "print(f\"Number of unique polarities: {num_polarities}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Define the list of pre-trained models to fine-tune\n",
    "pretrained_models = {\n",
    "    'bert-base-multilingual-cased': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'bert-base-multilingual-cased'\n",
    "    },\n",
    "    'sagorsarker/bangla-bert-base': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFBertModel,\n",
    "        'pretrained_name': 'sagorsarker/bangla-bert-base'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define selected models\n",
    "selected_models = list(pretrained_models.keys())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenization\n",
    "# -------------------------------\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(sentences, tokenizer, max_len=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tokenizes sentences in batches for efficiency.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Tokenizing\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        try:\n",
    "            encoded = tokenizer(\n",
    "                list(batch),\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization for batch starting at index {i}: {e}\")\n",
    "\n",
    "    # Concatenate all batches\n",
    "    input_ids = tf.concat(input_ids, axis=0).numpy()\n",
    "    attention_masks = tf.concat(attention_masks, axis=0).numpy()\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Tokenize the data for each model and store in a dictionary\n",
    "tokenized_data = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    print(f\"\\nTokenizing data for model: {model_name}\")\n",
    "    tokenizer_class = pretrained_models[model_name]['tokenizer']\n",
    "    pretrained_name = pretrained_models[model_name]['pretrained_name']\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {model_name}: {e}\")\n",
    "        continue\n",
    "    input_ids, attention_masks = tokenize_sentences(df_upsampled['Text'].values, tokenizer, max_len=20, batch_size=32)\n",
    "    tokenized_data[model_name] = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Preparing Labels and Splits\n",
    "# -------------------------------\n",
    "\n",
    "# Define labels for single-task learning\n",
    "labels_polarity = df_upsampled['Polarity_encoded'].values\n",
    "\n",
    "# Split the data into training and testing sets for each model\n",
    "X_train_ids_dict = {}\n",
    "X_test_ids_dict = {}\n",
    "X_train_masks_dict = {}\n",
    "X_test_masks_dict = {}\n",
    "y_train_polarity_dict = {}\n",
    "y_test_polarity_dict = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "    X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train_pol, y_test_pol = train_test_split(\n",
    "        tokenized_data[model_name]['input_ids'],\n",
    "        tokenized_data[model_name]['attention_masks'],\n",
    "        labels_polarity,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_polarity\n",
    "    )\n",
    "    X_train_ids_dict[model_name] = X_train_ids\n",
    "    X_test_ids_dict[model_name] = X_test_ids\n",
    "    X_train_masks_dict[model_name] = X_train_masks\n",
    "    X_test_masks_dict[model_name] = X_test_masks\n",
    "    y_train_polarity_dict[model_name] = y_train_pol\n",
    "    y_test_polarity_dict[model_name] = y_test_pol\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Model Building, Training, and Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "# Function to build and compile the model\n",
    "def build_model(pretrained_model_info, num_polarities, max_len=20):\n",
    "    \"\"\"\n",
    "    Builds a single-task model with shared pre-trained layers and a single output layer.\n",
    "    \"\"\"\n",
    "    tokenizer_class = pretrained_model_info['tokenizer']\n",
    "    model_class = pretrained_model_info['model']\n",
    "    pretrained_name = pretrained_model_info['pretrained_name']\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {pretrained_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the model with TensorFlow weights first\n",
    "        base_model = model_class.from_pretrained(pretrained_name)\n",
    "    except OSError:\n",
    "        # If TensorFlow weights are unavailable, try loading PyTorch weights\n",
    "        print(f\"TensorFlow weights not found for {pretrained_name}. Attempting to load PyTorch weights.\")\n",
    "        try:\n",
    "            base_model = model_class.from_pretrained(pretrained_name, from_pt=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model for {pretrained_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # Get base model outputs\n",
    "    base_outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = base_outputs[1]  # Typically the [CLS] token representation\n",
    "\n",
    "    # Shared Dense layer\n",
    "    shared_dense = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    shared_dense = tf.keras.layers.Dropout(0.3)(shared_dense)\n",
    "\n",
    "    # Polarity output\n",
    "    polarity_output = tf.keras.layers.Dense(num_polarities, activation='softmax', name='polarity')(shared_dense)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[polarity_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss={\n",
    "            'polarity': 'sparse_categorical_crossentropy',\n",
    "        },\n",
    "        metrics={\n",
    "            'polarity': 'accuracy',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model, X_train_ids, X_train_masks, y_train_pol,\n",
    "                       X_test_ids, X_test_masks, y_test_pol, model_name, epochs=3, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    history = model.fit(\n",
    "        {'input_ids': X_train_ids, 'attention_mask': X_train_masks},\n",
    "        {'polarity': y_train_pol},\n",
    "        validation_data=(\n",
    "            {'input_ids': X_test_ids, 'attention_mask': X_test_masks},\n",
    "            {'polarity': y_test_pol}\n",
    "        ),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    predictions = model.predict({'input_ids': X_test_ids, 'attention_mask': X_test_masks})\n",
    "    \n",
    "    # For single-output models, predictions is a single NumPy array\n",
    "    # Apply argmax directly on the predictions array\n",
    "    pred_polarities = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Polarity Evaluation\n",
    "    print(f\"\\nPolarity Classification Report for {model_name}:\")\n",
    "    print(classification_report(y_test_pol, pred_polarities, target_names=polarity_encoder.classes_))\n",
    "\n",
    "    # Return history and predictions if needed\n",
    "    return history, pred_polarities\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nBuilding model for: {model_name}\")\n",
    "    pretrained_model_info = pretrained_models[model_name]\n",
    "    model, tokenizer = build_model(pretrained_model_info, num_polarities, max_len=20)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"Skipping training for {model_name} due to build errors.\")\n",
    "        continue\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    history, pred_polarities = train_and_evaluate(\n",
    "        model,\n",
    "        X_train_ids_dict[model_name],\n",
    "        X_train_masks_dict[model_name],\n",
    "        y_train_polarity_dict[model_name],\n",
    "        X_test_ids_dict[model_name],\n",
    "        X_test_masks_dict[model_name],\n",
    "        y_test_polarity_dict[model_name],\n",
    "        model_name,\n",
    "        epochs=3,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    save_dir = f'./fine_tuned_models/{model_name.replace(\"/\", \"_\")}_polarity'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    try:\n",
    "        model.save(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Model and tokenizer saved to {save_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model for {model_name}: {e}\")\n",
    "\n",
    "    # Store results\n",
    "    model_results[model_name] = {\n",
    "        'history': history,\n",
    "        'pred_polarities': pred_polarities\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models have been trained and evaluated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Optional: Compare Model Performances\n",
    "# -------------------------------\n",
    "\n",
    "# Example: Plotting polarity accuracy for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "model_labels = []\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in model_results:\n",
    "        continue\n",
    "    history = model_results[model_name]['history']\n",
    "    train_acc.append(history.history['polarity_accuracy'][-1])\n",
    "    val_acc.append(history.history['val_polarity_accuracy'][-1])\n",
    "    model_labels.append(model_name)\n",
    "\n",
    "x = np.arange(len(model_labels))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, train_acc, width, label='Train Accuracy')\n",
    "rects2 = ax.bar(x + width/2, val_acc, width, label='Validation Accuracy')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Polarity Classification Accuracy by Model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Attach a text label above each bar\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth for 1 GPU(s).\n",
      "Initial DataFrame:\n",
      "                                                Text Category\n",
      "0  জয় বাংলা কাপ! তাও আবার স্বাধীনতার মাস মার্চে। ...    other\n",
      "1  জয় বাংলা কাপ! তাও আবার স্বাধীনতার মাস মার্চে। ...     team\n",
      "2               বাংলাদেশের পরে ভারতের সাপর্ট ই করি ?     team\n",
      "3                              সৌম্যকে বাদ দেওয়া হোক  batting\n",
      "4  প্রথমটি হচ্ছে, কোচ অত:পর সাকিব,সাকিব আর সাকিবর...     team\n",
      "Initial Data Shape: (2979, 2)\n",
      "DataFrame after text cleaning:\n",
      "                                                Text Category\n",
      "0  জয় বাংলা কাপ স্বাধীনতার মাস মার্চে মাথা চমৎকার...    other\n",
      "1  জয় বাংলা কাপ স্বাধীনতার মাস মার্চে মাথা চমৎকার...     team\n",
      "2                           বাংলাদেশের ভারতের সাপর্ট     team\n",
      "3                                        সৌম্যকে বাদ  batting\n",
      "4            প্রথমটি কোচ অতপর সাকিবসাকিব সাকিবরে দলে     team\n",
      "Category distribution after upsampling:\n",
      "Category\n",
      "bowling            1010\n",
      "other              1010\n",
      "team management    1010\n",
      "team               1010\n",
      "batting            1010\n",
      "Name: count, dtype: int64\n",
      "Encoded Category:\n",
      "          Category  Category_encoded\n",
      "0          bowling                 1\n",
      "1            other                 2\n",
      "2            other                 2\n",
      "3  team management                 4\n",
      "4          bowling                 1\n",
      "Number of unique categories: 5\n",
      "\n",
      "Tokenizing data for model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Tokenizing: 100%|██████████| 158/158 [00:00<00:00, 195.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model for: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: xlm-roberta-base\n",
      "Epoch 1/3\n",
      "127/127 [==============================] - 48s 267ms/step - loss: 1.5506 - accuracy: 0.3005 - val_loss: 1.2228 - val_accuracy: 0.5485\n",
      "Epoch 2/3\n",
      "127/127 [==============================] - 31s 245ms/step - loss: 1.2930 - accuracy: 0.5030 - val_loss: 1.0251 - val_accuracy: 0.6287\n",
      "Epoch 3/3\n",
      "127/127 [==============================] - 32s 248ms/step - loss: 1.1256 - accuracy: 0.5871 - val_loss: 1.0006 - val_accuracy: 0.6327\n",
      "\n",
      "Evaluating model: xlm-roberta-base\n",
      "32/32 [==============================] - 4s 58ms/step\n",
      "\n",
      "Category Classification Report for xlm-roberta-base:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        batting       0.52      0.86      0.65       202\n",
      "        bowling       0.80      0.49      0.61       202\n",
      "          other       0.55      0.50      0.53       202\n",
      "           team       0.64      0.59      0.62       202\n",
      "team management       0.81      0.72      0.76       202\n",
      "\n",
      "       accuracy                           0.63      1010\n",
      "      macro avg       0.66      0.63      0.63      1010\n",
      "   weighted avg       0.66      0.63      0.63      1010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_models/xlm-roberta-base_category\n",
      "\n",
      "All models have been trained and evaluated.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'category_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 416\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    415\u001b[0m history \u001b[38;5;241m=\u001b[39m model_results[model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 416\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mappend(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    417\u001b[0m val_acc\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_category_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    418\u001b[0m model_labels\u001b[38;5;241m.\u001b[39mappend(model_name)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'category_accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentiment_analysis_category_single_task_finetune_xlm_roberta.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFXLMRobertaModel,\n",
    ")\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK resources if not already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "# Note: NLTK may not have comprehensive Bengali stopwords. Consider using a custom list if needed.\n",
    "try:\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "except LookupError:\n",
    "    print(\"Bengali stopwords not found. Skipping stopword removal.\")\n",
    "    stop_words = set()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. GPU Memory Management\n",
    "# -------------------------------\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Enabled memory growth for {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "# Ensure the CSV has at least two columns: 'Text' and 'Category'\n",
    "data_path = r\"F:\\Context-Resonance Transformer\\Cricket\\Cricket - Sheet1.csv\"  # Update this path as needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['Text', 'Category']]\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"Initial Data Shape: {df.shape}\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Keep only Bengali characters: Unicode range for Bengali: \\u0980-\\u09FF\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    # Lemmatize and remove stopwords if available\n",
    "    if stop_words:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['Text'] = df['Text'].astype(str).apply(clean_text)\n",
    "print(\"DataFrame after text cleaning:\")\n",
    "print(df.head())\n",
    "\n",
    "# Upsampling 'Category' to balance classes\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Category'\n",
    "df_upsampled = upsample(df, 'Category')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Category distribution after upsampling:\")\n",
    "print(df_upsampled['Category'].value_counts())\n",
    "\n",
    "# Encode 'Category' labels\n",
    "category_encoder = LabelEncoder()\n",
    "df_upsampled['Category_encoded'] = category_encoder.fit_transform(df_upsampled['Category'])\n",
    "\n",
    "# Verify encoding\n",
    "print(\"Encoded Category:\")\n",
    "print(df_upsampled[['Category', 'Category_encoded']].head())\n",
    "\n",
    "# Verify number of unique classes\n",
    "num_categories = df_upsampled['Category_encoded'].nunique()\n",
    "print(f\"Number of unique categories: {num_categories}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Define the list of pre-trained models to fine-tune\n",
    "pretrained_models = {\n",
    "    'xlm-roberta-base': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFXLMRobertaModel,\n",
    "        'pretrained_name': 'xlm-roberta-base'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define selected models\n",
    "selected_models = list(pretrained_models.keys())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenization\n",
    "# -------------------------------\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(sentences, tokenizer, max_len=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tokenizes sentences in batches for efficiency.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Tokenizing\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        try:\n",
    "            encoded = tokenizer(\n",
    "                list(batch),\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization for batch starting at index {i}: {e}\")\n",
    "\n",
    "    # Concatenate all batches\n",
    "    input_ids = tf.concat(input_ids, axis=0).numpy()\n",
    "    attention_masks = tf.concat(attention_masks, axis=0).numpy()\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Tokenize the data for each model and store in a dictionary\n",
    "tokenized_data = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    print(f\"\\nTokenizing data for model: {model_name}\")\n",
    "    tokenizer_class = pretrained_models[model_name]['tokenizer']\n",
    "    pretrained_name = pretrained_models[model_name]['pretrained_name']\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {model_name}: {e}\")\n",
    "        continue\n",
    "    input_ids, attention_masks = tokenize_sentences(df_upsampled['Text'].values, tokenizer, max_len=20, batch_size=32)\n",
    "    tokenized_data[model_name] = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Preparing Labels and Splits\n",
    "# -------------------------------\n",
    "\n",
    "# Define labels for single-task learning\n",
    "labels_category = df_upsampled['Category_encoded'].values\n",
    "\n",
    "# Split the data into training and testing sets for each model\n",
    "X_train_ids_dict = {}\n",
    "X_test_ids_dict = {}\n",
    "X_train_masks_dict = {}\n",
    "X_test_masks_dict = {}\n",
    "y_train_category_dict = {}\n",
    "y_test_category_dict = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "    X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train_cat, y_test_cat = train_test_split(\n",
    "        tokenized_data[model_name]['input_ids'],\n",
    "        tokenized_data[model_name]['attention_masks'],\n",
    "        labels_category,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_category\n",
    "    )\n",
    "    X_train_ids_dict[model_name] = X_train_ids\n",
    "    X_test_ids_dict[model_name] = X_test_ids\n",
    "    X_train_masks_dict[model_name] = X_train_masks\n",
    "    X_test_masks_dict[model_name] = X_test_masks\n",
    "    y_train_category_dict[model_name] = y_train_cat\n",
    "    y_test_category_dict[model_name] = y_test_cat\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Model Building, Training, and Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "# Function to build and compile the model\n",
    "def build_model(pretrained_model_info, num_categories, max_len=20):\n",
    "    \"\"\"\n",
    "    Builds a single-task model with shared pre-trained layers and a single output layer.\n",
    "    \"\"\"\n",
    "    tokenizer_class = pretrained_model_info['tokenizer']\n",
    "    model_class = pretrained_model_info['model']\n",
    "    pretrained_name = pretrained_model_info['pretrained_name']\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {pretrained_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the model with TensorFlow weights first\n",
    "        base_model = model_class.from_pretrained(pretrained_name)\n",
    "    except OSError:\n",
    "        # If TensorFlow weights are unavailable, try loading PyTorch weights\n",
    "        print(f\"TensorFlow weights not found for {pretrained_name}. Attempting to load PyTorch weights.\")\n",
    "        try:\n",
    "            base_model = model_class.from_pretrained(pretrained_name, from_pt=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model for {pretrained_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # Get base model outputs\n",
    "    base_outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = base_outputs[1]  # Typically the [CLS] token representation\n",
    "\n",
    "    # Shared Dense layer\n",
    "    shared_dense = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    shared_dense = tf.keras.layers.Dropout(0.3)(shared_dense)\n",
    "\n",
    "    # Category output\n",
    "    category_output = tf.keras.layers.Dense(num_categories, activation='softmax', name='category')(shared_dense)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[category_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss={\n",
    "            'category': 'sparse_categorical_crossentropy',\n",
    "        },\n",
    "        metrics={\n",
    "            'category': 'accuracy',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model, X_train_ids, X_train_masks, y_train_cat,\n",
    "                       X_test_ids, X_test_masks, y_test_cat, model_name, epochs=3, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    history = model.fit(\n",
    "        {'input_ids': X_train_ids, 'attention_mask': X_train_masks},\n",
    "        {'category': y_train_cat},\n",
    "        validation_data=(\n",
    "            {'input_ids': X_test_ids, 'attention_mask': X_test_masks},\n",
    "            {'category': y_test_cat}\n",
    "        ),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    predictions = model.predict({'input_ids': X_test_ids, 'attention_mask': X_test_masks})\n",
    "    \n",
    "    # For single-output models, predictions is a single NumPy array\n",
    "    # Apply argmax directly on the predictions array\n",
    "    pred_categories = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Category Evaluation\n",
    "    print(f\"\\nCategory Classification Report for {model_name}:\")\n",
    "    print(classification_report(y_test_cat, pred_categories, target_names=category_encoder.classes_))\n",
    "\n",
    "    # Return history and predictions if needed\n",
    "    return history, pred_categories\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nBuilding model for: {model_name}\")\n",
    "    pretrained_model_info = pretrained_models[model_name]\n",
    "    model, tokenizer = build_model(pretrained_model_info, num_categories, max_len=20)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"Skipping training for {model_name} due to build errors.\")\n",
    "        continue\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    history, pred_categories = train_and_evaluate(\n",
    "        model,\n",
    "        X_train_ids_dict[model_name],\n",
    "        X_train_masks_dict[model_name],\n",
    "        y_train_category_dict[model_name],\n",
    "        X_test_ids_dict[model_name],\n",
    "        X_test_masks_dict[model_name],\n",
    "        y_test_category_dict[model_name],\n",
    "        model_name,\n",
    "        epochs=3,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    save_dir = f'./fine_tuned_models/{model_name.replace(\"/\", \"_\")}_category'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    try:\n",
    "        model.save(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Model and tokenizer saved to {save_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model for {model_name}: {e}\")\n",
    "\n",
    "    # Store results\n",
    "    model_results[model_name] = {\n",
    "        'history': history,\n",
    "        'pred_categories': pred_categories\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models have been trained and evaluated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Optional: Compare Model Performances\n",
    "# -------------------------------\n",
    "\n",
    "# Example: Plotting category accuracy for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "model_labels = []\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in model_results:\n",
    "        continue\n",
    "    history = model_results[model_name]['history']\n",
    "    train_acc.append(history.history['category_accuracy'][-1])\n",
    "    val_acc.append(history.history['val_category_accuracy'][-1])\n",
    "    model_labels.append(model_name)\n",
    "\n",
    "x = np.arange(len(model_labels))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, train_acc, width, label='Train Accuracy')\n",
    "rects2 = ax.bar(x + width/2, val_acc, width, label='Validation Accuracy')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Category Classification Accuracy by Model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Attach a text label above each bar\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth for 1 GPU(s).\n",
      "Initial DataFrame:\n",
      "                                                Text  Polarity\n",
      "0  জয় বাংলা কাপ! তাও আবার স্বাধীনতার মাস মার্চে। ...  positive\n",
      "1  জয় বাংলা কাপ! তাও আবার স্বাধীনতার মাস মার্চে। ...  positive\n",
      "2               বাংলাদেশের পরে ভারতের সাপর্ট ই করি ?  positive\n",
      "3                              সৌম্যকে বাদ দেওয়া হোক  negative\n",
      "4  প্রথমটি হচ্ছে, কোচ অত:পর সাকিব,সাকিব আর সাকিবর...  positive\n",
      "Initial Data Shape: (2979, 2)\n",
      "DataFrame after text cleaning:\n",
      "                                                Text  Polarity\n",
      "0  জয় বাংলা কাপ স্বাধীনতার মাস মার্চে মাথা চমৎকার...  positive\n",
      "1  জয় বাংলা কাপ স্বাধীনতার মাস মার্চে মাথা চমৎকার...  positive\n",
      "2                           বাংলাদেশের ভারতের সাপর্ট  positive\n",
      "3                                        সৌম্যকে বাদ  negative\n",
      "4            প্রথমটি কোচ অতপর সাকিবসাকিব সাকিবরে দলে  positive\n",
      "Polarity distribution after upsampling:\n",
      "Polarity\n",
      "neutral     2152\n",
      "positive    2152\n",
      "negative    2152\n",
      "Name: count, dtype: int64\n",
      "Encoded Polarity:\n",
      "   Polarity  Polarity_encoded\n",
      "0   neutral                 1\n",
      "1   neutral                 1\n",
      "2  positive                 2\n",
      "3   neutral                 1\n",
      "4  positive                 2\n",
      "Number of unique polarities: 3\n",
      "\n",
      "Tokenizing data for model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Tokenizing: 100%|██████████| 202/202 [00:00<00:00, 237.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model for: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: xlm-roberta-base\n",
      "Epoch 1/3\n",
      "162/162 [==============================] - 52s 233ms/step - loss: 1.1227 - accuracy: 0.3418 - val_loss: 1.0964 - val_accuracy: 0.3359\n",
      "Epoch 2/3\n",
      "162/162 [==============================] - 35s 216ms/step - loss: 1.0763 - accuracy: 0.4127 - val_loss: 0.9903 - val_accuracy: 0.5093\n",
      "Epoch 3/3\n",
      "162/162 [==============================] - 35s 216ms/step - loss: 1.0340 - accuracy: 0.4601 - val_loss: 0.9189 - val_accuracy: 0.5642\n",
      "\n",
      "Evaluating model: xlm-roberta-base\n",
      "41/41 [==============================] - 4s 48ms/step\n",
      "\n",
      "Polarity Classification Report for xlm-roberta-base:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.68      0.62       430\n",
      "     neutral       0.48      0.57      0.52       431\n",
      "    positive       0.71      0.45      0.55       431\n",
      "\n",
      "    accuracy                           0.56      1292\n",
      "   macro avg       0.59      0.56      0.56      1292\n",
      "weighted avg       0.59      0.56      0.56      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 421). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_models/xlm-roberta-base_polarity\n",
      "\n",
      "All models have been trained and evaluated.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'polarity_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 416\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    415\u001b[0m history \u001b[38;5;241m=\u001b[39m model_results[model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 416\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mappend(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolarity_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    417\u001b[0m val_acc\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_polarity_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    418\u001b[0m model_labels\u001b[38;5;241m.\u001b[39mappend(model_name)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'polarity_accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentiment_analysis_polarity_single_task_finetune_xlm_roberta.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFXLMRobertaModel,\n",
    ")\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK resources if not already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "# Note: NLTK may not have comprehensive Bengali stopwords. Consider using a custom list if needed.\n",
    "try:\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "except LookupError:\n",
    "    print(\"Bengali stopwords not found. Skipping stopword removal.\")\n",
    "    stop_words = set()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. GPU Memory Management\n",
    "# -------------------------------\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Enabled memory growth for {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "# Ensure the CSV has at least two columns: 'Text' and 'Polarity'\n",
    "data_path = r\"F:\\Context-Resonance Transformer\\Cricket\\Cricket - Sheet1.csv\"  # Update this path as needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['Text', 'Polarity']]\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"Initial Data Shape: {df.shape}\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Keep only Bengali characters: Unicode range for Bengali: \\u0980-\\u09FF\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    # Lemmatize and remove stopwords if available\n",
    "    if stop_words:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['Text'] = df['Text'].astype(str).apply(clean_text)\n",
    "print(\"DataFrame after text cleaning:\")\n",
    "print(df.head())\n",
    "\n",
    "# Upsampling 'Polarity' to balance classes\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Polarity'\n",
    "df_upsampled = upsample(df, 'Polarity')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Polarity distribution after upsampling:\")\n",
    "print(df_upsampled['Polarity'].value_counts())\n",
    "\n",
    "# Encode 'Polarity' labels\n",
    "polarity_encoder = LabelEncoder()\n",
    "df_upsampled['Polarity_encoded'] = polarity_encoder.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Verify encoding\n",
    "print(\"Encoded Polarity:\")\n",
    "print(df_upsampled[['Polarity', 'Polarity_encoded']].head())\n",
    "\n",
    "# Verify number of unique classes\n",
    "num_polarities = df_upsampled['Polarity_encoded'].nunique()\n",
    "print(f\"Number of unique polarities: {num_polarities}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Define the list of pre-trained models to fine-tune\n",
    "pretrained_models = {\n",
    "    'xlm-roberta-base': {\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': TFXLMRobertaModel,\n",
    "        'pretrained_name': 'xlm-roberta-base'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define selected models\n",
    "selected_models = list(pretrained_models.keys())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenization\n",
    "# -------------------------------\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(sentences, tokenizer, max_len=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tokenizes sentences in batches for efficiency.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Tokenizing\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        try:\n",
    "            encoded = tokenizer(\n",
    "                list(batch),\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization for batch starting at index {i}: {e}\")\n",
    "\n",
    "    # Concatenate all batches\n",
    "    input_ids = tf.concat(input_ids, axis=0).numpy()\n",
    "    attention_masks = tf.concat(attention_masks, axis=0).numpy()\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Tokenize the data for each model and store in a dictionary\n",
    "tokenized_data = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    print(f\"\\nTokenizing data for model: {model_name}\")\n",
    "    tokenizer_class = pretrained_models[model_name]['tokenizer']\n",
    "    pretrained_name = pretrained_models[model_name]['pretrained_name']\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {model_name}: {e}\")\n",
    "        continue\n",
    "    input_ids, attention_masks = tokenize_sentences(df_upsampled['Text'].values, tokenizer, max_len=20, batch_size=32)\n",
    "    tokenized_data[model_name] = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Preparing Labels and Splits\n",
    "# -------------------------------\n",
    "\n",
    "# Define labels for single-task learning\n",
    "labels_polarity = df_upsampled['Polarity_encoded'].values\n",
    "\n",
    "# Split the data into training and testing sets for each model\n",
    "X_train_ids_dict = {}\n",
    "X_test_ids_dict = {}\n",
    "X_train_masks_dict = {}\n",
    "X_test_masks_dict = {}\n",
    "y_train_polarity_dict = {}\n",
    "y_test_polarity_dict = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "    X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train_pol, y_test_pol = train_test_split(\n",
    "        tokenized_data[model_name]['input_ids'],\n",
    "        tokenized_data[model_name]['attention_masks'],\n",
    "        labels_polarity,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_polarity\n",
    "    )\n",
    "    X_train_ids_dict[model_name] = X_train_ids\n",
    "    X_test_ids_dict[model_name] = X_test_ids\n",
    "    X_train_masks_dict[model_name] = X_train_masks\n",
    "    X_test_masks_dict[model_name] = X_test_masks\n",
    "    y_train_polarity_dict[model_name] = y_train_pol\n",
    "    y_test_polarity_dict[model_name] = y_test_pol\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Model Building, Training, and Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "# Function to build and compile the model\n",
    "def build_model(pretrained_model_info, num_polarities, max_len=20):\n",
    "    \"\"\"\n",
    "    Builds a single-task model with shared pre-trained layers and a single output layer.\n",
    "    \"\"\"\n",
    "    tokenizer_class = pretrained_model_info['tokenizer']\n",
    "    model_class = pretrained_model_info['model']\n",
    "    pretrained_name = pretrained_model_info['pretrained_name']\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    try:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for {pretrained_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the model with TensorFlow weights first\n",
    "        base_model = model_class.from_pretrained(pretrained_name)\n",
    "    except OSError:\n",
    "        # If TensorFlow weights are unavailable, try loading PyTorch weights\n",
    "        print(f\"TensorFlow weights not found for {pretrained_name}. Attempting to load PyTorch weights.\")\n",
    "        try:\n",
    "            base_model = model_class.from_pretrained(pretrained_name, from_pt=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model for {pretrained_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # Get base model outputs\n",
    "    base_outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = base_outputs[1]  # Typically the [CLS] token representation\n",
    "\n",
    "    # Shared Dense layer\n",
    "    shared_dense = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    shared_dense = tf.keras.layers.Dropout(0.3)(shared_dense)\n",
    "\n",
    "    # Polarity output\n",
    "    polarity_output = tf.keras.layers.Dense(num_polarities, activation='softmax', name='polarity')(shared_dense)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[polarity_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss={\n",
    "            'polarity': 'sparse_categorical_crossentropy',\n",
    "        },\n",
    "        metrics={\n",
    "            'polarity': 'accuracy',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model, X_train_ids, X_train_masks, y_train_pol,\n",
    "                       X_test_ids, X_test_masks, y_test_pol, model_name, epochs=3, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    history = model.fit(\n",
    "        {'input_ids': X_train_ids, 'attention_mask': X_train_masks},\n",
    "        {'polarity': y_train_pol},\n",
    "        validation_data=(\n",
    "            {'input_ids': X_test_ids, 'attention_mask': X_test_masks},\n",
    "            {'polarity': y_test_pol}\n",
    "        ),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    predictions = model.predict({'input_ids': X_test_ids, 'attention_mask': X_test_masks})\n",
    "    \n",
    "    # For single-output models, predictions is a single NumPy array\n",
    "    # Apply argmax directly on the predictions array\n",
    "    pred_polarities = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Polarity Evaluation\n",
    "    print(f\"\\nPolarity Classification Report for {model_name}:\")\n",
    "    print(classification_report(y_test_pol, pred_polarities, target_names=polarity_encoder.classes_))\n",
    "\n",
    "    # Return history and predictions if needed\n",
    "    return history, pred_polarities\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in tokenized_data:\n",
    "        print(f\"Skipping model {model_name} due to previous errors.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nBuilding model for: {model_name}\")\n",
    "    pretrained_model_info = pretrained_models[model_name]\n",
    "    model, tokenizer = build_model(pretrained_model_info, num_polarities, max_len=20)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"Skipping training for {model_name} due to build errors.\")\n",
    "        continue\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    history, pred_polarities = train_and_evaluate(\n",
    "        model,\n",
    "        X_train_ids_dict[model_name],\n",
    "        X_train_masks_dict[model_name],\n",
    "        y_train_polarity_dict[model_name],\n",
    "        X_test_ids_dict[model_name],\n",
    "        X_test_masks_dict[model_name],\n",
    "        y_test_polarity_dict[model_name],\n",
    "        model_name,\n",
    "        epochs=3,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    save_dir = f'./fine_tuned_models/{model_name.replace(\"/\", \"_\")}_polarity'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    try:\n",
    "        model.save(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Model and tokenizer saved to {save_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model for {model_name}: {e}\")\n",
    "\n",
    "    # Store results\n",
    "    model_results[model_name] = {\n",
    "        'history': history,\n",
    "        'pred_polarities': pred_polarities\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models have been trained and evaluated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Optional: Compare Model Performances\n",
    "# -------------------------------\n",
    "\n",
    "# Example: Plotting polarity accuracy for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "model_labels = []\n",
    "\n",
    "for model_name in selected_models:\n",
    "    if model_name not in model_results:\n",
    "        continue\n",
    "    history = model_results[model_name]['history']\n",
    "    train_acc.append(history.history['polarity_accuracy'][-1])\n",
    "    val_acc.append(history.history['val_polarity_accuracy'][-1])\n",
    "    model_labels.append(model_name)\n",
    "\n",
    "x = np.arange(len(model_labels))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, train_acc, width, label='Train Accuracy')\n",
    "rects2 = ax.bar(x + width/2, val_acc, width, label='Validation Accuracy')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Polarity Classification Accuracy by Model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Attach a text label above each bar\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
