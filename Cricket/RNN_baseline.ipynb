{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Category and Polarity:\n",
      "          Category  Category_encoded  Polarity  Polarity_encoded\n",
      "0          bowling                 1  positive                 2\n",
      "1          bowling                 1  positive                 2\n",
      "2          batting                 0   neutral                 1\n",
      "3             team                 3   neutral                 1\n",
      "4  team management                 4   neutral                 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 128, 128)     15302016    ['input_ids[0][0]']              \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 128)          131584      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Category (Dense)               (None, 5)            645         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Polarity (Dense)               (None, 3)            387         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,434,632\n",
      "Trainable params: 15,434,632\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "238/238 [==============================] - 13s 27ms/step - loss: 2.7022 - Category_loss: 1.6052 - Polarity_loss: 1.0970 - Category_accuracy: 0.2618 - Polarity_accuracy: 0.3522 - val_loss: 2.6988 - val_Category_loss: 1.6027 - val_Polarity_loss: 1.0961 - val_Category_accuracy: 0.2322 - val_Polarity_accuracy: 0.3626\n",
      "Epoch 2/10\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 2.6916 - Category_loss: 1.5970 - Polarity_loss: 1.0947 - Category_accuracy: 0.2718 - Polarity_accuracy: 0.3829 - val_loss: 2.6881 - val_Category_loss: 1.5946 - val_Polarity_loss: 1.0935 - val_Category_accuracy: 0.2299 - val_Polarity_accuracy: 0.4171\n",
      "Epoch 3/10\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 2.6676 - Category_loss: 1.5783 - Polarity_loss: 1.0892 - Category_accuracy: 0.2714 - Polarity_accuracy: 0.4430 - val_loss: 2.6503 - val_Category_loss: 1.5652 - val_Polarity_loss: 1.0851 - val_Category_accuracy: 0.2299 - val_Polarity_accuracy: 0.4538\n",
      "Epoch 4/10\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 2.5450 - Category_loss: 1.4913 - Polarity_loss: 1.0537 - Category_accuracy: 0.3161 - Polarity_accuracy: 0.5042 - val_loss: 2.5130 - val_Category_loss: 1.4841 - val_Polarity_loss: 1.0289 - val_Category_accuracy: 0.3081 - val_Polarity_accuracy: 0.5379\n",
      "Epoch 5/10\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 2.4432 - Category_loss: 1.4240 - Polarity_loss: 1.0192 - Category_accuracy: 0.3813 - Polarity_accuracy: 0.5110 - val_loss: 2.4542 - val_Category_loss: 1.4531 - val_Polarity_loss: 1.0011 - val_Category_accuracy: 0.3436 - val_Polarity_accuracy: 0.5201\n",
      "Epoch 6/10\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 2.3814 - Category_loss: 1.3797 - Polarity_loss: 1.0017 - Category_accuracy: 0.4110 - Polarity_accuracy: 0.5156 - val_loss: 2.4148 - val_Category_loss: 1.4265 - val_Polarity_loss: 0.9884 - val_Category_accuracy: 0.3768 - val_Polarity_accuracy: 0.5284\n",
      "Epoch 7/10\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 2.3380 - Category_loss: 1.3495 - Polarity_loss: 0.9886 - Category_accuracy: 0.4344 - Polarity_accuracy: 0.5074 - val_loss: 2.4007 - val_Category_loss: 1.4184 - val_Polarity_loss: 0.9823 - val_Category_accuracy: 0.3874 - val_Polarity_accuracy: 0.5320\n",
      "Epoch 8/10\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 2.3097 - Category_loss: 1.3318 - Polarity_loss: 0.9779 - Category_accuracy: 0.4446 - Polarity_accuracy: 0.5155 - val_loss: 2.3718 - val_Category_loss: 1.3984 - val_Polarity_loss: 0.9734 - val_Category_accuracy: 0.4064 - val_Polarity_accuracy: 0.5379\n",
      "Epoch 9/10\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 2.2810 - Category_loss: 1.3128 - Polarity_loss: 0.9682 - Category_accuracy: 0.4527 - Polarity_accuracy: 0.5214 - val_loss: 2.3553 - val_Category_loss: 1.3865 - val_Polarity_loss: 0.9689 - val_Category_accuracy: 0.4242 - val_Polarity_accuracy: 0.5344\n",
      "Epoch 10/10\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 2.2540 - Category_loss: 1.2973 - Polarity_loss: 0.9567 - Category_accuracy: 0.4591 - Polarity_accuracy: 0.5318 - val_loss: 2.3387 - val_Category_loss: 1.3754 - val_Polarity_loss: 0.9633 - val_Category_accuracy: 0.4265 - val_Polarity_accuracy: 0.5391\n",
      "66/66 [==============================] - 0s 6ms/step - loss: 2.2855 - Category_loss: 1.3212 - Polarity_loss: 0.9643 - Category_accuracy: 0.4583 - Polarity_accuracy: 0.5157\n",
      "Test Loss and Accuracy: [2.2854666709899902, 1.321150779724121, 0.96431565284729, 0.45825427770614624, 0.5156546235084534]\n",
      "66/66 [==============================] - 2s 5ms/step\n",
      "Classification Report for Category:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        batting     0.4000    0.5324    0.4568       447\n",
      "        bowling     0.5277    0.8632    0.6550       563\n",
      "          other     1.0000    0.0027    0.0054       370\n",
      "           team     0.9167    0.0258    0.0502       426\n",
      "team management     0.3972    0.7616    0.5221       302\n",
      "\n",
      "       accuracy                         0.4583      2108\n",
      "      macro avg     0.6483    0.4372    0.3379      2108\n",
      "   weighted avg     0.6434    0.4583    0.3577      2108\n",
      "\n",
      "Classification Report for Polarity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5039    0.7283    0.5957       703\n",
      "     neutral     0.4461    0.2119    0.2874       703\n",
      "    positive     0.5620    0.6068    0.5836       702\n",
      "\n",
      "    accuracy                         0.5157      2108\n",
      "   macro avg     0.5040    0.5157    0.4889      2108\n",
      "weighted avg     0.5040    0.5157    0.4888      2108\n",
      "\n",
      "\n",
      "Macro Metrics for Category:\n",
      "Precision: 0.6483\n",
      "Recall:    0.4372\n",
      "F1-Score:  0.3379\n",
      "\n",
      "Macro Metrics for Polarity:\n",
      "Precision: 0.5040\n",
      "Recall:    0.5157\n",
      "F1-Score:  0.4889\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "# Note: NLTK does not include Bengali stopwords by default\n",
    "# Use a custom list or skip stopword removal if unavailable\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "except OSError:\n",
    "    print(\"Bengali stopwords not found. Skipping stopword removal.\")\n",
    "    stop_words = set()\n",
    "\n",
    "# Initialize lemmatizer (WordNetLemmatizer is for English)\n",
    "# Consider removing lemmatization for Bengali or use a Bengali-specific lemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load and Preprocess the Dataset\n",
    "# -------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"F:\\Context-Resonance Transformer\\Cricket\\Cricket - Sheet1.csv\")\n",
    "df = df[['Text', 'Category', 'Polarity']]\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep only Bengali characters\n",
    "    text = re.sub(r'\\d+', '', text)                 # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # Remove extra spaces\n",
    "    words = text.split()\n",
    "    if stop_words:\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "    # Optionally, remove lemmatization if not suitable for Bengali\n",
    "    # words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Upsampling for Class Balance\n",
    "# -------------------------------\n",
    "\n",
    "def upsample(df, target_column):\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        df_label = df[df[target_column] == label]\n",
    "        if len(df_label) < max_count:\n",
    "            df_upsampled = resample(\n",
    "                df_label,\n",
    "                replace=True,\n",
    "                n_samples=max_count,\n",
    "                random_state=42\n",
    "            )\n",
    "            upsampled_dfs.append(df_upsampled)\n",
    "        else:\n",
    "            upsampled_dfs.append(df_label)\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Upsample 'Category' and 'Polarity' separately\n",
    "# Note: Upsampling both can lead to a large dataset\n",
    "df = upsample(df, 'Category')\n",
    "df = upsample(df, 'Polarity')\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Encode Labels\n",
    "# -------------------------------\n",
    "\n",
    "label_encoder_cat = LabelEncoder()\n",
    "label_encoder_pol = LabelEncoder()\n",
    "\n",
    "df['Category_encoded'] = label_encoder_cat.fit_transform(df['Category'])\n",
    "df['Polarity_encoded'] = label_encoder_pol.fit_transform(df['Polarity'])\n",
    "\n",
    "# Display encoded labels\n",
    "print(\"Encoded Category and Polarity:\")\n",
    "print(df[['Category', 'Category_encoded', 'Polarity', 'Polarity_encoded']].head())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenize the Text\n",
    "# -------------------------------\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def tokenize_data(df, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in df['Text']:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,      # Add '[CLS]' and '[SEP]'\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    # Convert lists to tensors\n",
    "    input_ids = tf.concat(input_ids, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks, axis=0)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "input_ids, attention_masks = tokenize_data(df)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Split the Data\n",
    "# -------------------------------\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, att_mask_train, att_mask_test, y_cat_train, y_cat_test, y_pol_train, y_pol_test = train_test_split(\n",
    "    input_ids.numpy(), attention_masks.numpy(),\n",
    "    df['Category_encoded'].values, df['Polarity_encoded'].values,\n",
    "    test_size=0.2, random_state=42, stratify=df[['Category_encoded', 'Polarity_encoded']]\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. One-Hot Encode the Labels\n",
    "# -------------------------------\n",
    "\n",
    "num_cat_classes = len(label_encoder_cat.classes_)\n",
    "num_pol_classes = len(label_encoder_pol.classes_)\n",
    "\n",
    "y_cat_train = to_categorical(y_cat_train, num_classes=num_cat_classes)\n",
    "y_cat_test = to_categorical(y_cat_test, num_classes=num_cat_classes)\n",
    "y_pol_train = to_categorical(y_pol_train, num_classes=num_pol_classes)\n",
    "y_pol_test = to_categorical(y_pol_test, num_classes=num_pol_classes)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Define the Multi-Task LSTM Model\n",
    "# -------------------------------\n",
    "\n",
    "# Define input layers\n",
    "input_ids_layer = Input(shape=(128,), dtype='int32', name='input_ids')\n",
    "attention_mask_layer = Input(shape=(128,), dtype='int32', name='attention_mask')\n",
    "\n",
    "# Embedding layer with mask_zero=True to handle padding\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=tokenizer.vocab_size, \n",
    "    output_dim=128, \n",
    "    input_length=128, \n",
    "    mask_zero=True\n",
    ")(input_ids_layer)\n",
    "\n",
    "# Shared LSTM layer (unidirectional)\n",
    "lstm_layer = LSTM(128, return_sequences=False)(embedding_layer)\n",
    "dropout_layer = Dropout(0.3)(lstm_layer)\n",
    "\n",
    "# Task-specific Dense layers\n",
    "category_output = Dense(num_cat_classes, activation='softmax', name='Category')(dropout_layer)\n",
    "polarity_output = Dense(num_pol_classes, activation='softmax', name='Polarity')(dropout_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=[category_output, polarity_output])\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Compile the Model\n",
    "# -------------------------------\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss={\n",
    "        'Category': 'categorical_crossentropy',\n",
    "        'Polarity': 'categorical_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'Category': 'accuracy',\n",
    "        'Polarity': 'accuracy'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Train the Model with EarlyStopping\n",
    "# -------------------------------\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, att_mask_train],\n",
    "    {'Category': y_cat_train, 'Polarity': y_pol_train},\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Evaluate the Model\n",
    "# -------------------------------\n",
    "\n",
    "results = model.evaluate(\n",
    "    [X_test, att_mask_test],\n",
    "    {'Category': y_cat_test, 'Polarity': y_pol_test}\n",
    ")\n",
    "\n",
    "print(f\"Test Loss and Accuracy: {results}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Classification Report with Macro Metrics\n",
    "# -------------------------------\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict([X_test, att_mask_test])\n",
    "\n",
    "# Convert predictions and true labels from one-hot to integer labels\n",
    "y_cat_pred = np.argmax(predictions[0], axis=1)\n",
    "y_pol_pred = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "y_cat_true = np.argmax(y_cat_test, axis=1)\n",
    "y_pol_true = np.argmax(y_pol_test, axis=1)\n",
    "\n",
    "# Generate classification reports\n",
    "report_cat = classification_report(\n",
    "    y_cat_true, y_cat_pred, \n",
    "    target_names=label_encoder_cat.classes_, \n",
    "    digits=4,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "report_pol = classification_report(\n",
    "    y_pol_true, y_pol_pred, \n",
    "    target_names=label_encoder_pol.classes_, \n",
    "    digits=4,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(\"Classification Report for Category:\")\n",
    "print(report_cat)\n",
    "\n",
    "print(\"Classification Report for Polarity:\")\n",
    "print(report_pol)\n",
    "\n",
    "# Extract and print macro precision, recall, F1-score\n",
    "def extract_macro_metrics(report):\n",
    "    report_dict = classification_report(\n",
    "        y_true, y_pred, \n",
    "        target_names=label_encoder.classes_, \n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "        output_dict=True\n",
    "    )\n",
    "    macro_p = report_dict['macro avg']['precision']\n",
    "    macro_r = report_dict['macro avg']['recall']\n",
    "    macro_f1 = report_dict['macro avg']['f1-score']\n",
    "    return macro_p, macro_r, macro_f1\n",
    "\n",
    "# For Category\n",
    "report_cat_dict = classification_report(\n",
    "    y_cat_true, y_cat_pred, \n",
    "    target_names=label_encoder_cat.classes_, \n",
    "    digits=4,\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "macro_p_cat = report_cat_dict['macro avg']['precision']\n",
    "macro_r_cat = report_cat_dict['macro avg']['recall']\n",
    "macro_f1_cat = report_cat_dict['macro avg']['f1-score']\n",
    "\n",
    "print(\"\\nMacro Metrics for Category:\")\n",
    "print(f\"Precision: {macro_p_cat:.4f}\")\n",
    "print(f\"Recall:    {macro_r_cat:.4f}\")\n",
    "print(f\"F1-Score:  {macro_f1_cat:.4f}\")\n",
    "\n",
    "# For Polarity\n",
    "report_pol_dict = classification_report(\n",
    "    y_pol_true, y_pol_pred, \n",
    "    target_names=label_encoder_pol.classes_, \n",
    "    digits=4,\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "macro_p_pol = report_pol_dict['macro avg']['precision']\n",
    "macro_r_pol = report_pol_dict['macro avg']['recall']\n",
    "macro_f1_pol = report_pol_dict['macro avg']['f1-score']\n",
    "\n",
    "print(\"\\nMacro Metrics for Polarity:\")\n",
    "print(f\"Precision: {macro_p_pol:.4f}\")\n",
    "print(f\"Recall:    {macro_r_pol:.4f}\")\n",
    "print(f\"F1-Score:  {macro_f1_pol:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout, Multiply, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "237/237 [==============================] - 9s 24ms/step - loss: 2.7052 - Category_loss: 1.6070 - Polarity_loss: 1.0983 - Category_accuracy: 0.2577 - Polarity_accuracy: 0.3655 - val_loss: 2.7031 - val_Category_loss: 1.6052 - val_Polarity_loss: 1.0979 - val_Category_accuracy: 0.2536 - val_Polarity_accuracy: 0.4369\n",
      "Epoch 2/10\n",
      "237/237 [==============================] - 5s 19ms/step - loss: 2.7004 - Category_loss: 1.6030 - Polarity_loss: 1.0974 - Category_accuracy: 0.2660 - Polarity_accuracy: 0.3894 - val_loss: 2.6985 - val_Category_loss: 1.6014 - val_Polarity_loss: 1.0971 - val_Category_accuracy: 0.2548 - val_Polarity_accuracy: 0.4476\n",
      "Epoch 3/10\n",
      "237/237 [==============================] - 5s 19ms/step - loss: 2.6953 - Category_loss: 1.5989 - Polarity_loss: 1.0964 - Category_accuracy: 0.2663 - Polarity_accuracy: 0.4067 - val_loss: 2.6936 - val_Category_loss: 1.5974 - val_Polarity_loss: 1.0962 - val_Category_accuracy: 0.2548 - val_Polarity_accuracy: 0.4643\n",
      "Epoch 4/10\n",
      "237/237 [==============================] - 5s 20ms/step - loss: 2.6895 - Category_loss: 1.5942 - Polarity_loss: 1.0953 - Category_accuracy: 0.2649 - Polarity_accuracy: 0.4172 - val_loss: 2.6882 - val_Category_loss: 1.5931 - val_Polarity_loss: 1.0951 - val_Category_accuracy: 0.2548 - val_Polarity_accuracy: 0.4881\n",
      "Epoch 5/10\n",
      "237/237 [==============================] - 5s 20ms/step - loss: 2.6832 - Category_loss: 1.5895 - Polarity_loss: 1.0937 - Category_accuracy: 0.2648 - Polarity_accuracy: 0.4323 - val_loss: 2.6827 - val_Category_loss: 1.5889 - val_Polarity_loss: 1.0938 - val_Category_accuracy: 0.2548 - val_Polarity_accuracy: 0.4845\n",
      "Epoch 6/10\n",
      "237/237 [==============================] - 5s 22ms/step - loss: 2.6771 - Category_loss: 1.5849 - Polarity_loss: 1.0922 - Category_accuracy: 0.2648 - Polarity_accuracy: 0.4366 - val_loss: 2.6777 - val_Category_loss: 1.5854 - val_Polarity_loss: 1.0923 - val_Category_accuracy: 0.2548 - val_Polarity_accuracy: 0.4619\n",
      "Epoch 7/10\n",
      "237/237 [==============================] - 5s 22ms/step - loss: 2.6704 - Category_loss: 1.5810 - Polarity_loss: 1.0894 - Category_accuracy: 0.2648 - Polarity_accuracy: 0.4549 - val_loss: 2.6723 - val_Category_loss: 1.5823 - val_Polarity_loss: 1.0900 - val_Category_accuracy: 0.2548 - val_Polarity_accuracy: 0.4762\n",
      "Epoch 8/10\n",
      "237/237 [==============================] - 5s 21ms/step - loss: 2.6627 - Category_loss: 1.5768 - Polarity_loss: 1.0859 - Category_accuracy: 0.2648 - Polarity_accuracy: 0.4585 - val_loss: 2.6639 - val_Category_loss: 1.5775 - val_Polarity_loss: 1.0863 - val_Category_accuracy: 0.2548 - val_Polarity_accuracy: 0.4571\n",
      "Epoch 9/10\n",
      "237/237 [==============================] - 5s 20ms/step - loss: 2.6477 - Category_loss: 1.5688 - Polarity_loss: 1.0789 - Category_accuracy: 0.2648 - Polarity_accuracy: 0.4811 - val_loss: 2.6439 - val_Category_loss: 1.5652 - val_Polarity_loss: 1.0787 - val_Category_accuracy: 0.2548 - val_Polarity_accuracy: 0.4357\n",
      "Epoch 10/10\n",
      "237/237 [==============================] - 5s 19ms/step - loss: 2.5815 - Category_loss: 1.5236 - Polarity_loss: 1.0579 - Category_accuracy: 0.2811 - Polarity_accuracy: 0.4865 - val_loss: 2.5405 - val_Category_loss: 1.4928 - val_Polarity_loss: 1.0477 - val_Category_accuracy: 0.3060 - val_Polarity_accuracy: 0.5012\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 2.5317 - Category_loss: 1.4877 - Polarity_loss: 1.0440 - Category_accuracy: 0.3333 - Polarity_accuracy: 0.5271\n",
      "Test Loss and Accuracy: [2.5316624641418457, 1.4876947402954102, 1.0439684391021729, 0.3333333432674408, 0.5271428823471069]\n",
      "66/66 [==============================] - 1s 4ms/step\n",
      "Classification Report for Category:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.10      0.17       458\n",
      "           1       0.34      0.96      0.50       583\n",
      "           2       0.21      0.01      0.02       343\n",
      "           3       0.29      0.22      0.25       406\n",
      "           4       0.00      0.00      0.00       310\n",
      "\n",
      "    accuracy                           0.33      2100\n",
      "   macro avg       0.25      0.26      0.19      2100\n",
      "weighted avg       0.28      0.33      0.23      2100\n",
      "\n",
      "\n",
      "Classification Report for Polarity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.68      0.58       718\n",
      "           1       0.49      0.26      0.34       689\n",
      "           2       0.58      0.64      0.61       693\n",
      "\n",
      "    accuracy                           0.53      2100\n",
      "   macro avg       0.52      0.52      0.51      2100\n",
      "weighted avg       0.52      0.53      0.51      2100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define the multi-task GRU model\n",
    "input_ids_layer = Input(shape=(128,), dtype='int32', name='input_ids')\n",
    "attention_mask_layer = Input(shape=(128,), dtype='int32', name='attention_mask')\n",
    "\n",
    "# Embedding layer\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=tokenizer.vocab_size, \n",
    "    output_dim=128, \n",
    "    input_length=128, \n",
    "    mask_zero=True\n",
    ")(input_ids_layer)\n",
    "\n",
    "# Apply the attention mask to the embeddings\n",
    "mask_float = Lambda(lambda x: K.cast(x, dtype='float32'))(attention_mask_layer)\n",
    "masked_embedding = Multiply()([embedding_layer, mask_float])\n",
    "\n",
    "# Shared GRU layers\n",
    "gru_layer = GRU(128, return_sequences=False)(masked_embedding)\n",
    "dropout_layer = Dropout(0.3)(gru_layer)\n",
    "\n",
    "# Task-specific output layers\n",
    "category_output = Dense(len(y_cat_train[0]), activation='softmax', name='Category')(dropout_layer)\n",
    "polarity_output = Dense(len(y_polarity_train[0]), activation='softmax', name='Polarity')(dropout_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=[category_output, polarity_output])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss={'Category': 'categorical_crossentropy', 'Polarity': 'categorical_crossentropy'},\n",
    "    metrics={'Category': 'accuracy', 'Polarity': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, att_mask_train],\n",
    "    {'Category': y_cat_train, 'Polarity': y_polarity_train},\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(\n",
    "    [X_test, att_mask_test],\n",
    "    {'Category': y_cat_test, 'Polarity': y_polarity_test}\n",
    ")\n",
    "\n",
    "print(f\"Test Loss and Accuracy: {results}\")\n",
    "\n",
    "# Classification Report\n",
    "predictions = model.predict([X_test, att_mask_test])\n",
    "y_cat_pred = np.argmax(predictions[0], axis=1)\n",
    "y_polarity_pred = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "print(\"Classification Report for Category:\")\n",
    "print(classification_report(np.argmax(y_cat_test, axis=1), y_cat_pred))\n",
    "\n",
    "print(\"\\nClassification Report for Polarity:\")\n",
    "print(classification_report(np.argmax(y_polarity_test, axis=1), y_polarity_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category distribution after upsampling:\n",
      "Category\n",
      "bowling            2799\n",
      "batting            2226\n",
      "team               2094\n",
      "other              1913\n",
      "team management    1468\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Polarity distribution after upsampling:\n",
      "Polarity\n",
      "negative    3500\n",
      "neutral     3500\n",
      "positive    3500\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: (10500, 128)\n",
      "Attention masks shape: (10500, 128)\n",
      "Label 1 (Category) shape: (10500,)\n",
      "Label 2 (Polarity) shape: (10500,)\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_20 (Embedding)       (None, 128, 128)     15302016    ['input_ids[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional_9 (Bidirectional  (None, 128, 256)    198144      ['embedding_20[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_10 (Bidirectiona  (None, 128)         123648      ['bidirectional_9[0][0]']        \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           8256        ['bidirectional_10[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 64)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Category (Dense)               (None, 5)            325         ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " Polarity (Dense)               (None, 4)            260         ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,632,649\n",
      "Trainable params: 15,632,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "237/237 [==============================] - 25s 56ms/step - loss: 2.3187 - Category_loss: 1.3008 - Polarity_loss: 1.0178 - Category_accuracy: 0.4497 - Polarity_accuracy: 0.4788 - val_loss: 1.8738 - val_Category_loss: 0.9987 - val_Polarity_loss: 0.8751 - val_Category_accuracy: 0.6333 - val_Polarity_accuracy: 0.5988\n",
      "Epoch 2/10\n",
      "237/237 [==============================] - 12s 51ms/step - loss: 1.5727 - Category_loss: 0.8546 - Polarity_loss: 0.7181 - Category_accuracy: 0.7041 - Polarity_accuracy: 0.6911 - val_loss: 1.3996 - val_Category_loss: 0.7918 - val_Polarity_loss: 0.6078 - val_Category_accuracy: 0.7190 - val_Polarity_accuracy: 0.7607\n",
      "Epoch 3/10\n",
      "237/237 [==============================] - 12s 51ms/step - loss: 1.2178 - Category_loss: 0.7037 - Polarity_loss: 0.5141 - Category_accuracy: 0.7593 - Polarity_accuracy: 0.8045 - val_loss: 1.1191 - val_Category_loss: 0.6334 - val_Polarity_loss: 0.4858 - val_Category_accuracy: 0.7976 - val_Polarity_accuracy: 0.8286\n",
      "Epoch 4/10\n",
      "237/237 [==============================] - 12s 51ms/step - loss: 1.0004 - Category_loss: 0.6012 - Polarity_loss: 0.3992 - Category_accuracy: 0.7935 - Polarity_accuracy: 0.8574 - val_loss: 1.0351 - val_Category_loss: 0.6289 - val_Polarity_loss: 0.4061 - val_Category_accuracy: 0.7833 - val_Polarity_accuracy: 0.8524\n",
      "Epoch 5/10\n",
      "237/237 [==============================] - 12s 50ms/step - loss: 0.8591 - Category_loss: 0.5390 - Polarity_loss: 0.3201 - Category_accuracy: 0.8142 - Polarity_accuracy: 0.8915 - val_loss: 0.9659 - val_Category_loss: 0.5916 - val_Polarity_loss: 0.3742 - val_Category_accuracy: 0.7964 - val_Polarity_accuracy: 0.8893\n",
      "Epoch 6/10\n",
      "237/237 [==============================] - 12s 50ms/step - loss: 0.7342 - Category_loss: 0.4674 - Polarity_loss: 0.2668 - Category_accuracy: 0.8429 - Polarity_accuracy: 0.9116 - val_loss: 0.8271 - val_Category_loss: 0.5234 - val_Polarity_loss: 0.3037 - val_Category_accuracy: 0.8155 - val_Polarity_accuracy: 0.9107\n",
      "Epoch 7/10\n",
      "237/237 [==============================] - 12s 50ms/step - loss: 0.6805 - Category_loss: 0.4423 - Polarity_loss: 0.2381 - Category_accuracy: 0.8481 - Polarity_accuracy: 0.9239 - val_loss: 0.7845 - val_Category_loss: 0.4875 - val_Polarity_loss: 0.2971 - val_Category_accuracy: 0.8464 - val_Polarity_accuracy: 0.9071\n",
      "Epoch 8/10\n",
      "237/237 [==============================] - 12s 50ms/step - loss: 0.5710 - Category_loss: 0.3878 - Polarity_loss: 0.1833 - Category_accuracy: 0.8683 - Polarity_accuracy: 0.9454 - val_loss: 0.7301 - val_Category_loss: 0.4780 - val_Polarity_loss: 0.2521 - val_Category_accuracy: 0.8595 - val_Polarity_accuracy: 0.9214\n",
      "Epoch 9/10\n",
      "237/237 [==============================] - 12s 50ms/step - loss: 0.4960 - Category_loss: 0.3389 - Polarity_loss: 0.1570 - Category_accuracy: 0.8839 - Polarity_accuracy: 0.9512 - val_loss: 0.6748 - val_Category_loss: 0.4510 - val_Polarity_loss: 0.2238 - val_Category_accuracy: 0.8786 - val_Polarity_accuracy: 0.9345\n",
      "Epoch 10/10\n",
      "237/237 [==============================] - 12s 51ms/step - loss: 0.4616 - Category_loss: 0.3176 - Polarity_loss: 0.1440 - Category_accuracy: 0.8915 - Polarity_accuracy: 0.9557 - val_loss: 0.6545 - val_Category_loss: 0.4529 - val_Polarity_loss: 0.2016 - val_Category_accuracy: 0.8726 - val_Polarity_accuracy: 0.9417\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.5707 - Category_loss: 0.3866 - Polarity_loss: 0.1841 - Category_accuracy: 0.8752 - Polarity_accuracy: 0.9490\n",
      "Test Loss and Accuracy: [0.5706775188446045, 0.3865610957145691, 0.18411637842655182, 0.8752381205558777, 0.9490476250648499]\n",
      "66/66 [==============================] - 2s 20ms/step\n",
      "Classification Report for Category:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.88       458\n",
      "           1       0.89      0.95      0.92       583\n",
      "           2       0.86      0.86      0.86       343\n",
      "           3       0.82      0.78      0.80       406\n",
      "           4       0.88      0.91      0.90       310\n",
      "\n",
      "    accuracy                           0.88      2100\n",
      "   macro avg       0.87      0.87      0.87      2100\n",
      "weighted avg       0.87      0.88      0.87      2100\n",
      "\n",
      "\n",
      "Classification Report for Polarity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94       718\n",
      "           1       0.95      0.98      0.96       689\n",
      "           2       0.92      0.96      0.94       693\n",
      "\n",
      "    accuracy                           0.95      2100\n",
      "   macro avg       0.95      0.95      0.95      2100\n",
      "weighted avg       0.95      0.95      0.95      2100\n",
      "\n",
      "\n",
      "Macro-Averaged Scores for Category:\n",
      "Precision: 0.8724, Recall: 0.8701, F1 Score: 0.8707\n",
      "\n",
      "Macro-Averaged Scores for Polarity:\n",
      "Precision: 0.9496, Recall: 0.9496, F1 Score: 0.9491\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Layer, GRU, Bidirectional, Dense, Input, Reshape, GlobalAveragePooling1D\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv(r\"F:\\Context-Resonance Transformer\\Cricket\\Cricket - Sheet1.csv\")\n",
    "df.head()\n",
    "df = df[['Text', 'Category', 'Polarity']]\n",
    "df.head()\n",
    "df['Category'].value_counts()\n",
    "df['Polarity'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep only Bengali characters\n",
    "    text = re.sub(r'\\d+', '', text)                 # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "df.head()\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Category' and 'Polarity'\n",
    "df_upsampled_category = upsample(df, 'Category')\n",
    "df_upsampled_polarity = upsample(df_upsampled_category, 'Polarity')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled_polarity.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Category distribution after upsampling:\")\n",
    "print(df_upsampled['Category'].value_counts())\n",
    "print(\"\\nPolarity distribution after upsampling:\")\n",
    "print(df_upsampled['Polarity'].value_counts())\n",
    "\n",
    "df_upsampled.head()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "category_encoder = LabelEncoder()\n",
    "polarity_encoder = LabelEncoder()\n",
    "\n",
    "df_upsampled['Category_encoded'] = category_encoder.fit_transform(df_upsampled['Category'])\n",
    "df_upsampled['Polarity_encoded'] = polarity_encoder.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Tokenize the text using DistilBERT with padding and truncation\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')\n",
    "\n",
    "df_upsampled['tokens'] = df_upsampled['Text'].apply(lambda x: tokenize_function(x))\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df_upsampled, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Convert to TensorFlow Dataset\n",
    "def create_tensor_dataset(df):\n",
    "    # Tokenize input text and convert to TensorFlow tensors\n",
    "    inputs = tokenizer(list(df['Text']), padding='max_length', truncation=True, max_length=128, return_tensors='tf')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Convert labels to tensors\n",
    "    labels_category = tf.convert_to_tensor(df['Category_encoded'].values)\n",
    "    labels_polarity = tf.convert_to_tensor(df['Polarity_encoded'].values)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices(((input_ids, attention_mask), (labels_category, labels_polarity)))\n",
    "\n",
    "def tokenize_data(df_upsampled, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sentence in df_upsampled['Text']:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True, \n",
    "            max_length=max_length,    \n",
    "            padding='max_length',    \n",
    "            truncation=True,           \n",
    "            return_attention_mask=True, \n",
    "            return_tensors='tf'        \n",
    "        )\n",
    "        \n",
    "        # Append to lists\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    \n",
    "    # Squeeze the extra dimension\n",
    "    input_ids = tf.squeeze(input_ids, axis=1)\n",
    "    attention_masks = tf.squeeze(attention_masks, axis=1)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "input_ids, attention_masks = tokenize_data(df_upsampled)\n",
    "\n",
    "label_1 = tf.convert_to_tensor(df_upsampled['Category'])\n",
    "label_2 = tf.convert_to_tensor(df_upsampled['Polarity'])\n",
    "\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention masks shape: {attention_masks.shape}\")\n",
    "print(f\"Label 1 (Category) shape: {label_1.shape}\")\n",
    "print(f\"Label 2 (Polarity) shape: {label_2.shape}\")\n",
    "# Ensure input_ids and attention_masks are converted to integer type tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks, dtype=tf.int32)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoders for the string labels\n",
    "label_encoder_1 = LabelEncoder()\n",
    "label_encoder_2 = LabelEncoder()\n",
    "\n",
    "# Encode string labels into integers\n",
    "df_upsampled['Category'] = label_encoder_1.fit_transform(df_upsampled['Category'])\n",
    "df_upsampled['Polarity'] = label_encoder_2.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Convert labels to TensorFlow tensors\n",
    "label_1 = tf.convert_to_tensor(df_upsampled['Category'], dtype=tf.int32)\n",
    "label_2 = tf.convert_to_tensor(df_upsampled['Polarity'], dtype=tf.int32)\n",
    "\n",
    "# Ensure input_ids and attention_masks are correctly formatted as tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks, dtype=tf.int32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, att_mask_train, att_mask_test, y_cat_train, y_cat_test, y_gender_train, y_gender_test = train_test_split(\n",
    "    input_ids.numpy(), attention_masks.numpy(),\n",
    "    label_1.numpy(), label_2.numpy(),  \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_bilstm_model(input_shape):\n",
    "    input_ids = tf.keras.layers.Input(shape=(input_shape,), dtype='int32', name='input_ids')\n",
    "    attention_masks = tf.keras.layers.Input(shape=(input_shape,), dtype='int32', name='attention_masks')\n",
    "\n",
    "    # Embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=128)(input_ids)\n",
    "\n",
    "    # First BiLSTM layer with dropout\n",
    "    lstm_output = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(128, return_sequences=True, dropout=0.3))(embedding_layer)\n",
    "\n",
    "    # Second BiLSTM layer\n",
    "    lstm_output_2 = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(64, return_sequences=False, dropout=0.3))(lstm_output)\n",
    "\n",
    "    # Dense layer before output layers\n",
    "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(lstm_output_2)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    dropout_layer = tf.keras.layers.Dropout(0.3)(dense_layer)\n",
    "\n",
    "    # Output layers for multi-task learning\n",
    "    output_category = tf.keras.layers.Dense(5, activation='softmax', name='Category')(dropout_layer)\n",
    "    output_polarity = tf.keras.layers.Dense(4, activation='softmax', name='Polarity')(dropout_layer)\n",
    "\n",
    "    # Define the model with inputs and outputs\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_masks],\n",
    "                           outputs=[output_category, output_polarity])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the BiLSTM model\n",
    "bilstm_model = create_bilstm_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "bilstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'Category': 'sparse_categorical_crossentropy', 'Polarity': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'Category': 'accuracy', 'Polarity': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "bilstm_model.summary()\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "         \n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=3,          # Stop training after 3 epochs of no improvement\n",
    "    restore_best_weights=True  # Restore the best weights from the epoch with the lowest validation loss\n",
    ")\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "history = bilstm_model.fit(\n",
    "    [X_train, att_mask_train],  # Inputs\n",
    "    {'Category': y_cat_train, 'Polarity': y_gender_train},  # Outputs\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]  # Add the EarlyStopping callback\n",
    ")\n",
    "# Evaluate the model on the test set\n",
    "results = bilstm_model.evaluate(\n",
    "    [X_test, att_mask_test],\n",
    "    {'Category': y_cat_test, 'Polarity': y_gender_test}\n",
    ")\n",
    "\n",
    "print(f\"Test Loss and Accuracy: {results}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Get Predictions\n",
    "predictions = bilstm_model.predict([X_test, att_mask_test])\n",
    "\n",
    "# Step 2: Convert predictions to class labels\n",
    "y_cat_pred = np.argmax(predictions[0], axis=1)   # 'Category' prediction\n",
    "y_gender_pred = np.argmax(predictions[1], axis=1)  # 'Polarity' prediction\n",
    "\n",
    "# Step 3: Generate Classification Report with zero_division specified\n",
    "# For Category\n",
    "print(\"Classification Report for Category:\")\n",
    "print(classification_report(y_cat_test, y_cat_pred, zero_division=0))\n",
    "\n",
    "# For Polarity\n",
    "print(\"\\nClassification Report for Polarity:\")\n",
    "print(classification_report(y_gender_test, y_gender_pred, zero_division=0))\n",
    "\n",
    "# If you want the macro-averaged precision, recall, and F1 scores separately:\n",
    "cat_precision = precision_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "cat_recall = recall_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "cat_f1 = f1_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "\n",
    "gender_precision = precision_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "gender_recall = recall_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "gender_f1 = f1_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\nMacro-Averaged Scores for Category:\")\n",
    "print(f\"Precision: {cat_precision:.4f}, Recall: {cat_recall:.4f}, F1 Score: {cat_f1:.4f}\")\n",
    "\n",
    "print(\"\\nMacro-Averaged Scores for Polarity:\")\n",
    "print(f\"Precision: {gender_precision:.4f}, Recall: {gender_recall:.4f}, F1 Score: {gender_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category distribution after upsampling:\n",
      "Category\n",
      "bowling            2799\n",
      "batting            2226\n",
      "team               2094\n",
      "other              1913\n",
      "team management    1468\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Polarity distribution after upsampling:\n",
      "Polarity\n",
      "negative    3500\n",
      "neutral     3500\n",
      "positive    3500\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: (10500, 128)\n",
      "Attention masks shape: (10500, 128)\n",
      "Label 1 (Category) shape: (10500,)\n",
      "Label 2 (Polarity) shape: (10500,)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 128, 128)     15302016    ['input_ids[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 128, 256)     263168      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 128)         164352      ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           8256        ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Category (Dense)               (None, 5)            325         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Polarity (Dense)               (None, 4)            260         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,738,377\n",
      "Trainable params: 15,738,377\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "237/237 [==============================] - 23s 67ms/step - loss: 2.3854 - Category_loss: 1.3320 - Polarity_loss: 1.0533 - Category_accuracy: 0.4237 - Polarity_accuracy: 0.4714 - val_loss: 1.9102 - val_Category_loss: 1.0568 - val_Polarity_loss: 0.8535 - val_Category_accuracy: 0.6250 - val_Polarity_accuracy: 0.6119\n",
      "Epoch 2/10\n",
      "237/237 [==============================] - 14s 60ms/step - loss: 1.6017 - Category_loss: 0.9027 - Polarity_loss: 0.6991 - Category_accuracy: 0.6745 - Polarity_accuracy: 0.7143 - val_loss: 1.3704 - val_Category_loss: 0.7542 - val_Polarity_loss: 0.6162 - val_Category_accuracy: 0.7369 - val_Polarity_accuracy: 0.7810\n",
      "Epoch 3/10\n",
      "237/237 [==============================] - 14s 60ms/step - loss: 1.1876 - Category_loss: 0.7092 - Polarity_loss: 0.4785 - Category_accuracy: 0.7624 - Polarity_accuracy: 0.8238 - val_loss: 1.1507 - val_Category_loss: 0.6588 - val_Polarity_loss: 0.4919 - val_Category_accuracy: 0.7536 - val_Polarity_accuracy: 0.8107\n",
      "Epoch 4/10\n",
      "237/237 [==============================] - 14s 60ms/step - loss: 0.9549 - Category_loss: 0.5942 - Polarity_loss: 0.3607 - Category_accuracy: 0.8063 - Polarity_accuracy: 0.8763 - val_loss: 0.9806 - val_Category_loss: 0.6106 - val_Polarity_loss: 0.3700 - val_Category_accuracy: 0.7988 - val_Polarity_accuracy: 0.8679\n",
      "Epoch 5/10\n",
      "237/237 [==============================] - 14s 59ms/step - loss: 0.7968 - Category_loss: 0.5096 - Polarity_loss: 0.2872 - Category_accuracy: 0.8352 - Polarity_accuracy: 0.9074 - val_loss: 0.9505 - val_Category_loss: 0.6089 - val_Polarity_loss: 0.3416 - val_Category_accuracy: 0.7976 - val_Polarity_accuracy: 0.8798\n",
      "Epoch 6/10\n",
      "237/237 [==============================] - 14s 60ms/step - loss: 0.6791 - Category_loss: 0.4477 - Polarity_loss: 0.2315 - Category_accuracy: 0.8524 - Polarity_accuracy: 0.9249 - val_loss: 0.7835 - val_Category_loss: 0.5013 - val_Polarity_loss: 0.2822 - val_Category_accuracy: 0.8381 - val_Polarity_accuracy: 0.9155\n",
      "Epoch 7/10\n",
      "237/237 [==============================] - 15s 61ms/step - loss: 0.6008 - Category_loss: 0.4126 - Polarity_loss: 0.1883 - Category_accuracy: 0.8631 - Polarity_accuracy: 0.9396 - val_loss: 0.7158 - val_Category_loss: 0.4923 - val_Polarity_loss: 0.2236 - val_Category_accuracy: 0.8488 - val_Polarity_accuracy: 0.9286\n",
      "Epoch 8/10\n",
      "237/237 [==============================] - 14s 57ms/step - loss: 0.5411 - Category_loss: 0.3644 - Polarity_loss: 0.1767 - Category_accuracy: 0.8775 - Polarity_accuracy: 0.9443 - val_loss: 0.6418 - val_Category_loss: 0.4453 - val_Polarity_loss: 0.1965 - val_Category_accuracy: 0.8595 - val_Polarity_accuracy: 0.9440\n",
      "Epoch 9/10\n",
      "237/237 [==============================] - 13s 56ms/step - loss: 0.5100 - Category_loss: 0.3444 - Polarity_loss: 0.1656 - Category_accuracy: 0.8812 - Polarity_accuracy: 0.9478 - val_loss: 0.6708 - val_Category_loss: 0.4673 - val_Polarity_loss: 0.2035 - val_Category_accuracy: 0.8643 - val_Polarity_accuracy: 0.9369\n",
      "Epoch 10/10\n",
      "237/237 [==============================] - 14s 59ms/step - loss: 0.4492 - Category_loss: 0.3165 - Polarity_loss: 0.1327 - Category_accuracy: 0.8940 - Polarity_accuracy: 0.9589 - val_loss: 0.6335 - val_Category_loss: 0.4628 - val_Polarity_loss: 0.1706 - val_Category_accuracy: 0.8583 - val_Polarity_accuracy: 0.9500\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 0.5641 - Category_loss: 0.3954 - Polarity_loss: 0.1687 - Category_accuracy: 0.8724 - Polarity_accuracy: 0.9533\n",
      "Test Loss and Accuracy: [0.5640628933906555, 0.39538535475730896, 0.1686774045228958, 0.8723809719085693, 0.95333331823349]\n",
      "66/66 [==============================] - 3s 22ms/step\n",
      "Classification Report for Category:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88       458\n",
      "           1       0.90      0.95      0.93       583\n",
      "           2       0.88      0.85      0.87       343\n",
      "           3       0.87      0.70      0.77       406\n",
      "           4       0.84      0.92      0.88       310\n",
      "\n",
      "    accuracy                           0.87      2100\n",
      "   macro avg       0.87      0.87      0.87      2100\n",
      "weighted avg       0.87      0.87      0.87      2100\n",
      "\n",
      "\n",
      "Classification Report for Polarity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       718\n",
      "           1       0.94      0.99      0.96       689\n",
      "           2       0.95      0.96      0.95       693\n",
      "\n",
      "    accuracy                           0.95      2100\n",
      "   macro avg       0.95      0.95      0.95      2100\n",
      "weighted avg       0.95      0.95      0.95      2100\n",
      "\n",
      "\n",
      "Macro-Averaged Scores for Category:\n",
      "Precision: 0.8697, Recall: 0.8660, F1 Score: 0.8653\n",
      "\n",
      "Macro-Averaged Scores for Polarity:\n",
      "Precision: 0.9539, Recall: 0.9539, F1 Score: 0.9533\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Layer, LSTM, Bidirectional, Dense, Input, Reshape, GlobalAveragePooling1D\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv(r\"F:\\Context-Resonance Transformer\\Cricket\\Cricket - Sheet1.csv\")\n",
    "df.head()\n",
    "df = df[['Text', 'Category', 'Polarity']]\n",
    "df.head()\n",
    "df['Category'].value_counts()\n",
    "df['Polarity'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep only Bengali characters\n",
    "    text = re.sub(r'\\d+', '', text)                 # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "df.head()\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Category' and 'Polarity'\n",
    "df_upsampled_category = upsample(df, 'Category')\n",
    "df_upsampled_polarity = upsample(df_upsampled_category, 'Polarity')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled_polarity.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Category distribution after upsampling:\")\n",
    "print(df_upsampled['Category'].value_counts())\n",
    "print(\"\\nPolarity distribution after upsampling:\")\n",
    "print(df_upsampled['Polarity'].value_counts())\n",
    "\n",
    "df_upsampled.head()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "category_encoder = LabelEncoder()\n",
    "polarity_encoder = LabelEncoder()\n",
    "\n",
    "df_upsampled['Category_encoded'] = category_encoder.fit_transform(df_upsampled['Category'])\n",
    "df_upsampled['Polarity_encoded'] = polarity_encoder.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Tokenize the text using DistilBERT with padding and truncation\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')\n",
    "\n",
    "df_upsampled['tokens'] = df_upsampled['Text'].apply(lambda x: tokenize_function(x))\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df_upsampled, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Convert to TensorFlow Dataset\n",
    "def create_tensor_dataset(df):\n",
    "    # Tokenize input text and convert to TensorFlow tensors\n",
    "    inputs = tokenizer(list(df['Text']), padding='max_length', truncation=True, max_length=128, return_tensors='tf')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Convert labels to tensors\n",
    "    labels_category = tf.convert_to_tensor(df['Category_encoded'].values)\n",
    "    labels_polarity = tf.convert_to_tensor(df['Polarity_encoded'].values)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices(((input_ids, attention_mask), (labels_category, labels_polarity)))\n",
    "\n",
    "def tokenize_data(df_upsampled, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sentence in df_upsampled['Text']:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True, \n",
    "            max_length=max_length,    \n",
    "            padding='max_length',    \n",
    "            truncation=True,           \n",
    "            return_attention_mask=True, \n",
    "            return_tensors='tf'        \n",
    "        )\n",
    "        \n",
    "        # Append to lists\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    \n",
    "    # Squeeze the extra dimension\n",
    "    input_ids = tf.squeeze(input_ids, axis=1)\n",
    "    attention_masks = tf.squeeze(attention_masks, axis=1)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "input_ids, attention_masks = tokenize_data(df_upsampled)\n",
    "\n",
    "label_1 = tf.convert_to_tensor(df_upsampled['Category'])\n",
    "label_2 = tf.convert_to_tensor(df_upsampled['Polarity'])\n",
    "\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention masks shape: {attention_masks.shape}\")\n",
    "print(f\"Label 1 (Category) shape: {label_1.shape}\")\n",
    "print(f\"Label 2 (Polarity) shape: {label_2.shape}\")\n",
    "# Ensure input_ids and attention_masks are converted to integer type tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks, dtype=tf.int32)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoders for the string labels\n",
    "label_encoder_1 = LabelEncoder()\n",
    "label_encoder_2 = LabelEncoder()\n",
    "\n",
    "# Encode string labels into integers\n",
    "df_upsampled['Category'] = label_encoder_1.fit_transform(df_upsampled['Category'])\n",
    "df_upsampled['Polarity'] = label_encoder_2.fit_transform(df_upsampled['Polarity'])\n",
    "\n",
    "# Convert labels to TensorFlow tensors\n",
    "label_1 = tf.convert_to_tensor(df_upsampled['Category'], dtype=tf.int32)\n",
    "label_2 = tf.convert_to_tensor(df_upsampled['Polarity'], dtype=tf.int32)\n",
    "\n",
    "# Ensure input_ids and attention_masks are correctly formatted as tensors\n",
    "input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "attention_masks = tf.convert_to_tensor(attention_masks, dtype=tf.int32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, att_mask_train, att_mask_test, y_cat_train, y_cat_test, y_gender_train, y_gender_test = train_test_split(\n",
    "    input_ids.numpy(), attention_masks.numpy(),\n",
    "    label_1.numpy(), label_2.numpy(),  \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_bilstm_model(input_shape):\n",
    "    input_ids = tf.keras.layers.Input(shape=(input_shape,), dtype='int32', name='input_ids')\n",
    "    attention_masks = tf.keras.layers.Input(shape=(input_shape,), dtype='int32', name='attention_masks')\n",
    "\n",
    "    # Embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=128)(input_ids)\n",
    "\n",
    "    # First BiLSTM layer with dropout\n",
    "    lstm_output = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3))(embedding_layer)\n",
    "\n",
    "    # Second BiLSTM layer\n",
    "    lstm_output_2 = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False, dropout=0.3))(lstm_output)\n",
    "\n",
    "    # Dense layer before output layers\n",
    "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(lstm_output_2)\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    dropout_layer = tf.keras.layers.Dropout(0.3)(dense_layer)\n",
    "\n",
    "    # Output layers for multi-task learning\n",
    "    output_category = tf.keras.layers.Dense(5, activation='softmax', name='Category')(dropout_layer)\n",
    "    output_polarity = tf.keras.layers.Dense(4, activation='softmax', name='Polarity')(dropout_layer)\n",
    "\n",
    "    # Define the model with inputs and outputs\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_masks],\n",
    "                           outputs=[output_category, output_polarity])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the BiLSTM model\n",
    "bilstm_model = create_bilstm_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "bilstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'Category': 'sparse_categorical_crossentropy', 'Polarity': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'Category': 'accuracy', 'Polarity': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "bilstm_model.summary()\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "         \n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=3,          # Stop training after 3 epochs of no improvement\n",
    "    restore_best_weights=True  # Restore the best weights from the epoch with the lowest validation loss\n",
    ")\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "history = bilstm_model.fit(\n",
    "    [X_train, att_mask_train],  # Inputs\n",
    "    {'Category': y_cat_train, 'Polarity': y_gender_train},  # Outputs\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]  # Add the EarlyStopping callback\n",
    ")\n",
    "# Evaluate the model on the test set\n",
    "results = bilstm_model.evaluate(\n",
    "    [X_test, att_mask_test],\n",
    "    {'Category': y_cat_test, 'Polarity': y_gender_test}\n",
    ")\n",
    "\n",
    "print(f\"Test Loss and Accuracy: {results}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Get Predictions\n",
    "predictions = bilstm_model.predict([X_test, att_mask_test])\n",
    "\n",
    "# Step 2: Convert predictions to class labels\n",
    "y_cat_pred = np.argmax(predictions[0], axis=1)   # 'Category' prediction\n",
    "y_gender_pred = np.argmax(predictions[1], axis=1)  # 'Polarity' prediction\n",
    "\n",
    "# Step 3: Generate Classification Report with zero_division specified\n",
    "# For Category\n",
    "print(\"Classification Report for Category:\")\n",
    "print(classification_report(y_cat_test, y_cat_pred, zero_division=0))\n",
    "\n",
    "# For Polarity\n",
    "print(\"\\nClassification Report for Polarity:\")\n",
    "print(classification_report(y_gender_test, y_gender_pred, zero_division=0))\n",
    "\n",
    "# If you want the macro-averaged precision, recall, and F1 scores separately:\n",
    "cat_precision = precision_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "cat_recall = recall_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "cat_f1 = f1_score(y_cat_test, y_cat_pred, average='macro', zero_division=0)\n",
    "\n",
    "gender_precision = precision_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "gender_recall = recall_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "gender_f1 = f1_score(y_gender_test, y_gender_pred, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\nMacro-Averaged Scores for Category:\")\n",
    "print(f\"Precision: {cat_precision:.4f}, Recall: {cat_recall:.4f}, F1 Score: {cat_f1:.4f}\")\n",
    "\n",
    "print(\"\\nMacro-Averaged Scores for Polarity:\")\n",
    "print(f\"Precision: {gender_precision:.4f}, Recall: {gender_recall:.4f}, F1 Score: {gender_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
